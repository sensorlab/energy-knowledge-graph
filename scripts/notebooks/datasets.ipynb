{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Using cached pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.23.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-13.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEKN\n",
    "\n",
    "https://data.open-power-system-data.org/household_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Energy_graph/data/temp/household_data_15min_singleindex_filtered.csv\")\n",
    "df =df.drop(columns=[\"utc_timestamp\", \"interpolated\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cet_cest_timestamp\"] = df[\"cet_cest_timestamp\"].apply(lambda x: x.split(\"+\")[0])\n",
    "df[\"cet_cest_timestamp\"] = pd.to_datetime(df[\"cet_cest_timestamp\"], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "df = df.set_index(\"cet_cest_timestamp\")\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract household identifiers\n",
    "households = set(column.split('_')[2] for column in df.columns)\n",
    "\n",
    "# Create a dictionary of dataframes, one for each household\n",
    "dfs = {}\n",
    "\n",
    "for household in households:\n",
    "    # Filter columns relevant to this household\n",
    "    relevant_columns = [col for col in df.columns if household in col]\n",
    "    temp_df = df[relevant_columns].copy()\n",
    "\n",
    "    # Rename columns to remove the prefix and retain the device name\n",
    "    rename_dict = {col: col.replace(f\"DE_KN_{household}_\", \"\") for col in relevant_columns}\n",
    "    temp_df.rename(columns=rename_dict, inplace=True)\n",
    "    temp_df.rename(columns={'cet_cest_timestamp': 'timestamp', \"grid_import\": \"aggregate\"}, inplace=True)\n",
    "    if \"grid_export\" in temp_df.columns:\n",
    "        temp_df.drop(columns=['grid_export'], inplace=True)\n",
    "    if \"pv\" in temp_df.columns:\n",
    "        temp_df.drop(columns=['pv'], inplace=True)\n",
    "    # temp_df.drop(columns=['grid_export', 'pv'], inplace=True)\n",
    "    data = {}\n",
    "    name =\"DEKN_\" +str(household[-1])\n",
    "    for c in temp_df.columns:\n",
    "        data[c] = pd.DataFrame(temp_df[c].dropna())\n",
    "        \n",
    "    dfs[name] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"./Energy_graph/data/temp/household_data.xlsx\")\n",
    "df2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREEND\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sourceforge.net/projects/greend/\n",
    "\n",
    "\n",
    "\n",
    "GREEND download form\n",
    "Great to get to know you! \n",
    "\n",
    "Here are our dataset snapshots and the associated password:\n",
    "\n",
    "v0.1: \n",
    "http://sourceforge.net/projects/greend/files/GREEND_0-1_311014.zip/download\n",
    "\n",
    "PWD:\"Vienna\"\n",
    "\n",
    "\n",
    "https://www.academia.edu/7794767/GREEND_An_Energy_Consumption_Dataset_of_Households_in_Italy_and_Austria\n",
    "\n",
    "http://www.andreatonello.com/wp-content/uploads/PAPERS/CONFERENCES/SGC2014_2.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Energy_graph/data/temp/GREEND/building0/dataset_2013-12-07.csv\", on_bad_lines=\"skip\")\n",
    "df\n",
    "\n",
    "\n",
    "# TODO either fix NILMTK if possible or try to get id to device mapping from somewhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENERTALK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Energy_graph/data/temp/ENERTALK/enertalk\"\n",
    "def convert2KRtime(df):\n",
    "    \"\"\"\n",
    "    convert dateframe's unix timestamp into Asia/Seoul Timezone\n",
    "    \n",
    "    input\n",
    "    ----\n",
    "        df: dataframe (columns: timestamp, active_power, reactive_power, appliance)\n",
    "    \n",
    "    output\n",
    "    ----\n",
    "        df_kr: dataframe (columns: timestamp, active_power, reactive_power, appliance, KR timezone)\n",
    "    \"\"\" \n",
    "\n",
    "    df_kr = df\n",
    "    df_kr['timestamp'] = df_kr['timestamp'].dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "    df_kr = df_kr.set_index(pd.DatetimeIndex(df_kr['timestamp']))\n",
    "    return df_kr\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the name of the file to get the device name\"\n",
    "    \"\"\"\n",
    "    df.drop(columns=[\"reactive_power\"], inplace=True)\n",
    "    # convert unix timestamp to datetime and set as index\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\").dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "    # convert to kWh\n",
    "    df  = df/1000 * (1/15)/3600\n",
    "    # resample to 1 second\n",
    "    df = df.resample(\"1S\").sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the name of the file to get the device name\"\n",
    "    \"\"\"\n",
    "    # remove the extension\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    # get the device name\n",
    "    file_name = file_name.split(\"_\")[1]\n",
    " \n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "\n",
    "def process_house(house):\n",
    "    house_path = os.path.join(data_path, house)\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    \n",
    "    for day in os.listdir(house_path):\n",
    "        day_path = os.path.join(house_path, day)\n",
    "        \n",
    "        for device in os.listdir(day_path):\n",
    "            device_path = os.path.join(day_path, device)\n",
    "            name = parse_name(device)\n",
    "            \n",
    "            df = preprocess_dataframe(pd.read_parquet(device_path))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "    \n",
    "    return house_name, house_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serial program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "data_path = \"./Energy_graph/data/temp/ENERTALK/enertalk\"\n",
    "data_dict = {}\n",
    "for house in os.listdir(data_path):\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    for day in tqdm(os.listdir(data_path + \"/\" + house)):\n",
    "        for device in os.listdir(data_path + \"/\" + house + \"/\" + day):\n",
    "            name = parse_name(device)\n",
    "            df = preprocess_dataframe(pd.read_parquet(data_path + \"/\" + house + \"/\" + day + \"/\" + device))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "    \n",
    "    data_dict[house_name] = house_dict\n",
    "    break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"./Energy_graph/data/temp/ENERTALK/enertalk\"\n",
    "data_dict = {}\n",
    "\n",
    "def process_house(house, progress_bar=None):\n",
    "    house_path = os.path.join(data_path, house)\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    \n",
    "    for day in os.listdir(house_path):\n",
    "        day_path = os.path.join(house_path, day)\n",
    "        for device in os.listdir(day_path):\n",
    "            device_path = os.path.join(day_path, device)\n",
    "            name = parse_name(device)\n",
    "            df = preprocess_dataframe(pd.read_parquet(device_path))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "\n",
    "    if progress_bar:\n",
    "        progress_bar.update(1)  # Increment the progress bar when a house is processed\n",
    "\n",
    "    return house_name, house_dict\n",
    "\n",
    "houses = os.listdir(data_path)\n",
    "# Create a progress bar with a total equal to the number of houses\n",
    "with tqdm(total=len(houses), desc=\"Processing houses\", unit=\"house\") as progress_bar:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Pass the progress_bar to the worker function\n",
    "        futures = [executor.submit(process_house, house, progress_bar) for house in houses]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            house_name, house_dict = future.result()\n",
    "            data_dict[house_name] = house_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "def process_house(house_path, queue):\n",
    "    house = os.path.basename(house_path)  # Extract house name from the path\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    \n",
    "    for day in os.listdir(house_path):\n",
    "        day_path = os.path.join(house_path, day)\n",
    "        for device in os.listdir(day_path):\n",
    "            device_path = os.path.join(day_path, device)\n",
    "            name = parse_name(device)\n",
    "            df = preprocess_dataframe(pd.read_parquet(device_path))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "\n",
    "    queue.put(1)  # Indicate that one house has been processed\n",
    "    return house_name, house_dict\n",
    "\n",
    "# Construct full paths for each house directory\n",
    "data_path = \"./Energy_graph/data/temp/ENERTALK/\"\n",
    "house_paths = [os.path.join(data_path, house) for house in os.listdir(data_path)]\n",
    "queue = multiprocessing.Manager().Queue()\n",
    "\n",
    "with tqdm(total=len(house_paths), desc=\"Processing houses\", unit=\"house\") as progress_bar:\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=os.cpu_count()/2) as executor:\n",
    "        futures = [executor.submit(process_house, house_path, queue) for house_path in house_paths]\n",
    "        \n",
    "        # Update progress bar based on queue\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            progress_bar.update(queue.get())\n",
    "\n",
    "        for future in futures:\n",
    "            house_name, house_dict = future.result()\n",
    "            data_dict[house_name] = house_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"ENERTALK_0\"]\n",
    "# save with pickle\n",
    "import pickle\n",
    "with open(\"./Energy_graph/data/processed/ENERTALK.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(\"./Energy_graph/data/temp/ENERTALK/enertalk/00/20161101/02_washing-machine.parquet.gzip\").drop(columns=[\"reactive_power\"])\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\").dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "df  = df/1000 * (1/15)/3600\n",
    "df.resample(\"1S\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"02_washing-machine.parquet.gzip\"\n",
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the name of the file to get the device name\"\n",
    "    \"\"\"\n",
    "    # remove the extension\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    # get the device name\n",
    "    file_name = file_name.split(\"_\")[1]\n",
    " \n",
    "\n",
    "    return file_name\n",
    "\n",
    "print(parse_name(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEDDIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get map of item_id to label for appliance\n",
    "labels = pd.read_csv(\"./Energy_graph/data/temp/DEDDIAG/house_08/items.tsv\", sep=\"\\t\")\n",
    "labels.set_index(\"item_id\", inplace=True)\n",
    "id_label_map = labels[\"category\"].to_dict()\n",
    "id_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_id(file_name : str) -> int:\n",
    "    return int(file_name.split('_')[1])\n",
    "\n",
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Energy_graph/data/temp/DEDDIAG/house_08/\"\n",
    "from tqdm import tqdm\n",
    "data = {}\n",
    "\n",
    "for device in tqdm([d for d in os.listdir(data_path) if \"data\" in d]):\n",
    "    label = id_label_map[parse_id(device)]\n",
    "    if \"Phase\" not in label:\n",
    "        if \"Total\" in label:\n",
    "            label = \"aggregate\"\n",
    "        df = pd.read_csv(data_path + device, sep=\"\\t\")\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df.drop(columns=[\"item_id\"], inplace=True)\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        df = df.resample(\"1s\").ffill()\n",
    "        df.dropna(inplace=True)\n",
    "        df = watts2kwh(df, 1/3600)\n",
    "        print(label)\n",
    "        data[label] = df\n",
    "\n",
    "    \n",
    "data_dict = {\n",
    "    \"DEDDIAG_8\": data,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"aggregate\"].resample(\"D\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSTData2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Energy_graph/data/temp/SUSTData/\"\n",
    "# aggregate consumption data\n",
    "df_aggregate = pd.DataFrame()\n",
    "for file in os.listdir(path + \"aggregate\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_aggregate = pd.concat([df_aggregate,(pd.read_csv(path+\"aggregate/\" + file))])\n",
    "\n",
    "df_aggregate[\"timestamp\"] = pd.to_datetime(df_aggregate[\"timestamp\"])\n",
    "df_aggregate.set_index(\"timestamp\", inplace=True)\n",
    "df_aggregate.drop(columns=['Unnamed: 0', \"Q\",\"V\",\"I\"], inplace=True)\n",
    "df_aggregate.rename(columns={\"P\":\"power\"}, inplace=True)\n",
    "data_dict = {\"aggregate\":df_aggregate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the file name to get the appliance name\n",
    "    \"\"\"\n",
    "    # appliance name\n",
    "    appliance_name = file_name.split(\".\")[0].split(\"_\")[1]\n",
    "    # date\n",
    "    return appliance_name\n",
    "\n",
    "\n",
    "# appliance consumption data\n",
    "for file in os.listdir(path+\"appliances/\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(parse_name(file))\n",
    "        data_dict[parse_name(file)] = pd.read_csv(path + \"appliances/\" + file).set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"aggregate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSTData1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_samples_d4_1.csv\n",
      "power_samples_d4_2.csv\n",
      "power_samples_d3_2.csv\n",
      "power_samples_d3_3.csv\n",
      "power_samples_d3_4.csv\n",
      "power_samples_d3_1.csv\n",
      ".ipynb_checkpoints\n",
      "power_samples_d1_1.csv\n",
      ".DS_Store\n",
      "power_samples_d1_2.csv\n",
      "power_samples_d2_2.csv\n",
      "power_samples_d2_5.csv\n",
      ".ipynb_checkpoints\n",
      "power_samples_d2_4.csv\n",
      "power_samples_d2_3.csv\n",
      ".DS_Store\n",
      "power_samples_d2_1.csv\n"
     ]
    }
   ],
   "source": [
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataframe\n",
    "    \"\"\"\n",
    "   \n",
    "    df = df.drop(columns=['Imin', 'Imax', 'Iavg', 'Vmin', 'Vmax',\n",
    "        'Vavg', 'Pmin', 'Pmax', 'Qmin', 'Qmax', 'Qavg', 'PFmin',\n",
    "        'PFmax', 'PFavg', 'miss_flag', 'iid', 'deploy']).dropna().set_index(\"tmstp\").sort_index()\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    df = df.resample(\"min\").fillna(method=\"nearest\", limit=5).dropna()# if there is data within 5 minutes, fill it in else drop it\n",
    "    df = watts2kwh(df, 1/60)\n",
    "    return df\n",
    "\n",
    "path = \"./Energy_graph/data/temp/SUST/SUST1/aggregate/\"\n",
    "data = {}\n",
    "for house in range(1,51):\n",
    "    name = \"SUST1_\" + str(house)\n",
    "    tmp = {\"aggregate\" : pd.DataFrame()}\n",
    "    data[name] = tmp\n",
    "homes = set()\n",
    "for folder in os.listdir(path):\n",
    "    for file in os.listdir(path + folder):\n",
    "        print(file)\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path + folder + \"/\" + file)\n",
    "            # drop rows with missing data\n",
    "            df = df[df['miss_flag'] == 0]\n",
    "            # convert timestamp to datetime\n",
    "            df[\"tmstp\"] = pd.to_datetime(df[\"tmstp\"])\n",
    "            df.rename(columns={\"Pavg\":\"aggregate\"}, inplace=True)\n",
    "            for iid in df[\"iid\"].unique():\n",
    "                name = \"SUST1_\" + str(iid)\n",
    "                data[name][\"aggregate\"] = pd.concat([data[name][\"aggregate\"], preprocess_df(df[df[\"iid\"] == iid])], axis=0)\n",
    "        # break\n",
    "    # break\n",
    "\n",
    "for house in data:\n",
    "    data[house][\"aggregate\"] = data[house][\"aggregate\"].sort_index()\n",
    "    data[house][\"aggregate\"] = data[house][\"aggregate\"][~data[house][\"aggregate\"].index.duplicated(keep='first')]\n",
    "# homes = list(homes).sort()\n",
    "\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./Energy_graph/data/temp/SUST/SUST1/aggregate/power_samples_d1/power_samples_d1_1.csv\")\n",
    "# df = df[df['miss_flag'] == 0]\n",
    "# df[\"tmstp\"] = pd.to_datetime(df[\"tmstp\"])\n",
    "# df = df[(df[\"iid\"] == 2)].drop(columns=['Imin', 'Imax', 'Iavg', 'Vmin', 'Vmax',\n",
    "#        'Vavg', 'Pmin', 'Pmax', 'Qmin', 'Qmax', 'Qavg', 'PFmin',\n",
    "#        'PFmax', 'PFavg', 'miss_flag', 'iid', 'deploy']).dropna().set_index(\"tmstp\").sort_index()\n",
    "# df.resample(\"1min\").fillna(method=\"nearest\", limit=5).dropna()[:180].plot()# if there is data within 5 minutes, fill it in else drop it\n",
    "data[\"SUST1_50\"][\"aggregate\"][:600].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>tmstp</th>\n",
       "      <th>deploy</th>\n",
       "      <th>Pavg</th>\n",
       "      <th>miss_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-25 20:33:34</td>\n",
       "      <td>2</td>\n",
       "      <td>823.51100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-25 20:34:36</td>\n",
       "      <td>2</td>\n",
       "      <td>838.49400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-25 20:35:39</td>\n",
       "      <td>2</td>\n",
       "      <td>831.41400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-25 20:36:41</td>\n",
       "      <td>2</td>\n",
       "      <td>831.80400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-25 20:37:44</td>\n",
       "      <td>2</td>\n",
       "      <td>839.35400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958532</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-04-10 19:52:34</td>\n",
       "      <td>2</td>\n",
       "      <td>541.82800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958533</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-04-10 19:53:37</td>\n",
       "      <td>2</td>\n",
       "      <td>537.85700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958534</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-04-10 19:54:09</td>\n",
       "      <td>2</td>\n",
       "      <td>559.01700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959613</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-05-12 13:53:46</td>\n",
       "      <td>2</td>\n",
       "      <td>6.78973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959614</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-05-12 13:54:22</td>\n",
       "      <td>2</td>\n",
       "      <td>8.55725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2791173 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         iid                tmstp  deploy       Pavg  miss_flag\n",
       "1233       1  2010-11-25 20:33:34       2  823.51100          0\n",
       "1234       1  2010-11-25 20:34:36       2  838.49400          0\n",
       "1235       1  2010-11-25 20:35:39       2  831.41400          0\n",
       "1236       1  2010-11-25 20:36:41       2  831.80400          0\n",
       "1237       1  2010-11-25 20:37:44       2  839.35400          0\n",
       "...      ...                  ...     ...        ...        ...\n",
       "2958532    5  2012-04-10 19:52:34       2  541.82800          0\n",
       "2958533    5  2012-04-10 19:53:37       2  537.85700          0\n",
       "2958534    5  2012-04-10 19:54:09       2  559.01700          0\n",
       "2959613    5  2012-05-12 13:53:46       2    6.78973          0\n",
       "2959614    5  2012-05-12 13:54:22       2    8.55725          0\n",
       "\n",
       "[2791173 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Imin', 'Imax', 'Iavg', 'Vmin', 'Vmax',\n",
    "       'Vavg', 'Pmin', 'Pmax', 'Qmin', 'Qmax', 'Qavg', 'PFmin',\n",
    "       'PFmax', 'PFavg']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where missing flag is true\n",
    "df = df[df['miss_flag'] == 0]\n",
    "df[\"iid\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFRED\n",
    "unused for now because of aggregated apartments might be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Energy_graph/data/temp/MFRED/MFRED_Aggregates_15min_2019Q1-Q4.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBED\n",
    "TODO\n",
    "http://embed-dataset.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue</td>\n",
       "      <td>Aug 06 2013</td>\n",
       "      <td>13:29:48</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2013-08-06 13:29:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue</td>\n",
       "      <td>Aug 06 2013</td>\n",
       "      <td>13:29:49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2013-08-06 13:29:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue</td>\n",
       "      <td>Aug 06 2013</td>\n",
       "      <td>13:29:49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2013-08-06 13:29:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue</td>\n",
       "      <td>Aug 06 2013</td>\n",
       "      <td>13:29:50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2013-08-06 13:29:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue</td>\n",
       "      <td>Aug 06 2013</td>\n",
       "      <td>13:29:50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2013-08-06 13:29:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243973</th>\n",
       "      <td>Mon</td>\n",
       "      <td>Aug 19 2013</td>\n",
       "      <td>10:57:24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2013-08-19 10:57:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243974</th>\n",
       "      <td>Mon</td>\n",
       "      <td>Aug 19 2013</td>\n",
       "      <td>10:57:25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2013-08-19 10:57:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243975</th>\n",
       "      <td>Mon</td>\n",
       "      <td>Aug 19 2013</td>\n",
       "      <td>10:57:27</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2013-08-19 10:57:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243976</th>\n",
       "      <td>Mon</td>\n",
       "      <td>Aug 19 2013</td>\n",
       "      <td>10:57:28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2013-08-19 10:57:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243977</th>\n",
       "      <td>Mon</td>\n",
       "      <td>Aug 19 2013</td>\n",
       "      <td>10:57:29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2013-08-19 10:57:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1243978 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0            1         2     3            datetime\n",
       "0        Tue  Aug 06 2013  13:29:48  0.16 2013-08-06 13:29:48\n",
       "1        Tue  Aug 06 2013  13:29:49  0.15 2013-08-06 13:29:49\n",
       "2        Tue  Aug 06 2013  13:29:49  0.15 2013-08-06 13:29:49\n",
       "3        Tue  Aug 06 2013  13:29:50  0.16 2013-08-06 13:29:50\n",
       "4        Tue  Aug 06 2013  13:29:50  0.16 2013-08-06 13:29:50\n",
       "...      ...          ...       ...   ...                 ...\n",
       "1243973  Mon  Aug 19 2013  10:57:24  0.25 2013-08-19 10:57:24\n",
       "1243974  Mon  Aug 19 2013  10:57:25  0.23 2013-08-19 10:57:25\n",
       "1243975  Mon  Aug 19 2013  10:57:27  0.23 2013-08-19 10:57:27\n",
       "1243976  Mon  Aug 19 2013  10:57:28  0.25 2013-08-19 10:57:28\n",
       "1243977  Mon  Aug 19 2013  10:57:29  0.23 2013-08-19 10:57:29\n",
       "\n",
       "[1243978 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./Energy_graph/data/temp/EMBED/Apt1_GT_Plug/Hair Dryer.csv\"\n",
    "\n",
    "df = pd.read_csv(path, header=None)\n",
    "df['datetime'] = pd.to_datetime(df[1].str.cat(df[2], sep=' '))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'data'])\n",
      "Shape of t_power: (587654, 1)\n",
      "Shape of tt_power: (587654, 1)\n",
      "Shape of Qa: (9, 587654)\n",
      "Shape of Qb: (9, 587654)\n",
      "Shape of Pa: (9, 587654)\n",
      "Shape of Pb: (9, 587654)\n",
      "Shape of startDate: (1, 1)\n",
      "Shape of startTime: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "\n",
    "# Load the .mat file\n",
    "mat_data = loadmat('./Energy_graph/data/temp/EMBED/Apt1_data (1).mat')\n",
    "\n",
    "# The data could be stored under various keys. Usually, it's not under meta keys like '__header__', '__version__', '__globals__'.\n",
    "# Let's say your data is under the key 'dataKey'. If you're unsure about the key, print mat_data.keys() to inspect.\n",
    "print(mat_data.keys())\n",
    "data = mat_data['data']\n",
    "\n",
    "# The shape (1, 1) suggests that data is essentially a 2D array with a single element. This single element could itself be an array or another complex datatype.\n",
    "\n",
    "# To further inspect this, let's access the inner contents of this 2D array:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "inner_content = data[0, 0]\n",
    "# print(type(inner_content))\n",
    "# print(inner_content.shape)\n",
    "# print(inner_content.dtype.names)\n",
    "\n",
    "field_names = inner_content.dtype.names\n",
    "for name in field_names:\n",
    "    print(f\"Shape of {name}: {inner_content[name].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 1D arrays for t_power and tt_power\n",
    "data_dict = {\n",
    "    't_power': inner_content['t_power'].squeeze(),\n",
    "    'tt_power': inner_content['tt_power'].squeeze(),\n",
    "    'startDate': [inner_content['startDate'][0,0]] * inner_content['t_power'].shape[0],\n",
    "    'startTime': [inner_content['startTime'][0,0]] * inner_content['t_power'].shape[0],\n",
    "}\n",
    "\n",
    "# Extracting columns for Qa, Qb, Pa, Pb\n",
    "for i in range(inner_content['Qa'].shape[0]):\n",
    "    data_dict[f'Qa_{i}'] = inner_content['Qa'][i, :]\n",
    "    data_dict[f'Qb_{i}'] = inner_content['Qb'][i, :]\n",
    "    data_dict[f'Pa_{i}'] = inner_content['Pa'][i, :]\n",
    "    data_dict[f'Pb_{i}'] = inner_content['Pb'][i, :]\n",
    "\n",
    "# Creating the dataframe\n",
    "df = pd.DataFrame(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_power</th>\n",
       "      <th>tt_power</th>\n",
       "      <th>startDate</th>\n",
       "      <th>startTime</th>\n",
       "      <th>Qa_0</th>\n",
       "      <th>Qb_0</th>\n",
       "      <th>Pa_0</th>\n",
       "      <th>Pb_0</th>\n",
       "      <th>Qa_1</th>\n",
       "      <th>Qb_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Pa_6</th>\n",
       "      <th>Pb_6</th>\n",
       "      <th>Qa_7</th>\n",
       "      <th>Qb_7</th>\n",
       "      <th>Pa_7</th>\n",
       "      <th>Pb_7</th>\n",
       "      <th>Qa_8</th>\n",
       "      <th>Qb_8</th>\n",
       "      <th>Pa_8</th>\n",
       "      <th>Pb_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>735452.562383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>-43.508073</td>\n",
       "      <td>78.877231</td>\n",
       "      <td>703.465276</td>\n",
       "      <td>715.219317</td>\n",
       "      <td>0.041146</td>\n",
       "      <td>0.039373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>-0.015485</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.003185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>735452.562384</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>-44.122336</td>\n",
       "      <td>78.802943</td>\n",
       "      <td>703.344349</td>\n",
       "      <td>711.424172</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>0.021345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019564</td>\n",
       "      <td>-0.005687</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.002874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>735452.562384</td>\n",
       "      <td>0.033330</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>-44.184718</td>\n",
       "      <td>74.937676</td>\n",
       "      <td>704.097946</td>\n",
       "      <td>703.786992</td>\n",
       "      <td>0.018148</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>-0.038401</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>-0.001763</td>\n",
       "      <td>-0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>735452.562384</td>\n",
       "      <td>0.049995</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>-43.558715</td>\n",
       "      <td>75.549708</td>\n",
       "      <td>703.883711</td>\n",
       "      <td>701.394935</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022956</td>\n",
       "      <td>-0.025042</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.002171</td>\n",
       "      <td>-0.006395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>735452.562384</td>\n",
       "      <td>0.066660</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>-43.025862</td>\n",
       "      <td>71.860145</td>\n",
       "      <td>703.007584</td>\n",
       "      <td>697.097498</td>\n",
       "      <td>-0.031575</td>\n",
       "      <td>-0.029193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025455</td>\n",
       "      <td>-0.063981</td>\n",
       "      <td>-0.000932</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>-0.001925</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>-0.004158</td>\n",
       "      <td>-0.011050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587649</th>\n",
       "      <td>735452.675912</td>\n",
       "      <td>9245.825325</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>17.471694</td>\n",
       "      <td>57.984443</td>\n",
       "      <td>20.281512</td>\n",
       "      <td>95.579867</td>\n",
       "      <td>-0.001957</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.039465</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>-0.003815</td>\n",
       "      <td>-0.015920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587650</th>\n",
       "      <td>735452.675912</td>\n",
       "      <td>9245.841990</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>17.451093</td>\n",
       "      <td>50.329093</td>\n",
       "      <td>20.159681</td>\n",
       "      <td>88.707211</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.001924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>-0.022520</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.013823</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>-0.024756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587651</th>\n",
       "      <td>735452.675912</td>\n",
       "      <td>9245.858655</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>17.363645</td>\n",
       "      <td>55.373979</td>\n",
       "      <td>20.036955</td>\n",
       "      <td>97.431764</td>\n",
       "      <td>-0.000943</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.011371</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>-0.020717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587652</th>\n",
       "      <td>735452.675912</td>\n",
       "      <td>9245.875320</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>17.378752</td>\n",
       "      <td>55.292825</td>\n",
       "      <td>20.016533</td>\n",
       "      <td>91.867555</td>\n",
       "      <td>-0.001335</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.027379</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>-0.000241</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>-0.004861</td>\n",
       "      <td>-0.018995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587653</th>\n",
       "      <td>735452.675912</td>\n",
       "      <td>9245.891985</td>\n",
       "      <td>[[[2013/08/06]]]</td>\n",
       "      <td>[[[13:29:49.934163324999489551]]]</td>\n",
       "      <td>17.375853</td>\n",
       "      <td>51.185004</td>\n",
       "      <td>20.080722</td>\n",
       "      <td>86.021643</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>-0.002779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>0.011923</td>\n",
       "      <td>-0.006124</td>\n",
       "      <td>-0.019285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>587654 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_power     tt_power         startDate  \\\n",
       "0       735452.562383     0.000000  [[[2013/08/06]]]   \n",
       "1       735452.562384     0.016665  [[[2013/08/06]]]   \n",
       "2       735452.562384     0.033330  [[[2013/08/06]]]   \n",
       "3       735452.562384     0.049995  [[[2013/08/06]]]   \n",
       "4       735452.562384     0.066660  [[[2013/08/06]]]   \n",
       "...               ...          ...               ...   \n",
       "587649  735452.675912  9245.825325  [[[2013/08/06]]]   \n",
       "587650  735452.675912  9245.841990  [[[2013/08/06]]]   \n",
       "587651  735452.675912  9245.858655  [[[2013/08/06]]]   \n",
       "587652  735452.675912  9245.875320  [[[2013/08/06]]]   \n",
       "587653  735452.675912  9245.891985  [[[2013/08/06]]]   \n",
       "\n",
       "                                startTime       Qa_0       Qb_0        Pa_0  \\\n",
       "0       [[[13:29:49.934163324999489551]]] -43.508073  78.877231  703.465276   \n",
       "1       [[[13:29:49.934163324999489551]]] -44.122336  78.802943  703.344349   \n",
       "2       [[[13:29:49.934163324999489551]]] -44.184718  74.937676  704.097946   \n",
       "3       [[[13:29:49.934163324999489551]]] -43.558715  75.549708  703.883711   \n",
       "4       [[[13:29:49.934163324999489551]]] -43.025862  71.860145  703.007584   \n",
       "...                                   ...        ...        ...         ...   \n",
       "587649  [[[13:29:49.934163324999489551]]]  17.471694  57.984443   20.281512   \n",
       "587650  [[[13:29:49.934163324999489551]]]  17.451093  50.329093   20.159681   \n",
       "587651  [[[13:29:49.934163324999489551]]]  17.363645  55.373979   20.036955   \n",
       "587652  [[[13:29:49.934163324999489551]]]  17.378752  55.292825   20.016533   \n",
       "587653  [[[13:29:49.934163324999489551]]]  17.375853  51.185004   20.080722   \n",
       "\n",
       "              Pb_0      Qa_1      Qb_1  ...      Pa_6      Pb_6      Qa_7  \\\n",
       "0       715.219317  0.041146  0.039373  ...  0.014270 -0.015485  0.000294   \n",
       "1       711.424172  0.022737  0.021345  ...  0.019564 -0.005687  0.000274   \n",
       "2       703.786992  0.018148  0.017963  ...  0.017582 -0.038401  0.000102   \n",
       "3       701.394935  0.000925  0.001153  ...  0.022956 -0.025042 -0.000189   \n",
       "4       697.097498 -0.031575 -0.029193  ...  0.025455 -0.063981 -0.000932   \n",
       "...            ...       ...       ...  ...       ...       ...       ...   \n",
       "587649   95.579867 -0.001957  0.002833  ...  0.003424  0.039465 -0.000505   \n",
       "587650   88.707211 -0.000790 -0.001924  ...  0.000714 -0.022520  0.000091   \n",
       "587651   97.431764 -0.000943 -0.001010  ...  0.001774  0.013797  0.000029   \n",
       "587652   91.867555 -0.001335 -0.000311  ...  0.002663  0.027379 -0.000113   \n",
       "587653   86.021643 -0.001331 -0.002779  ...  0.002904  0.001161  0.000578   \n",
       "\n",
       "            Qb_7      Pa_7      Pb_7      Qa_8      Qb_8      Pa_8      Pb_8  \n",
       "0       0.000065 -0.000131 -0.000047  0.003548  0.005824 -0.000030 -0.003185  \n",
       "1       0.000086 -0.000008  0.000078  0.003150  0.004905  0.000098 -0.002874  \n",
       "2       0.000084 -0.000126  0.000068  0.002345  0.007185 -0.001763 -0.005700  \n",
       "3       0.000555 -0.000327 -0.000216  0.002190  0.007141 -0.002171 -0.006395  \n",
       "4       0.000897 -0.000242 -0.001925  0.000285  0.009445 -0.004158 -0.011050  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "587649  0.000107  0.000202 -0.000419  0.005693  0.007678 -0.003815 -0.015920  \n",
       "587650  0.000266  0.000301 -0.000106  0.003128  0.013823 -0.007928 -0.024756  \n",
       "587651  0.000257  0.000336 -0.000218  0.004908  0.011371 -0.006064 -0.020717  \n",
       "587652  0.000456  0.000475 -0.000241  0.005837  0.010365 -0.004861 -0.018995  \n",
       "587653  0.001192  0.000380 -0.001282  0.004342  0.011923 -0.006124 -0.019285  \n",
       "\n",
       "[587654 rows x 40 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n",
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the file name to get the house name\n",
    "    \"\"\"\n",
    "    # appliance name\n",
    "    appliance_name = file_name.split(\".\")[0]\n",
    "\n",
    "    # date\n",
    "    return appliance_name[:5] + \"_\" + appliance_name[5:]\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"./Energy_graph/data/temp/HEART/HERON33.csv\")\n",
    "# df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n",
    "\n",
    "# df = df.set_index(\"Timestamp\").drop(columns=[\"dw\", \"wm\"])\n",
    "# df = watts2kwh(df, 1/3600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Energy_graph/data/temp/HEART/\"\n",
    "data_dict = {}\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # \n",
    "        df = pd.read_csv(data_path + file)\n",
    "        # convert unix timestamp to datetime\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n",
    "        # set datetime as index and drop unnecessary columns\n",
    "        df = df.set_index(\"Timestamp\").drop(columns=[\"dw\", \"wm\"])\n",
    "        \n",
    "        df.rename(columns={\"Value\": \"aggregate\"}, inplace=True)\n",
    "        # convert watts to kilowatt hours\n",
    "        df = watts2kwh(df, 1/3600)\n",
    "        df.dropna(inplace=True)\n",
    "        # create a dictionary of dataframes for each device\n",
    "        devices_dict = {}\n",
    "        for device in df.columns:\n",
    "                devices_dict[device] = pd.DataFrame(df[device])\n",
    "        # add the device dictionary to the data dictionary\n",
    "        data_dict[parse_name(file)] = devices_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n",
    "def read_and_preprocess_df(path):\n",
    "    df = pd.read_csv(path, header=None, names=[\"timestamp\", \"value\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    # set timestamp as index\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    df.sort_index(inplace=True)\n",
    "    # resample to 7s and forward fill up to 35s\n",
    "    df = df.resample(\"7s\").ffill(limit=7).dropna()\n",
    "\n",
    "    # convert to kWh\n",
    "    df = watts2kwh(df, 7/3600)\n",
    "    return df\n",
    "# get house name and appliance name from file name\n",
    "def parse_name(file_name : str):\n",
    "    file_name = file_name.split(\"_\")\n",
    "    house_name = file_name[0].replace(\"home\", \"IDEAL_\")\n",
    "    appliance_name = file_name[3]\n",
    "    if appliance_name == \"electric-mains\":\n",
    "        appliance_name = \"aggregate\"\n",
    "\n",
    "    if appliance_name == \"electric-appliance\":\n",
    "        appliance_name = file_name[4].split(\".\")[0]\n",
    "    return house_name, appliance_name\n",
    "\n",
    "def process_house(house, file_list, data_path):\n",
    "    house_data = {}\n",
    "    for file in file_list:\n",
    "        _, label, df = process_file(file, data_path)\n",
    "        house_data[label] = df\n",
    "    return house, house_data\n",
    "\n",
    "\n",
    "def process_file(file,data_path):\n",
    "    house, label = parse_name(file)\n",
    "    return house, label, read_and_preprocess_df(data_path + \"data_merged/\" + file)\n",
    "\n",
    "def process_files_for_home(house, file_list, data_path):\n",
    "    house_data = {}\n",
    "    for file in file_list:\n",
    "        _, label, df = process_file(file, data_path)\n",
    "        house_data[label] = df\n",
    "    return house, house_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"home168_kitchen1534_sensor12520_electric-appliance_washingmachinetumbledrier.csv.gz\"\n",
    "\n",
    "\n",
    "parse_name(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serial program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data_path = \"./Energy_graph/data/temp/IDEAL/\"\n",
    "files = [file for file in os.listdir(data_path + \"data_merged/\") if (\"electric-appliance\" in file or \"electric-mains\" in file) and \"home223\" not in file]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    house, label = parse_name(file)\n",
    "    data.setdefault(house, {})[label] = read_and_preprocess_df(data_path+\"data_merged/\" + file)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessed(takes around 1m:30s with 64 cores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "def unpack_and_process(p):\n",
    "    return process_house(*p)\n",
    "# Main script body\n",
    "data_path = \"./Energy_graph/data/temp/IDEAL/\"\n",
    "data_dict = {}\n",
    "files_grouped_by_home = defaultdict(list)\n",
    "files = [file for file in os.listdir(data_path + \"data_merged/\") if (\"electric-appliance\" in file or \"electric-mains\" in file) and \"home223\" not in file]\n",
    "for file in files:\n",
    "    house, _ = parse_name(file)\n",
    "    files_grouped_by_home[house].append(file)\n",
    "\n",
    "total_houses = len(files_grouped_by_home)\n",
    "\n",
    "print(\"Processing houses...\")\n",
    "with ProcessPoolExecutor(max_workers=int(os.cpu_count()/2)) as executor, tqdm(total=total_houses, desc=\"Processing houses\", unit=\"house\") as t:\n",
    "    args = ((house, files_grouped_by_home[house], data_path) for house in files_grouped_by_home)\n",
    "    \n",
    "    for house_name, house_data in executor.map(unpack_and_process, args):\n",
    "        data_dict[house_name] = house_data\n",
    "        t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save with pickle to: energy-knowledge-graph\\data\\processed\\IDEAL.pkl\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./Energy_graph/data/processed/IDEAL.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAE\n",
    "Room data not appliance\n",
    "\n",
    "needs to be cited\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZJW4LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Energy_graph/data/temp/RAE/test/house1_subs_blk2.csv\"\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix_ts</th>\n",
       "      <th>sub</th>\n",
       "      <th>V</th>\n",
       "      <th>f</th>\n",
       "      <th>I</th>\n",
       "      <th>dPF</th>\n",
       "      <th>aPF</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Qt</th>\n",
       "      <th>St</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457251200</td>\n",
       "      <td>1</td>\n",
       "      <td>119.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1950</td>\n",
       "      <td>46</td>\n",
       "      <td>4074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1457251200</td>\n",
       "      <td>2</td>\n",
       "      <td>119.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3139</td>\n",
       "      <td>526</td>\n",
       "      <td>4699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1457251200</td>\n",
       "      <td>3</td>\n",
       "      <td>119.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1457251200</td>\n",
       "      <td>4</td>\n",
       "      <td>119.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7238</td>\n",
       "      <td>1840</td>\n",
       "      <td>8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1457251200</td>\n",
       "      <td>5</td>\n",
       "      <td>119.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18935</td>\n",
       "      <td>4462</td>\n",
       "      <td>21632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126974635</th>\n",
       "      <td>1462690799</td>\n",
       "      <td>20</td>\n",
       "      <td>120.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>115</td>\n",
       "      <td>141</td>\n",
       "      <td>183</td>\n",
       "      <td>74782</td>\n",
       "      <td>77334</td>\n",
       "      <td>112016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126974636</th>\n",
       "      <td>1462690799</td>\n",
       "      <td>21</td>\n",
       "      <td>119.9</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>268792</td>\n",
       "      <td>19868</td>\n",
       "      <td>300476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126974637</th>\n",
       "      <td>1462690799</td>\n",
       "      <td>22</td>\n",
       "      <td>120.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>279871</td>\n",
       "      <td>54855</td>\n",
       "      <td>304911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126974638</th>\n",
       "      <td>1462690799</td>\n",
       "      <td>23</td>\n",
       "      <td>119.9</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.48</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3408</td>\n",
       "      <td>3898</td>\n",
       "      <td>9459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126974639</th>\n",
       "      <td>1462690799</td>\n",
       "      <td>24</td>\n",
       "      <td>120.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>158</td>\n",
       "      <td>21</td>\n",
       "      <td>182</td>\n",
       "      <td>275191</td>\n",
       "      <td>31511</td>\n",
       "      <td>338545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126974640 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              unix_ts  sub      V     f    I   dPF   aPF    P    Q    S  \\\n",
       "0          1457251200    1  119.7  60.0  0.0  0.99  0.03    0    0    3   \n",
       "1          1457251200    2  119.1  60.0  0.0  0.98  0.36    1    0    3   \n",
       "2          1457251200    3  119.7  60.0  0.0  0.26  0.01    0    0    1   \n",
       "3          1457251200    4  119.1  60.0  0.0  0.59  0.53    1    2    3   \n",
       "4          1457251200    5  119.7  60.0  0.0  0.99  0.21    0    0    3   \n",
       "...               ...  ...    ...   ...  ...   ...   ...  ...  ...  ...   \n",
       "126974635  1462690799   20  120.2  60.0  1.5  0.63  0.62  115  141  183   \n",
       "126974636  1462690799   21  119.9  60.0  0.0  0.77  0.45    2    2    6   \n",
       "126974637  1462690799   22  120.2  60.0  0.0  0.58  0.36    2    3    7   \n",
       "126974638  1462690799   23  119.9  60.0  0.0  0.91  0.48    4    1    8   \n",
       "126974639  1462690799   24  120.2  60.0  1.5  0.98  0.86  158   21  182   \n",
       "\n",
       "               Pt     Qt      St  \n",
       "0            1950     46    4074  \n",
       "1            3139    526    4699  \n",
       "2              39     41     806  \n",
       "3            7238   1840    8348  \n",
       "4           18935   4462   21632  \n",
       "...           ...    ...     ...  \n",
       "126974635   74782  77334  112016  \n",
       "126974636  268792  19868  300476  \n",
       "126974637  279871  54855  304911  \n",
       "126974638    3408   3898    9459  \n",
       "126974639  275191  31511  338545  \n",
       "\n",
       "[126974640 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.copy()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"unix_ts\"] = pd.to_datetime(test[\"unix_ts\"], unit=\"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f1d0ac7c340>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.set_index(\"unix_ts\").groupby(\"sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Kitchen', 'Wall', 'Oven', 'L1']\n",
      "['2', 'Kitchen', 'Wall', 'Oven', 'L2']\n",
      "['3', 'Kitchen', 'Counter', 'Plugs', 'L1']\n",
      "['4', 'Kitchen', 'Counter', 'Plugs', 'L2']\n",
      "['5', 'Clothes', 'Dryer', 'L1']\n",
      "['6', 'Clothes', 'Dryer', 'L2']\n",
      "['7', 'Upstairs', 'Bedroom', 'AFCI', 'Arc-Fault', 'Plugs']\n",
      "['8', 'Kitchen', 'Fridge']\n",
      "['9', 'Clothes', 'Washer']\n",
      "['10', 'Kitchen', 'Dishwasher']\n",
      "['11', 'Furnace', '&', 'Hot', 'Water', 'Unit', '(incl.', 'Furnace', 'Room', 'Plug)']\n",
      "['12', 'Basement', 'Plugs', '&', 'Lights', '(incl.', 'Outside', 'Plugs)']\n",
      "['13', 'Heat', 'Pump', 'L1']\n",
      "['14', 'Heat', 'Pump', 'L2']\n",
      "['15', 'Garage', 'Sub-Panel', 'L1']\n",
      "['16', 'Garage', 'Sub-Panel', 'L2']\n",
      "['17', 'Upstairs', 'Plugs', '&', 'Lights', 'L1', '(incl.', 'Bathroom', 'Lights', 'and', 'Vent', 'Fan,', 'Smoke', 'Alarms,', 'Living', 'Room', 'Plugs)']\n",
      "['18', 'Upstairs', 'Plugs', '&', 'Lights', 'L2', '(incl.', 'Bathroom', 'Lights', 'and', 'Vent', 'Fan,', 'Smoke', 'Alarms,', 'Living', 'Room', 'Plugs)']\n",
      "['19', 'Basement', 'Blue', 'Plugs', '(incl.', 'Ent.', 'TV/Amp/DVD/PVR)']\n",
      "['20', 'Bathrooms', '(incl.', '3', 'GFCI', 'Plugs,', '2', 'Lights,', '1', 'Vent', 'Fan,', 'Chest', 'Freezer)']\n",
      "['21', 'Rental', 'Suite', 'Sub-Panel', 'L1']\n",
      "['22', 'Rental', 'Suite', 'Sub-Panel', 'L2']\n",
      "['23', 'Misc.', 'Plugs', '(incl.', 'Dining', 'Room,', 'Gas', 'Cooktop,', 'Microwave)']\n",
      "['24', 'Home', 'Office', '(incl.', 'Telco/Cable/Net/Security', 'Equip.)']\n",
      "{1: 'Kitchen', 2: 'Kitchen', 3: 'Kitchen', 4: 'Kitchen', 5: 'Clothes', 6: 'Clothes', 7: 'Upstairs', 8: 'Kitchen', 9: 'Clothes', 10: 'Kitchen', 11: 'Furnace', 12: 'Basement', 13: 'Heat', 14: 'Heat', 15: 'Garage', 16: 'Garage', 17: 'Upstairs', 18: 'Upstairs', 19: 'Basement', 20: 'Bathrooms', 21: 'Rental', 22: 'Rental', 23: 'Misc.', 24: 'Home'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Energy_graph/data/temp/RAE/house1_labels.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "\n",
    "# Process each line to build the dictionary\n",
    "data_dict = {}\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split(' ')\n",
    "    print(parts)\n",
    "    key = int(parts[0])\n",
    "    value = parts[1].strip()\n",
    "    data_dict[key] = value\n",
    "\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECD-UY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_file(file):\n",
    "    file_path = os.path.join(DATA_PATH, 'consumption_data', file)\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    # pivot the dataframe so that each column is a different house with timestamps as the index and the values are the consumption\n",
    "    df = df.pivot(index=\"datetime\", columns=\"id\", values=\"value\")\n",
    "    # convert the timestamps to datetime objects and set the correct timezone\n",
    "    df.index = pd.to_datetime(df.index, unit='s', utc=True).tz_convert('America/Montevideo')\n",
    "    \n",
    "    temp_data = defaultdict(lambda: {\"aggregate\": []})\n",
    "    # iterate over each column and add the data to the dictionary and drop missing values\n",
    "    for col in df.columns:\n",
    "        name = \"ECDUY_\" + str(col)\n",
    "        temp_data[name][\"aggregate\"].append(df[col].dropna())\n",
    "    \n",
    "    return dict(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [01:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './consumption_data/consumption_data_201906.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/tmp/ipykernel_1792/2605759464.py\", line 4, in process_file\n    df = pd.read_csv(file_path)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n    self.handles = get_handle(\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/common.py\", line 856, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: './consumption_data/consumption_data_201906.csv'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m batch_files \u001b[39m=\u001b[39m files[i:i \u001b[39m+\u001b[39m batch_size]\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39mcpu_count) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> 16\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(executor\u001b[39m.\u001b[39;49mmap(process_file, batch_files, data_path))\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/process.py:570\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    565\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    571\u001b[0m         element\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    572\u001b[0m         \u001b[39mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    622\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    320\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './consumption_data/consumption_data_201906.csv'"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor \n",
    "data_path = './Energy_graph/data/temp/ECD-UY/'\n",
    "# set this to the number of cores you want to use\n",
    "cpu_count = 32\n",
    "\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(os.path.join(data_path, 'consumption_data')) if f.endswith('.csv')]\n",
    "len(files)\n",
    "batch_size = 11  # or whatever size you deem fit, based on your system's number of cores and memory there is 22 file total for size 11 around 500gb is\n",
    "data = defaultdict(lambda: {\"aggregate\": []})\n",
    "\n",
    "for i in tqdm(range(0, len(files), batch_size)):\n",
    "    batch_files = files[i:i + batch_size]\n",
    "    with ProcessPoolExecutor(max_workers=cpu_count) as executor:\n",
    "        results = list(executor.map(process_file, batch_files, data_path))\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            data[key][\"aggregate\"].extend(value[\"aggregate\"])\n",
    "\n",
    "# Convert defaultdict back to a normal dictionary if needed\n",
    "data = dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110952/110952 [01:19<00:00, 1394.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(data):\n",
    "    data[key][\"aggregate\"] = pd.concat(data[key][\"aggregate\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save  with pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"./Energy_graph/data/processed/ECDUY.pkl\", 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75338/75338 [00:02<00:00, 30842.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = defaultdict(lambda: {\"aggregate\": []})\n",
    "data_path = './Energy_graph/data/temp/ECD-UY/'\n",
    "\n",
    "# Load data from csv files\n",
    "for file in (os.listdir(os.path.join(data_path, 'consumption_data'))):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(data_path, 'consumption_data', file)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.pivot(index=\"datetime\", columns=\"id\", values=\"value\")\n",
    "        df.index = pd.to_datetime(df.index, unit='s').tz_localize('UTC').tz_convert('America/Montevideo')\n",
    "        \n",
    "        for col in tqdm(df.columns):\n",
    "            name = \"ECDUY_\" + str(col)\n",
    "            data[name][\"aggregate\"].append(df[col])\n",
    "\n",
    "    break\n",
    "data = dict(data)\n",
    "print(\"Processed files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
