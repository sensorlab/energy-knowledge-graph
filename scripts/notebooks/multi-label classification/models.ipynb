{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "By downloading and using the cuDNN conda packages, you accept the terms and conditions of the NVIDIA cuDNN EULA -\n",
      "  https://docs.nvidia.com/deeplearning/cudnn/sla/index.html\n",
      "\n",
      "done\n",
      "\u001b[33mWARNING: Skipping nilmtk as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping nilm_metadata as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/nilmtk/nilmtk@master\n",
      "  Cloning https://github.com/nilmtk/nilmtk (to revision master) to /tmp/pip-req-build-p3_9sev9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilmtk /tmp/pip-req-build-p3_9sev9\n",
      "  Resolved https://github.com/nilmtk/nilmtk to commit b2c514479cef478cab872cb635056da08d5352a1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilmtk\n",
      "  Building wheel for nilmtk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilmtk: filename=nilmtk-0.4.0.dev1+git.b2c5144-py3-none-any.whl size=279177 sha256=5184bfba1c66555fcd1289b3c7da0fd5d66d6d98e1b66da1026ac149229aa820\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ueafigc3/wheels/05/a4/8e/b767c3d1714f61fd30ec991c25780be8cc04c474da8205aeee\n",
      "Successfully built nilmtk\n",
      "Installing collected packages: nilmtk\n",
      "Successfully installed nilmtk-0.4.0.dev1+git.b2c5144\n",
      "Collecting git+https://github.com/nilmtk/nilm_metadata@master\n",
      "  Cloning https://github.com/nilmtk/nilm_metadata (to revision master) to /tmp/pip-req-build-rzn2f3on\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilm_metadata /tmp/pip-req-build-rzn2f3on\n",
      "  Resolved https://github.com/nilmtk/nilm_metadata to commit 7ed4bab9062d04cb35c6b6000b451715dc5ab4af\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilm-metadata\n",
      "  Building wheel for nilm-metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilm-metadata: filename=nilm_metadata-0.2.5-py3-none-any.whl size=24340 sha256=2f94f894a1a8c39394f2acd88a6cb250c45e5f1ab667235562791ae69e8a96e7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j3otgx2b/wheels/7e/2f/7f/e41975c202542dbc11228875ff8473ed4cb95e04a274222bb7\n",
      "Successfully built nilm-metadata\n",
      "Installing collected packages: nilm-metadata\n",
      "Successfully installed nilm-metadata-0.2.5\n"
     ]
    }
   ],
   "source": [
    "#!rm -r /home/jovyan/.local/share/jupyter/kernels/nilmtk\n",
    "!mamba install tensorflow-gpu==2.10 -y -q\n",
    "#!mamba install tensorflow-gpu==2.11.0 -y -q\n",
    "!pip uninstall -y -q nilmtk nilm_metadata\n",
    "#!pip install -q pandas networkx tables scikit-learn hmmlearn pyyaml matplotlib==3.1.3 xgboost pyts\n",
    "# Trick to install NILM regardless of its dependencies\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilm_metadata.git\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilmtk.git@0.4.3\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilmtk@master\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilm_metadata@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 08:29:43.147878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100 80GB PCIe MIG 3g.40gb, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from nilmtk import DataSet\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Memory\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "#from nilmtk import DataSet\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "!pip install -q scikit-learn-intelex\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "from sklearn import pipeline, metrics, linear_model, model_selection, multioutput, tree, ensemble, neural_network\n",
    "!pip install xgboost -q\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "#import graphviz\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from ann_visualizer.visualize import ann_viz\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix\n",
    "# GPU goes brrrrrrrrrrrrrrrrrrrr\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "!pip3 install tqdm -q\n",
    "from tqdm import tqdm\n",
    "import NUK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HolyDataset_UKD = pickle.load(open('./datasets/HolyDatasetDALE.pkl', 'rb'))\n",
    "HolyDataset_REF = pickle.load(open('./datasets/HolyDataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data for the neural network\n",
    "\n",
    "for dataset in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "epochs = 20\n",
    "window_size = 2700\n",
    "batch_size = 1024\n",
    "NmDevices = 84\n",
    "# table_of_options = [1,2,3,4]\n",
    "k = 0.25\n",
    "# Define a learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    return lr\n",
    "    if epoch < 200:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "test_size = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_UKD, y_UKD, labels_UKD = HolyDataset_UKD[0], HolyDataset_UKD[2], HolyDataset_UKD[4]\n",
    "# class_weights_UKD = NUK.class_weights_tool(y_UKD)\n",
    "\n",
    "# x_REF, y_REF, labels_REF = HolyDataset_REF[0], HolyDataset_REF[2], HolyDataset_REF[4]\n",
    "# class_weights_REF = NUK.class_weights_tool(y_REF)\n",
    "\n",
    "\n",
    "# # For UKD dataset\n",
    "# x_train_UKD, x_test_UKD, y_train_UKD, y_test_UKD = train_test_split(x_UKD, y_UKD, test_size=test_size, random_state=RANDOM_SEED)\n",
    "\n",
    "# # For REF dataset\n",
    "# x_train_REF, x_test_REF, y_train_REF, y_test_REF = train_test_split(x_REF, y_REF, test_size=test_size, random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle(\"../../Energy_graph/data/processed_all_X_Y_2700.pkl\")\n",
    "data = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/synthetic_test_100k.pkl\")\n",
    "labels = pd.read_pickle(\"../../Energy_graph/labeles.pkl\")\n",
    "# Separate the tuples into X and y\n",
    "# X_syn = np.array([i[0] for i in data_syn])\n",
    "# y_syn = np.array([i[1] for i in data_syn])\n",
    "\n",
    "X = np.array([i[0] for i in data])\n",
    "y = np.array([i[1] for i in data])\n",
    "\n",
    "# X = np.concatenate((X, X_syn), axis=0)\n",
    "# y = np.concatenate((y, y_syn), axis=0)\n",
    "\n",
    "# X_test = np.array([i[0] for i in data_test])\n",
    "# y_test = np.array([i[1] for i in data_test])\n",
    "evaluation_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [    0     1     2 ... 99996 99997 99998] Test indices: [    8    23    39 ... 99991 99993 99999]\n",
      "Epoch 1/20\n",
      "157/157 [==============================] - 8s 43ms/step - loss: 0.4120 - F1Score: 0.0012 - WeightedF1: 0.0011 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "  9/157 [>.............................] - ETA: 6s - loss: 0.3956 - F1Score: 0.0019 - WeightedF1: 0.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:11, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\foglo\\Documents\\IJS\\energy-knowledge-graph\\scripts\\notebooks\\multi-label classification\\models.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m LearningRateScheduler(scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# for class weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights_UKD, callbacks=[lr_scheduler])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, callbacks\u001b[39m=\u001b[39;49m[lr_scheduler])\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = NUK.PC0(NmDevices, window_size, 'LSTM', 128, k)\n",
    "    model.build((len(y_train) + len(y_test), window_size, 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    # for class weights\n",
    "    # model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights_UKD, callbacks=[lr_scheduler])\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 7s 26ms/step - loss: 0.2283 - F1Score: 8.1720e-04 - WeightedF1: 5.8214e-04 - lr: 3.0000e-04\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.2011 - F1Score: 0.0021 - WeightedF1: 0.0030 - lr: 3.0000e-04\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1980 - F1Score: 0.0074 - WeightedF1: 0.0370 - lr: 3.0000e-04\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1972 - F1Score: 0.0097 - WeightedF1: 0.0514 - lr: 3.0000e-04\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1963 - F1Score: 0.0103 - WeightedF1: 0.0545 - lr: 3.0000e-04\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1951 - F1Score: 0.0110 - WeightedF1: 0.0564 - lr: 3.0000e-04\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.1924 - F1Score: 0.0137 - WeightedF1: 0.0606 - lr: 3.0000e-04\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1888 - F1Score: 0.0232 - WeightedF1: 0.0691 - lr: 3.0000e-04\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1845 - F1Score: 0.0405 - WeightedF1: 0.0781 - lr: 3.0000e-04\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1790 - F1Score: 0.0629 - WeightedF1: 0.0967 - lr: 3.0000e-04\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.1720 - F1Score: 0.0949 - WeightedF1: 0.1253 - lr: 3.0000e-04\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1622 - F1Score: 0.1428 - WeightedF1: 0.1717 - lr: 3.0000e-04\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1442 - F1Score: 0.2348 - WeightedF1: 0.2688 - lr: 3.0000e-04\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.1135 - F1Score: 0.3932 - WeightedF1: 0.4387 - lr: 3.0000e-04\n",
      "Epoch 15/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0758 - F1Score: 0.5797 - WeightedF1: 0.6523 - lr: 3.0000e-04\n",
      "Epoch 16/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0455 - F1Score: 0.7096 - WeightedF1: 0.7846 - lr: 3.0000e-04\n",
      "Epoch 17/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0325 - F1Score: 0.7628 - WeightedF1: 0.8451 - lr: 3.0000e-04\n",
      "Epoch 18/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0211 - F1Score: 0.7988 - WeightedF1: 0.8808 - lr: 3.0000e-04\n",
      "Epoch 19/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0164 - F1Score: 0.8144 - WeightedF1: 0.9036 - lr: 3.0000e-04\n",
      "Epoch 20/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0131 - F1Score: 0.8242 - WeightedF1: 0.9107 - lr: 3.0000e-04\n",
      "Epoch 21/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0108 - F1Score: 0.8303 - WeightedF1: 0.9172 - lr: 3.0000e-04\n",
      "Epoch 22/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0094 - F1Score: 0.8332 - WeightedF1: 0.9208 - lr: 3.0000e-04\n",
      "Epoch 23/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0089 - F1Score: 0.8355 - WeightedF1: 0.9207 - lr: 3.0000e-04\n",
      "Epoch 24/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0081 - F1Score: 0.8387 - WeightedF1: 0.9269 - lr: 3.0000e-04\n",
      "Epoch 25/50\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.0073 - F1Score: 0.8392 - WeightedF1: 0.9293 - lr: 3.0000e-04\n",
      "Epoch 26/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0070 - F1Score: 0.8403 - WeightedF1: 0.9296 - lr: 3.0000e-04\n",
      "Epoch 27/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0064 - F1Score: 0.8429 - WeightedF1: 0.9289 - lr: 3.0000e-04\n",
      "Epoch 28/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0060 - F1Score: 0.8422 - WeightedF1: 0.9337 - lr: 3.0000e-04\n",
      "Epoch 29/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0093 - F1Score: 0.8347 - WeightedF1: 0.9200 - lr: 3.0000e-04\n",
      "Epoch 30/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0055 - F1Score: 0.8447 - WeightedF1: 0.9333 - lr: 3.0000e-04\n",
      "Epoch 31/50\n",
      "196/196 [==============================] - 5s 27ms/step - loss: 0.0051 - F1Score: 0.8458 - WeightedF1: 0.9347 - lr: 3.0000e-04\n",
      "Epoch 32/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0049 - F1Score: 0.8448 - WeightedF1: 0.9301 - lr: 3.0000e-04\n",
      "Epoch 33/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0048 - F1Score: 0.8485 - WeightedF1: 0.9364 - lr: 3.0000e-04\n",
      "Epoch 34/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0047 - F1Score: 0.8446 - WeightedF1: 0.9366 - lr: 3.0000e-04\n",
      "Epoch 35/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0045 - F1Score: 0.8471 - WeightedF1: 0.9377 - lr: 3.0000e-04\n",
      "Epoch 36/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0050 - F1Score: 0.8469 - WeightedF1: 0.9382 - lr: 3.0000e-04\n",
      "Epoch 37/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0043 - F1Score: 0.8469 - WeightedF1: 0.9329 - lr: 3.0000e-04\n",
      "Epoch 38/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0043 - F1Score: 0.8463 - WeightedF1: 0.9345 - lr: 3.0000e-04\n",
      "Epoch 39/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0053 - F1Score: 0.8474 - WeightedF1: 0.9306 - lr: 3.0000e-04\n",
      "Epoch 40/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0036 - F1Score: 0.8493 - WeightedF1: 0.9424 - lr: 3.0000e-04\n",
      "Epoch 41/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0039 - F1Score: 0.8481 - WeightedF1: 0.9383 - lr: 3.0000e-04\n",
      "Epoch 42/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0036 - F1Score: 0.8497 - WeightedF1: 0.9366 - lr: 3.0000e-04\n",
      "Epoch 43/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0036 - F1Score: 0.8494 - WeightedF1: 0.9401 - lr: 3.0000e-04\n",
      "Epoch 44/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0041 - F1Score: 0.8489 - WeightedF1: 0.9363 - lr: 3.0000e-04\n",
      "Epoch 45/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0034 - F1Score: 0.8494 - WeightedF1: 0.9389 - lr: 3.0000e-04\n",
      "Epoch 46/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0034 - F1Score: 0.8500 - WeightedF1: 0.9396 - lr: 3.0000e-04\n",
      "Epoch 47/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0033 - F1Score: 0.8492 - WeightedF1: 0.9358 - lr: 3.0000e-04\n",
      "Epoch 48/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0035 - F1Score: 0.8492 - WeightedF1: 0.9342 - lr: 3.0000e-04\n",
      "Epoch 49/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0043 - F1Score: 0.8475 - WeightedF1: 0.9360 - lr: 3.0000e-04\n",
      "Epoch 50/50\n",
      "196/196 [==============================] - 5s 26ms/step - loss: 0.0029 - F1Score: 0.8495 - WeightedF1: 0.9390 - lr: 3.0000e-04\n",
      "1490/1490 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model = NUK.PC0(NmDevices, window_size, 'LSTM', 128, k)\n",
    "model.build((len(y), window_size, 1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=[lr_scheduler])\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_tf = (y_pred > 0.5)\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "# report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "# report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "df = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fridge</th>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.171306</td>\n",
       "      <td>0.028475</td>\n",
       "      <td>467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.030666</td>\n",
       "      <td>0.097387</td>\n",
       "      <td>0.046644</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washing machine</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pc</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iron</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projector</th>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.049881</td>\n",
       "      <td>0.039886</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.050801</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.034044</td>\n",
       "      <td>172305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.055182</td>\n",
       "      <td>0.015145</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>172305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.387717</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.040033</td>\n",
       "      <td>172305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.019207</td>\n",
       "      <td>0.022482</td>\n",
       "      <td>172305.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 precision    recall  f1-score   support\n",
       "fridge            0.015528  0.171306  0.028475     467.0\n",
       "ac                0.030666  0.097387  0.046644     421.0\n",
       "washing machine   0.000000  0.000000  0.000000       0.0\n",
       "pc                0.000000  0.000000  0.000000      18.0\n",
       "iron              0.000000  0.000000  0.000000       0.0\n",
       "...                    ...       ...       ...       ...\n",
       "projector         0.033228  0.049881  0.039886     421.0\n",
       "micro avg         0.050801  0.025600  0.034044  172305.0\n",
       "macro avg         0.055182  0.015145  0.012648  172305.0\n",
       "weighted avg      0.387717  0.025600  0.040033  172305.0\n",
       "samples avg       0.032362  0.019207  0.022482  172305.0\n",
       "\n",
       "[88 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_staticLR_LSTM_combined.csv\")\n",
    "    \n",
    "# evaluation_results+ evaluation_results[1].to_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
