{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "By downloading and using the cuDNN conda packages, you accept the terms and conditions of the NVIDIA cuDNN EULA -\n",
      "  https://docs.nvidia.com/deeplearning/cudnn/sla/index.html\n",
      "\n",
      "done\n",
      "\u001b[33mWARNING: Skipping nilmtk as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping nilm_metadata as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/nilmtk/nilmtk@master\n",
      "  Cloning https://github.com/nilmtk/nilmtk (to revision master) to /tmp/pip-req-build-_zp7aika\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilmtk /tmp/pip-req-build-_zp7aika\n",
      "  Resolved https://github.com/nilmtk/nilmtk to commit b2c514479cef478cab872cb635056da08d5352a1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilmtk\n",
      "  Building wheel for nilmtk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilmtk: filename=nilmtk-0.4.0.dev1+git.b2c5144-py3-none-any.whl size=279177 sha256=1411a49bc8b72829ea0152ebdc4e1c06b9c1975efb0d8e52d10fb07599a01557\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-iz_qui_7/wheels/05/a4/8e/b767c3d1714f61fd30ec991c25780be8cc04c474da8205aeee\n",
      "Successfully built nilmtk\n",
      "Installing collected packages: nilmtk\n",
      "Successfully installed nilmtk-0.4.0.dev1+git.b2c5144\n",
      "Collecting git+https://github.com/nilmtk/nilm_metadata@master\n",
      "  Cloning https://github.com/nilmtk/nilm_metadata (to revision master) to /tmp/pip-req-build-6x_hthny\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilm_metadata /tmp/pip-req-build-6x_hthny\n",
      "  Resolved https://github.com/nilmtk/nilm_metadata to commit 7ed4bab9062d04cb35c6b6000b451715dc5ab4af\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilm-metadata\n",
      "  Building wheel for nilm-metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilm-metadata: filename=nilm_metadata-0.2.5-py3-none-any.whl size=24340 sha256=34717d41b4ad95e0f75ee8c495d4b72705404e28d34c71e4a9546ce8b8a70405\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zqwbz1jg/wheels/7e/2f/7f/e41975c202542dbc11228875ff8473ed4cb95e04a274222bb7\n",
      "Successfully built nilm-metadata\n",
      "Installing collected packages: nilm-metadata\n",
      "Successfully installed nilm-metadata-0.2.5\n"
     ]
    }
   ],
   "source": [
    "#!rm -r /home/jovyan/.local/share/jupyter/kernels/nilmtk\n",
    "!mamba install tensorflow-gpu==2.10 -y -q\n",
    "# !mamba install tensorflow-gpu==2.11.0 -y -q\n",
    "!pip uninstall -y -q nilmtk nilm_metadata\n",
    "#!pip install -q pandas networkx tables scikit-learn hmmlearn pyyaml matplotlib==3.1.3 xgboost pyts\n",
    "# Trick to install NILM regardless of its dependencies\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilm_metadata.git\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilmtk.git@0.4.3\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilmtk@master\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilm_metadata@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 11:34:35.424528: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100 80GB PCIe MIG 3g.40gb, compute capability 8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from nilmtk import DataSet\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Memory\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "#from nilmtk import DataSet\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "!pip install -q scikit-learn-intelex\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "from sklearn import pipeline, metrics, linear_model, model_selection, multioutput, tree, ensemble, neural_network\n",
    "!pip install xgboost -q\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "#import graphviz\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from ann_visualizer.visualize import ann_viz\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix\n",
    "# GPU goes brrrrrrrrrrrrrrrrrrrr\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "!pip3 install tqdm -q\n",
    "from tqdm.notebook import tqdm\n",
    "import NUK\n",
    "\n",
    "# import garbage collector\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(physical_devices)\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU found, model will train on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "epochs =1200\n",
    "window_size = 2688\n",
    "batch_size = 512\n",
    "NmDevices = 64\n",
    "# table_of_options = [1,2,3,4]\n",
    "k = 0.125\n",
    "# Define a learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    return lr\n",
    "    if epoch == 0 or epoch == 1:\n",
    "        return lr\n",
    "    if epoch == 35:\n",
    "        return lr *0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "test_size = 0.2\n",
    "lambda_l2=0\n",
    "function = \"GRU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_100000_upper32_gap3600_numD64_ideal.pkl\")\n",
    "# data = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_50000_upper32_gap3600_numD64_ideal.pkl\") \n",
    "# data_syn_ideal = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_50000_upper32_gap3600_numD64_ideal.pkl\") \n",
    "# data_syn_unmetered = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_50000_upper32_gap3600_numD64_unmetered.pkl\")\n",
    "\n",
    "labels = pd.read_pickle(\"../../Energy_graph/data/labels_new.pkl\")\n",
    "\n",
    "\n",
    "# Separate the tuples into X and y\n",
    "# X_syn_ideal = np.array([i[0] for i in data_syn_ideal])\n",
    "# y_syn_ideal = np.array([i[1] for i in data_syn_ideal])\n",
    "\n",
    "# X_syn_unmetered = np.array([i[0] for i in data_syn_unmetered])\n",
    "# y_syn_unmetered = np.array([i[1] for i in data_syn_unmetered])\n",
    "\n",
    "X = np.array([i[0] for i in data])\n",
    "y = np.array([i[1] for i in data])\n",
    "\n",
    "\n",
    "# X = np.concatenate((X, X_syn_ideal,X_syn_unmetered), axis=0)\n",
    "# y = np.concatenate((y, y_syn_ideal, y_syn_unmetered), axis=0)\n",
    "\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "# X_test = np.array([i[0] for i in data_test])\n",
    "# y_test = np.array([i[1] for i in data_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(X):\n",
    "    max_value = 0\n",
    "\n",
    "    for x in X:\n",
    "        v = np.max(x)\n",
    "        if v > max_value:\n",
    "            max_value = v\n",
    "\n",
    "    if max_value == 0:\n",
    "        return X\n",
    "    return X / max_value\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionTime class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2= pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_100000_upper32_gap3600_numD64_ideal.pkl\")\n",
    "# y2 = np.array([i[1] for i in data])\n",
    "class_weighs_pre = NUK.class_weights_tool(y)\n",
    "\n",
    "class Classifier_INCEPTION:\n",
    "\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, iteration, verbose=False, build=True, batch_size=64,\n",
    "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500, lr=0.001):\n",
    "\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "        self.nb_filters = nb_filters\n",
    "        self.use_residual = use_residual\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size - 1\n",
    "        self.callbacks = None\n",
    "        self.batch_size = batch_size\n",
    "        self.bottleneck_size = 32\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.iteration = iteration\n",
    "\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            # if (verbose == True):\n",
    "                # self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "\n",
    "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
    "\n",
    "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
    "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
    "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
    "\n",
    "        conv_list = []\n",
    "\n",
    "        for i in range(len(kernel_size_s)):\n",
    "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
    "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
    "                input_inception))\n",
    "\n",
    "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
    "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_6)\n",
    "\n",
    "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
    "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
    "                                         padding='same', use_bias=False)(input_tensor)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        x = input_layer\n",
    "        input_res = input_layer\n",
    "\n",
    "        for d in range(self.depth):\n",
    "\n",
    "            x = self._inception_module(x)\n",
    "\n",
    "            if self.use_residual and d % 3 == 2:\n",
    "                x = self._shortcut_layer(input_res, x)\n",
    "                input_res = x\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='sigmoid')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
    "                      metrics=['accuracy', NUK.F1Score, NUK.WeightedF1Score(class_weighs_pre)])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=7, min_lr=0.0001)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=9, restore_best_weights=True, monitor='loss')\n",
    "\n",
    "        file_path = self.output_directory + f'best_model_{self.iteration}.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "        # lr_scheduler = LearningRateScheduler(scheduler)\n",
    "        self.callbacks = [reduce_lr, model_checkpoint, early_stopping]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "       \n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
    "        else:\n",
    "            mini_batch_size = self.batch_size\n",
    "\n",
    "        self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, callbacks=self.callbacks, validation_split=0.1)\n",
    "\n",
    "\n",
    "        self.model.save(self.output_directory + f'last_model_{self.iteration}.hdf5')\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        model_path = self.output_directory + f'last_model_{self.iteration}.hdf5'\n",
    "        model = keras.models.load_model(model_path, custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
    "    #     # if return_df_metrics:\n",
    "    #     #     y_pred = np.argmax(y_pred, axis=1)\n",
    "    #     #     df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
    "    #     #     return df_metrics\n",
    "    #     # else:\n",
    "    #     #     test_duration = time.time() - start_time\n",
    "    #     #     save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
    "    #     #     return y_pred\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "itr = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    # print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    # model = NUK.PC0_reg(NmDevices, window_size, function, 128, k, lambda_l2=lambda_l2)\n",
    "    print(batch_size, epochs)\n",
    "    model = Classifier_INCEPTION(output_directory=\"./models/\", input_shape=(window_size, 1), nb_classes=78, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=6)\n",
    "    model.fit(X_train, y_train)\n",
    "    # model.model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test, y_pred))\n",
    "    t = (y_pred_tf, y_test, y_pred)\n",
    "    with open(f\"./predictions/test_inception_pickle_{itr}.pkl\", 'wb') as f:\n",
    "        pickle.dump(t, f)\n",
    "\n",
    "    itr += 1\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/Inception_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\foglo\\Documents\\IJS\\energy-knowledge-graph\\scripts\\notebooks\\multi-label classification\\models.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m evaluation_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m predictions \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m class_weights_all \u001b[39m=\u001b[39m NUK\u001b[39m.\u001b[39;49mclass_weights_tool(y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold,(train_index, test_index) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(kf\u001b[39m.\u001b[39msplit(X))):\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# print(\"Train indices:\", train_index, \"Test indices:\", test_index)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m X[train_index], X[test_index]\n",
      "File \u001b[0;32m~/shared/to_vid/NUK.py:550\u001b[0m, in \u001b[0;36mclass_weights_tool\u001b[0;34m(y_test)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39m# this loop goes over the rows in the y_test dataset\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(y_test)):\n\u001b[1;32m    548\u001b[0m     \n\u001b[1;32m    549\u001b[0m     \u001b[39m# we count Trues in the whole column of y_test dataset\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mif\u001b[39;00m y_test[i][j] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m: count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m     \n\u001b[1;32m    552\u001b[0m \u001b[39m# we append Trues for the column of y_test dataset to the list light_weight\u001b[39;00m\n\u001b[1;32m    553\u001b[0m light_weight\u001b[39m.\u001b[39mappend(count)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_splits = 3\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "for fold,(train_index, test_index) in tqdm(enumerate(kf.split(X))):\n",
    "    # print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # normalization\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    model_predictions = []\n",
    "\n",
    "    print(batch_size, epochs)\n",
    "    for i in range(5):\n",
    "\n",
    "        model = Classifier_INCEPTION(output_directory=f\"./models/{fold}/\", input_shape=(window_size, 1), nb_classes=78, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=8, iteration=i)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(X_test)\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    y_pred = np.mean(model_predictions, axis=0)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test, y_pred))\n",
    "    t = (y_pred_tf, y_test, y_pred)\n",
    "    with open(f\"./predictions/test_inception_pickle_{fold}.pkl\", 'wb') as f:\n",
    "        pickle.dump(t, f)\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    \n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / num_splits\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_3fold.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "model_predictions = []\n",
    "\n",
    "print(batch_size, epochs)\n",
    "for i in range(5):\n",
    "\n",
    "    model = Classifier_INCEPTION(output_directory=f\"./models/performance/\", input_shape=(window_size, 1), nb_classes=NmDevices, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=10, iteration=i, kernel_size=128, kernel_size=128)\n",
    "    # model = keras.models.load_model(f\"./models/test/last_model_{i}.hdf5\", custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_predictions.append(y_pred)\n",
    "\n",
    "y_pred = np.mean(model_predictions, axis=0)\n",
    "y_pred_tf = (y_pred > 0.3)\n",
    "\n",
    "predictions.append((y_pred_tf, y_test, y_pred))\n",
    "t = (y_pred_tf, y_test, y_pred)\n",
    "with open(f\"./predictions/test_inception_pickle_performance_testing.pkl\", 'wb') as f:\n",
    "    pickle.dump(t, f)\n",
    "\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "print(report[\"weighted avg\"])\n",
    "\n",
    "evaluation_results.append(report)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_performance_testing.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 5\n",
      "Epoch 1/5\n",
      "207/207 [==============================] - 330s 2s/step - loss: 0.3249 - accuracy: 0.0859 - F1Score: 0.0203 - WeightedF1: 0.1213 - val_loss: 0.3521 - val_accuracy: 5.9544e-04 - val_F1Score: 0.0026 - val_WeightedF1: 0.0015 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2847 - accuracy: 0.1037 - F1Score: 0.0501 - WeightedF1: 0.2598 - val_loss: 0.3316 - val_accuracy: 0.0115 - val_F1Score: 0.0057 - val_WeightedF1: 0.0091 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2765 - accuracy: 0.1190 - F1Score: 0.0766 - WeightedF1: 0.3217 - val_loss: 0.3225 - val_accuracy: 0.0249 - val_F1Score: 0.0224 - val_WeightedF1: 0.0388 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2699 - accuracy: 0.1256 - F1Score: 0.1004 - WeightedF1: 0.3611 - val_loss: 0.2979 - val_accuracy: 0.0947 - val_F1Score: 0.1027 - val_WeightedF1: 0.2987 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2641 - accuracy: 0.1311 - F1Score: 0.1260 - WeightedF1: 0.3894 - val_loss: 0.3055 - val_accuracy: 0.0885 - val_F1Score: 0.1028 - val_WeightedF1: 0.3224 - lr: 0.0010\n",
      "58/58 [==============================] - 20s 340ms/step\n",
      "{'precision': 0.3441303811150662, 'recall': 0.18797617912282374, 'f1-score': 0.18487712925876545, 'support': 188742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:/tmp/nilmtk-jgsyvjt5.h5...done\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "window_size = 2688\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "model_predictions = []\n",
    "\n",
    "print(batch_size, epochs)\n",
    "for i in range(1):\n",
    "\n",
    "    model = Classifier_INCEPTION(output_directory=f\"./models/performance/\", input_shape=(window_size, 1), nb_classes=NmDevices, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=10, iteration=i, kernel_size=128)\n",
    "    # model = keras.models.load_model(f\"./models/test/last_model_{i}.hdf5\", custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_predictions.append(y_pred)\n",
    "\n",
    "y_pred = np.mean(model_predictions, axis=0)\n",
    "y_pred_tf = (y_pred > 0.3)\n",
    "\n",
    "predictions.append((y_pred_tf, y_test, y_pred))\n",
    "t = (y_pred_tf, y_test, y_pred)\n",
    "with open(f\"./predictions/test_inception_pickle_performance_testing.pkl\", 'wb') as f:\n",
    "    pickle.dump(t, f)\n",
    "\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "print(report[\"weighted avg\"])\n",
    "\n",
    "evaluation_results.append(report)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_performance_testing.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = keras.layers.Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv1D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(y)\n",
    "    out = keras.layers.Dense(nb_classes, activation='sigmoid')(full)\n",
    "    print ('        -- model was built.')\n",
    "    # input = keras.layers.Input(shape=(input_shape))\n",
    "    model = keras.models.Model(inputs=x, outputs=out, name=\"Resnet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "treshold = 0.5\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train[:, 0] = normalize(X_train[:, 0])\n",
    "    X_test[:,0] = normalize(X_test[:, 0])\n",
    "\n",
    "    model = build_resnet((window_size, 1), 32, NmDevices)\n",
    "    # model.build((len(y_train) + len(y_test), window_size, 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=3, min_lr=0.000001)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True, monitor='loss')\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduce_lr,early_stopping])\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > treshold)\n",
    "\n",
    "    predictions.append((y_pred, y_pred_tf, y_test))\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"], \"|||\", report[\"samples avg\"])\n",
    "\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "df = df / num_splits\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/ResNet_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_mixed_t0.3_d10.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PC0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "treshold = 0.5\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train[:, 0] = normalize(X_train[:, 0])\n",
    "    X_test[:,0] = normalize(X_test[:, 0])\n",
    "\n",
    "    # break\n",
    "    # class_weights_all = NUK.class_weights_tool(y_train)\n",
    "    # for k in class_weights_all:\n",
    "    #     class_weights_all[k] += 1\n",
    "    \n",
    "    # print(class_weights_all)\n",
    "    # break\n",
    "    model = NUK.PC0_reg(NmDevices, window_size, function, 128, k, lambda_l2=lambda_l2)\n",
    "    # model = NUK.VGG11_1D(NmDevices, window_size)\n",
    "    model.build((len(y_train) + len(y_test), window_size, 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "    # lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=3, min_lr=0.000001)\n",
    "    # callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1,patience=2, min_lr=0.0000002)\n",
    "    # for class weights\n",
    "    # model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights_UKD, callbacks=[lr_scheduler])\n",
    "    # class_weights = NUK.class_weights_tool(y_train)\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduce_lr])\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > treshold)\n",
    "\n",
    "    predictions.append((y_pred, y_pred_tf, y_test))\n",
    "\n",
    "    # n_labels = y_test.shape[1]\n",
    "    # co_occurrence_matrix = np.zeros((n_labels, n_labels))\n",
    "\n",
    "    # for true, pred in zip(y_test, y_pred_tf):\n",
    "    #     fn_labels = np.where((true == 1) & (pred == 0))[0]  # False negatives\n",
    "    #     fp_labels = np.where((true == 0) & (pred == 1))[0]  # False positives\n",
    "\n",
    "    #     for fn in fn_labels:\n",
    "    #         for fp in fp_labels:\n",
    "    #             co_occurrence_matrix[fn, fp] += 1\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"], \"|||\", report[\"samples avg\"])\n",
    "\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSMixer\n",
    "https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement TSMixer for multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [     1      2      3 ... 147053 147054 147055] Test indices: [     0      4     12 ... 147049 147050 147052]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "jobs = 110\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize the Random Forest classifier wrapped in a MultiOutputClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=jobs)\n",
    "    # classifier = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "    evaluation_results.append(report)\n",
    "\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "# df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_staticLR_LSTM_watts_mixed100k.csv\")\n",
    "df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_classes{NmDevices}_watts_RF_mixed50k.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "model = NUK.PC0(NmDevices, window_size, 'GRU',128, k)\n",
    "model.build((len(y), window_size, 1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=[lr_scheduler])\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_tf = (y_pred > 0.5)\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "# report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "# report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "df = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Using cached catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.5.2)\n",
      "Collecting plotly\n",
      "  Using cached plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.9.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2022.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.0.6)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, graphviz, plotly, catboost\n",
      "Successfully installed catboost-1.2.2 graphviz-0.20.1 plotly-5.18.0 tenacity-8.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "import catboost as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2580415ae82e4ad7b6557595d4fec752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [    0     1     3 ... 99996 99997 99999] Test indices: [    2    14    15 ... 99988 99995 99998]\n",
      "Learning rate set to 0.049298\n",
      "0:\tlearn: 0.6559391\ttotal: 545ms\tremaining: 4m 31s\n",
      "499:\tlearn: 0.3240652\ttotal: 3m 46s\tremaining: 0us\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "{'precision': 0.33921599749844483, 'recall': 0.03869671504015228, 'f1-score': 0.05241679545629883, 'support': 160763}\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED=32\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    # break\n",
    "\n",
    "    # Initialize CatBoost multilabel classifier\n",
    "    model = cb.CatBoostClassifier(\n",
    "        loss_function='MultiLogloss',\n",
    "        verbose=500,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        task_type=\"GPU\",\n",
    "        devices='0:1',\n",
    "        # class_weights=class_weights_all,\n",
    "        iterations=500,\n",
    "        \n",
    "\n",
    "        )  # Add other hyperparameters if needed\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert predictions to boolean format similar to y_test for evaluation\n",
    "    y_pred_tf = (y_pred == 1)\n",
    "    print(y_pred)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    break\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.4.44)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.7.0 optuna-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 11:52:36,241] A new study created in memory with name: no-name-d246abf3-585a-46f8-b674-167c2bf5e652\n",
      "[W 2023-11-24 12:00:51,716] Trial 0 failed with parameters: {'iterations': 939, 'depth': 7, 'learning_rate': 0.2795703639511178} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_57/2536419308.py\", line 26, in objective\n",
      "    score = cross_val_score(model, X_train_norm, y_train, cv=5, scoring='f1_weighted').mean()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/catboost/core.py\", line 5100, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/catboost/core.py\", line 2319, in _fit\n",
      "    self._train(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/catboost/core.py\", line 1723, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4645, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4694, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-24 12:00:51,722] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\foglo\\Documents\\IJS\\energy-knowledge-graph\\scripts\\notebooks\\multi-label classification\\models.ipynb Cell 38\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m score\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)  \u001b[39m# Adjust the number of trials\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials))\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m, study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\foglo\\Documents\\IJS\\energy-knowledge-graph\\scripts\\notebooks\\multi-label classification\\models.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model \u001b[39m=\u001b[39m CatBoostClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m X_train_norm \u001b[39m=\u001b[39m normalize(X_train)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m score \u001b[39m=\u001b[39m cross_val_score(model, X_train_norm, y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mf1_weighted\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5098\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[1;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[1;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[1;32m   5103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2320\u001b[0m         train_pool,\n\u001b[1;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2322\u001b[0m         params,\n\u001b[1;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import logging\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import metrics\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "RANDOM_SEED = 32\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# Configure logging to file\n",
    "logging.basicConfig(filename='../../Energy_graph/data/optuna_log.log', level=logging.INFO, \n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "# Enable logging at the INFO level\n",
    "optuna.logging.get_logger(\"optuna\").setLevel(logging.INFO)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        # Add other parameters here\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'loss_function': 'MultiLogloss',\n",
    "        'verbose': False,\n",
    "        'task_type': \"GPU\",\n",
    "        'devices': '0:1'\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**param)\n",
    "    X_train_norm = normalize(X_train)\n",
    "    score = cross_val_score(model, X_train_norm, y_train, cv=5, scoring='f1_weighted').mean()\n",
    "    return score\n",
    "\n",
    "# Define a simple callback function to print the best value so far\n",
    "def callback(study, trial):\n",
    "    if study.best_trial == trial:\n",
    "        print(f\"New best value: {trial.value:.4f}\")\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, callbacks=[callback])  # Adjust the number of trials\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_model = CatBoostClassifier(**best_params)\n",
    "best_model.fit(normalize(X_train), y_train)\n",
    "\n",
    "# Evaluate the model on the holdout validation set\n",
    "X_val_norm = normalize(X_val)\n",
    "val_predictions = best_model.predict(X_val_norm)\n",
    "val_score = metrics.f1_score(y_val, val_predictions, average='weighted')\n",
    "print(\"Validation F1 Score:\", val_score)\n",
    "\n",
    "# Plot the optimization history\n",
    "plot_optimization_history(study)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/PC0_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_mix.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "# df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_classes{NmDevices}_watts_RF_mixed50k.csv\")\n",
    "    \n",
    "# evaluation_results+ evaluation_results[1].to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2688, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 2688, 1)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 2688, 32)     1280        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 2688, 32)     640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 2688, 32)     320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 2688, 32)     32          ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2688, 128)    0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 2688, 128)   512         ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 2688, 128)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 2688, 32)     4096        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 2688, 128)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 2688, 32)     40960       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 2688, 32)     20480       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 2688, 32)     10240       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 2688, 32)     4096        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2688, 128)    0           ['conv1d_5[0][0]',               \n",
      "                                                                  'conv1d_6[0][0]',               \n",
      "                                                                  'conv1d_7[0][0]',               \n",
      "                                                                  'conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 2688, 128)   512         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 2688, 128)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 2688, 32)     4096        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 2688, 128)    0           ['conv1d_10[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]',              \n",
      "                                                                  'conv1d_12[0][0]',              \n",
      "                                                                  'conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 2688, 128)    128         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 2688, 128)   512         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 2688, 128)   512         ['conv1d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 2688, 128)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 2688, 128)    0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 2688, 128)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 2688, 32)     4096        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 2688, 128)    0           ['conv1d_16[0][0]',              \n",
      "                                                                  'conv1d_17[0][0]',              \n",
      "                                                                  'conv1d_18[0][0]',              \n",
      "                                                                  'conv1d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 2688, 128)   512         ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 2688, 128)    0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 2688, 32)     4096        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 2688, 128)    0           ['conv1d_21[0][0]',              \n",
      "                                                                  'conv1d_22[0][0]',              \n",
      "                                                                  'conv1d_23[0][0]',              \n",
      "                                                                  'conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 2688, 128)   512         ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 2688, 128)    0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 2688, 32)     4096        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 2688, 128)    0           ['conv1d_26[0][0]',              \n",
      "                                                                  'conv1d_27[0][0]',              \n",
      "                                                                  'conv1d_28[0][0]',              \n",
      "                                                                  'conv1d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 2688, 128)    16384       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 2688, 128)   512         ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 2688, 128)   512         ['conv1d_30[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 2688, 128)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 2688, 128)    0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 2688, 128)    0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 2688, 32)     4096        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 2688, 128)    0           ['conv1d_32[0][0]',              \n",
      "                                                                  'conv1d_33[0][0]',              \n",
      "                                                                  'conv1d_34[0][0]',              \n",
      "                                                                  'conv1d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 2688, 128)   512         ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 2688, 128)    0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 2688, 32)     4096        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_7[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 2688, 128)    0           ['conv1d_37[0][0]',              \n",
      "                                                                  'conv1d_38[0][0]',              \n",
      "                                                                  'conv1d_39[0][0]',              \n",
      "                                                                  'conv1d_40[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 2688, 128)   512         ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 2688, 128)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)             (None, 2688, 32)     4096        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_42 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_45 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 2688, 128)    0           ['conv1d_42[0][0]',              \n",
      "                                                                  'conv1d_43[0][0]',              \n",
      "                                                                  'conv1d_44[0][0]',              \n",
      "                                                                  'conv1d_45[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_46 (Conv1D)             (None, 2688, 128)    16384       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 2688, 128)   512         ['concatenate_8[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 2688, 128)   512         ['conv1d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 2688, 128)    0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 2688, 128)    0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 2688, 128)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)             (None, 2688, 32)     4096        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_48 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_49 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_50 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_51 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2688, 128)    0           ['conv1d_48[0][0]',              \n",
      "                                                                  'conv1d_49[0][0]',              \n",
      "                                                                  'conv1d_50[0][0]',              \n",
      "                                                                  'conv1d_51[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 2688, 128)   512         ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 2688, 128)    0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['activation_12[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 768,928\n",
      "Trainable params: 765,600\n",
      "Non-trainable params: 3,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"./models/test/last_model_4.hdf5\", custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../Energy_graph/data/processed_watts/REFIT_clean.pkl\")\n",
    "df[\"REFIT_1\"]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../../Energy_graph/data/processed_watts/refit/REFIT1_clean.pkl\", 'wb') as f:\n",
    "    pickle.dump({\"REFIT_1\" : df[\"REFIT_1\"]}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "# from helper import preprocess_string\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_string(string : str) -> str:\n",
    "    string = string.lower().strip()\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    string = string.replace(\"-\", \" \")\n",
    "    string = string.replace(\"&\", \" \")\n",
    "    string = string.split(\"(\")[0]\n",
    "    string = string.split(\"#\")[0]\n",
    "\n",
    "    string = string.strip()\n",
    "\n",
    "    # handle known synoynms\n",
    "    synonyms = {\n",
    "        \"refrigerator\": \"fridge\",\n",
    "        \"vaccumcleaner\": \"vacuum cleaner\",\n",
    "        \"breadmaker\": \"bread maker\",\n",
    "      \n",
    "        \n",
    "    }\n",
    "    if \"freezer\" in string:\n",
    "        string = \"fridge\"\n",
    "\n",
    "    if string in synonyms:\n",
    "        string = synonyms[string]\n",
    "\n",
    "    if 'hi fi' in string:\n",
    "        string = \"audio system\"\n",
    "\n",
    "    if \"router\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    if \"treadmill\" in string:\n",
    "        string = \"running machine\"\n",
    "        \n",
    "\n",
    "    if \"laptop\" in string:\n",
    "        string = \"laptop\"\n",
    "    \n",
    "    if \"server\" in string:\n",
    "        string = \"server\"\n",
    "\n",
    "    if \"monitor\" in string and not \"baby\" in string:\n",
    "        string = \"monitor\"\n",
    "    # special cases\n",
    "    if \"computer\" in string and \"charger\" not in string:\n",
    "        string = \"pc\"\n",
    "\n",
    "    if \"tv\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"television\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"macbook\" in string:\n",
    "        string = \"laptop\"\n",
    "        \n",
    "    if \"car charger\" == string:\n",
    "        string = \"ev\"\n",
    "    \n",
    "    if \"toast\" in string:\n",
    "        string = \"toaster\"\n",
    "    \n",
    "    if \"modem\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    # we treat all audio devices as speakers so subwoofer is also a speaker\n",
    "    if \"subwoofer\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"speaker\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"iron\" in string and \"soldering\" not in string:\n",
    "        string = \"iron\"\n",
    "\n",
    "    \n",
    "    if \"coffeemachine\" in string:\n",
    "        string = \"coffee machine\"\n",
    "    if \"coffee maker\" in string:\n",
    "        string = \"coffee machine\"\n",
    "\n",
    "    if \"dishwasher\" in string:\n",
    "        string = \"dish washer\"\n",
    "    if \"air conditioner\" in string:\n",
    "        string = \"ac\"\n",
    "\n",
    "    if \"air conditioning\" in string:\n",
    "        string = \"ac\"\n",
    "    \n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = re.sub(r'\\d+', '', string)\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_dictionary(data: dict, values=0) -> pd.DataFrame:\n",
    "\n",
    "    ignored_devices = [\n",
    "        \"light\",\n",
    "        \"outlet\",\n",
    "        \"sockets\",\n",
    "        \"lamp\",\n",
    "        \"plug\",\n",
    "        'CE appliance'\n",
    "        'kettle/toaster',\n",
    "        'dehumidifier/heater',\n",
    "        'HairDryer-Straightener',\n",
    "        'Office Desk',\n",
    "        'heat basement',\n",
    "        'set top box',\n",
    "        'subpanel',\n",
    "    ]\n",
    "    dfs = []\n",
    "    for device in data:\n",
    "        # ignore devices\n",
    "        if any(ignored_device in device.lower() for ignored_device in ignored_devices):\n",
    "            continue\n",
    "        if device == \"aggregate\":\n",
    "            continue\n",
    "        # preprocess device name\n",
    "        device_name = preprocess_string(device)\n",
    "        \n",
    "        df = data[device]\n",
    "        df = df.resample(\"8s\").mean()\n",
    "\n",
    "        # rename column to standardized device name\n",
    "        df.columns = [device_name]\n",
    "        if df.max().max() < 2:\n",
    "            print(\"device with zeros: \", device_name)\n",
    "            continue\n",
    "\n",
    "        time_diffs = df.index.to_series().diff()\n",
    "        median_interval = time_diffs.median()\n",
    "\n",
    "        # if there is less than 3 days of data drop the device\n",
    "        if len(df) < (3*24 * 60 * 60) / median_interval.total_seconds():\n",
    "            print(\"less than 3 days of data for device: \", device_name)\n",
    "            continue\n",
    "        df.dropna(inplace=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # concatenate all dataframes\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # handle missing values\n",
    "    df = df.ffill(limit=6)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # handle negative values\n",
    "    df[df<0] = 0\n",
    "\n",
    "    df[\"aggregate\"] = df.sum(axis=1)\n",
    "    # df.drop(columns=[\"aggregate\"], inplace=True)\n",
    "\n",
    "    # df.rename(columns={\"sum_ideal\": \"aggregate\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # treshold in watts\n",
    "    treshold = 5\n",
    "    if values == 0:\n",
    "        # put 1 if device is on and 0 if device is off\n",
    "        for c in df.columns:\n",
    "            if c == \"aggregate\":\n",
    "                continue\n",
    "            # if power is less than treshold device is off\n",
    "            df[c] = (df[c] > treshold).astype(int)\n",
    "\n",
    "    # find duplicate columns\n",
    "    column_counts = Counter(df.columns)\n",
    "    duplicates = [col for col, count in column_counts.items() if count > 1]\n",
    "    # Sum duplicate columns\n",
    "    for duplicate in duplicates:\n",
    "        duplicate_cols = [col for i, col in enumerate(df.columns) if col == duplicate]\n",
    "        df[duplicate] = df[duplicate_cols].sum(axis=1)\n",
    "        # Drop other duplicate columns if needed\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep='last')]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def process_dataset(dataset_path):\n",
    "    data = pd.read_pickle(dataset_path)\n",
    "    # print(dataset_path)\n",
    "    for house in data:\n",
    "        data[house] = process_dictionary(data[house], house)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data(path : str, labels_path : str, values=0):\n",
    "        \n",
    "    # path = \"./Energy_graph/data/processed_watts/\"\n",
    "    dataset_paths = [os.path.join(path, dataset) for dataset in os.listdir(path) if dataset.endswith('.pkl')]\n",
    "        \n",
    "    cpu_count = int(os.cpu_count() / 2)\n",
    "    data_dict = {}\n",
    "\n",
    "    with tqdm(total=len(dataset_paths), desc=\"Processing datasets\", unit=\"dataset\") as progress_bar:\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count) as executor:\n",
    "            futures = {executor.submit(process_dataset, dataset_path): dataset_path for dataset_path in dataset_paths}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                dataset_path = futures[future]\n",
    "                try:\n",
    "                    processed_data = future.result()\n",
    "                    data_dict.update(processed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Dataset {dataset_path} generated an exception: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "\n",
    "    labels = pd.read_pickle(labels_path)\n",
    "    labels.sort()\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def create_windows(data : dict, labels_path: str, save_path : str, time_window=2700, upper_bound=pd.Timedelta(seconds=32), max_gap = pd.Timedelta(seconds=3600)):\n",
    "    \"\"\"Creates windows of time_window seconds from the data and discards windows with gaps of more than 1h or 15 gaps of 32 seconds or more\"\"\"\n",
    "    \n",
    "    labels = pd.read_pickle(labels_path)\n",
    "    # windows = []\n",
    "    X_Y = [] # list of tuples (X, Y)\n",
    "    skip_count_1 = 0\n",
    "    skip_count_2 = 0\n",
    "    skip_count_3 = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for df in data.values():\n",
    "        print(len(df[\"aggregate\"]))\n",
    "        for i in range(0, len(df) - time_window, time_window + 1):\n",
    "            window = df.iloc[i:i + time_window]\n",
    "            total_count += 1\n",
    "            # if there is a gap of more than max_gap skip the window\n",
    "            time_diffs = window.index.to_series().diff().dropna()\n",
    "            if  (time_diffs >= max_gap).any():\n",
    "                skip_count_1 += 1\n",
    "                continue\n",
    "            # if there are more than 15 gaps of upper_bound or more skip the window\n",
    "            if len(time_diffs[time_diffs > upper_bound]) > 15:\n",
    "                skip_count_2 += 1\n",
    "                continue\n",
    "\n",
    "            x = window[\"aggregate\"].values\n",
    "            # if there is a value bigger than 50000 skip the window\n",
    "            if (x > 50000).any():\n",
    "                skip_count_3 += 1\n",
    "                continue\n",
    "            devices = [False] * len(labels)\n",
    "            # check if device is on in the window\n",
    "            for c in window.columns:\n",
    "                if c == \"aggregate\":\n",
    "                    continue\n",
    "                on = (window[c] > 0)\n",
    "                ix = labels.index(c)\n",
    "                devices[ix] = on.any()\n",
    "\n",
    "            X_Y.append((x, devices))\n",
    "            \n",
    "\n",
    "\n",
    "            # windows.append(window)\n",
    "    print(\"Total windows: \", total_count, \"Skipped windows due to 30min gap: \", skip_count_1, \"Skipped windows due to 15 gaps of 32s or more: \", skip_count_2 ,\"Skipped windows due to values larger than 50k: \", skip_count_3 ,\"Procentage skipped: \", (skip_count_1+skip_count_2+ skip_count_3) / total_count * 100)\n",
    "    return X_Y\n",
    "    # with open(save_path+ f\"/X_Y_wsize{time_window}_upper{int(upper_bound.total_seconds())}_gap{int(max_gap.total_seconds())}_numD{len(labels)}.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(X_Y, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to base folder:  ../../Energy_graph/\n",
      "Path to save windows:  ../../Energy_graph//data/training_data/processed/\n",
      "Time window:  2688 rows |  21504 seconds\n",
      "Upper bound:  0 days 00:00:32\n",
      "Max gap:  0 days 01:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/10 [00:00<?, ?dataset/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  50%|     | 5/10 [00:06<00:04,  1.04dataset/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 3 days of data for device:  games console\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|| 10/10 [02:10<00:00, 13.07s/dataset]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = \"\"\n",
    "# Initialize paths\n",
    "path_to_base = \"../../Energy_graph/\"\n",
    "path = path_to_base + \"/data/processed_watts/\"+folder\n",
    "labels_path = path_to_base + \"/data/labels_new.pkl\"\n",
    "save_path = path_to_base + \"/data/training_data/processed/\"+folder\n",
    "\n",
    "# Initialize parameters\n",
    "time_window = 2688\n",
    "upper_bound = pd.Timedelta(seconds=32)\n",
    "max_gap = pd.Timedelta(seconds=3600)\n",
    "\n",
    "# Print parameters\n",
    "print(\"Path to base folder: \", path_to_base)\n",
    "print(\"Path to save windows: \", save_path)\n",
    "print(\"Time window: \", time_window, \"rows | \", time_window*8, \"seconds\")\n",
    "print(\"Upper bound: \", upper_bound)\n",
    "print(\"Max gap: \", max_gap)\n",
    "\n",
    "# Get data and create windows\n",
    "data = get_data(path, labels_path)\n",
    "# create_windows(data, labels_path, save_path, time_window, upper_bound, max_gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAWE_1 620447\n",
      "620447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 1/64 [00:00<00:20,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  230 Skipped windows due to 30min gap:  14 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  6.086956521739131\n",
      "HES_1 1152268\n",
      "1152268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 3/64 [00:01<00:24,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  428 Skipped windows due to 30min gap:  5 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.1682242990654206\n",
      "REDD_1 358273\n",
      "358273\n",
      "Total windows:  133 Skipped windows due to 30min gap:  5 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.7593984962406015\n",
      "REDD_2 148341\n",
      "148341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 5/64 [00:01<00:12,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  55 Skipped windows due to 30min gap:  3 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  5.454545454545454\n",
      "REDD_3 192499\n",
      "192499\n",
      "Total windows:  71 Skipped windows due to 30min gap:  9 Skipped windows due to 15 gaps of 32s or more:  1 Skipped windows due to values larger than 50k:  0 Procentage skipped:  14.084507042253522\n",
      "REDD_4 272058\n",
      "272058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 6/64 [00:01<00:10,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  101 Skipped windows due to 30min gap:  6 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  5.9405940594059405\n",
      "REDD_5 39395\n",
      "39395\n",
      "Total windows:  14 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  28.57142857142857\n",
      "REDD_6 152445\n",
      "152445\n",
      "Total windows:  56 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  7.142857142857142\n",
      "DRED_1 1657798\n",
      "1657798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 9/64 [00:02<00:15,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  616 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "HEART_33 1004400\n",
      "1004400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 10/64 [00:03<00:18,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  373 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "HEART_7 1004400\n",
      "1004400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 11/64 [00:03<00:21,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  373 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "DEDDIAG_8 4500232\n",
      "4500232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 12/64 [00:05<00:37,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1673 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "ECO_1 2494800\n",
      "2494800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 13/64 [00:06<00:41,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  927 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.10787486515641855\n",
      "ECO_6 1998000\n",
      "1998000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 14/64 [00:07<00:41,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  743 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.2691790040376851\n",
      "ECO_2 2592000\n",
      "2592000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 15/64 [00:08<00:47,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  963 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.20768431983385255\n",
      "ECO_5 2354400\n",
      "2354400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 16/64 [00:09<00:47,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  875 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.1142857142857143\n",
      "ECO_4 2106000\n",
      "2106000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 17/64 [00:10<00:42,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  783 Skipped windows due to 30min gap:  3 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.38314176245210724\n",
      "ECO_3 1047600\n",
      "1047600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 18/64 [00:10<00:35,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  389 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5141388174807198\n",
      "ENERTALK_1 1211429\n",
      "1211429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|      | 20/64 [00:11<00:23,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  450 Skipped windows due to 30min gap:  14 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.111111111111111\n",
      "ENERTALK_18 509433\n",
      "509433\n",
      "Total windows:  189 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5291005291005291\n",
      "ENERTALK_12 1282152\n",
      "1282152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 21/64 [00:12<00:23,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  476 Skipped windows due to 30min gap:  6 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.2605042016806722\n",
      "ENERTALK_20 647946\n",
      "647946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 23/64 [00:12<00:14,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  240 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4166666666666667\n",
      "ENERTALK_15 495183\n",
      "495183\n",
      "Total windows:  184 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5434782608695652\n",
      "ENERTALK_6 464668\n",
      "464668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 24/64 [00:12<00:12,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  172 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5813953488372093\n",
      "ENERTALK_8 657789\n",
      "657789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 26/64 [00:13<00:09,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  244 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4098360655737705\n",
      "ENERTALK_2 334755\n",
      "334755\n",
      "Total windows:  124 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "ENERTALK_11 322716\n",
      "322716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 27/64 [00:13<00:07,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  120 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  1 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.8333333333333334\n",
      "ENERTALK_16 767570\n",
      "767570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 28/64 [00:13<00:07,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  285 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.4035087719298245\n",
      "ENERTALK_5 617543\n",
      "617543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 29/64 [00:13<00:07,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  229 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.43668122270742354\n",
      "ENERTALK_7 657441\n",
      "657441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 30/64 [00:13<00:07,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  244 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4098360655737705\n",
      "ENERTALK_14 1062395\n",
      "1062395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 31/64 [00:14<00:07,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  395 Skipped windows due to 30min gap:  10 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.5316455696202533\n",
      "ENERTALK_13 947873\n",
      "947873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 32/64 [00:14<00:08,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  352 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.1363636363636365\n",
      "ENERTALK_19 637122\n",
      "637122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 33/64 [00:14<00:07,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  236 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.847457627118644\n",
      "ENERTALK_21 657552\n",
      "657552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 34/64 [00:15<00:07,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  244 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4098360655737705\n",
      "ENERTALK_0 936253\n",
      "936253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 35/64 [00:15<00:09,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  348 Skipped windows due to 30min gap:  6 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.7241379310344827\n",
      "ENERTALK_4 906831\n",
      "906831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 36/64 [00:15<00:09,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  337 Skipped windows due to 30min gap:  7 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.0771513353115725\n",
      "ENERTALK_17 912460\n",
      "912460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 37/64 [00:16<00:09,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  339 Skipped windows due to 30min gap:  7 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.0648967551622417\n",
      "ENERTALK_10 1249051\n",
      "1249051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 38/64 [00:16<00:09,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  464 Skipped windows due to 30min gap:  8 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.7241379310344827\n",
      "ENERTALK_3 1215991\n",
      "1215991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 39/64 [00:16<00:09,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  452 Skipped windows due to 30min gap:  11 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.433628318584071\n",
      "ENERTALK_9 1293445\n",
      "1293445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 40/64 [00:17<00:09,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  481 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.8316008316008316\n",
      "REFIT_13 4075515\n",
      "4075515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 41/64 [00:19<00:19,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1515 Skipped windows due to 30min gap:  35 Skipped windows due to 15 gaps of 32s or more:  124 Skipped windows due to values larger than 50k:  0 Procentage skipped:  10.495049504950495\n",
      "REFIT_6 5246422\n",
      "5246422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 42/64 [00:22<00:32,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1951 Skipped windows due to 30min gap:  22 Skipped windows due to 15 gaps of 32s or more:  32 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.7678113787801126\n",
      "REFIT_1 6003014\n",
      "6003014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 43/64 [00:25<00:40,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2232 Skipped windows due to 30min gap:  25 Skipped windows due to 15 gaps of 32s or more:  13 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.702508960573477\n",
      "REFIT_21 4770403\n",
      "4770403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 44/64 [00:27<00:41,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1774 Skipped windows due to 30min gap:  18 Skipped windows due to 15 gaps of 32s or more:  5 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.2965050732807215\n",
      "REFIT_8 5346837\n",
      "5346837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 45/64 [00:30<00:43,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1988 Skipped windows due to 30min gap:  18 Skipped windows due to 15 gaps of 32s or more:  11 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.4587525150905434\n",
      "REFIT_9 5161252\n",
      "5161252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 46/64 [00:33<00:44,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1919 Skipped windows due to 30min gap:  16 Skipped windows due to 15 gaps of 32s or more:  9 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.3027618551328817\n",
      "REFIT_20 4651687\n",
      "4651687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 47/64 [00:35<00:42,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1729 Skipped windows due to 30min gap:  12 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.156737998843262\n",
      "REFIT_7 5758682\n",
      "5758682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 48/64 [00:38<00:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2141 Skipped windows due to 30min gap:  24 Skipped windows due to 15 gaps of 32s or more:  10 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.5880429705744978\n",
      "REFIT_15 5262277\n",
      "5262277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 49/64 [00:41<00:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1956 Skipped windows due to 30min gap:  30 Skipped windows due to 15 gaps of 32s or more:  9 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.9938650306748467\n",
      "REFIT_12 4884120\n",
      "4884120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 50/64 [00:43<00:34,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1816 Skipped windows due to 30min gap:  16 Skipped windows due to 15 gaps of 32s or more:  43 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.248898678414097\n",
      "REFIT_4 5881600\n",
      "5881600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 51/64 [00:46<00:33,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2187 Skipped windows due to 30min gap:  40 Skipped windows due to 15 gaps of 32s or more:  27 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.063557384545039\n",
      "REFIT_3 5858072\n",
      "5858072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 52/64 [00:49<00:33,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2178 Skipped windows due to 30min gap:  20 Skipped windows due to 15 gaps of 32s or more:  11 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.423324150596878\n",
      "REFIT_18 4488860\n",
      "4488860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 53/64 [00:51<00:28,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1669 Skipped windows due to 30min gap:  16 Skipped windows due to 15 gaps of 32s or more:  4 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.1983223487118035\n",
      "REFIT_11 3745057\n",
      "3745057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 54/64 [00:53<00:24,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1392 Skipped windows due to 30min gap:  15 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.6522988505747127\n",
      "REFIT_16 4829646\n",
      "4829646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 55/64 [00:56<00:21,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1796 Skipped windows due to 30min gap:  32 Skipped windows due to 15 gaps of 32s or more:  6 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.1158129175946545\n",
      "REFIT_17 4733420\n",
      "4733420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 56/64 [00:58<00:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1760 Skipped windows due to 30min gap:  17 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.4204545454545454\n",
      "REFIT_10 5573669\n",
      "5573669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 57/64 [01:01<00:17,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2072 Skipped windows due to 30min gap:  32 Skipped windows due to 15 gaps of 32s or more:  51 Skipped windows due to values larger than 50k:  0 Procentage skipped:  4.005791505791506\n",
      "REFIT_19 4720415\n",
      "4720415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 58/64 [01:03<00:14,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1755 Skipped windows due to 30min gap:  15 Skipped windows due to 15 gaps of 32s or more:  20 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.9943019943019942\n",
      "REFIT_2 4999158\n",
      "4999158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 59/64 [01:06<00:12,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1859 Skipped windows due to 30min gap:  33 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.205486820871436\n",
      "REFIT_5 6287475\n",
      "6287475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 60/64 [01:10<00:11,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2338 Skipped windows due to 30min gap:  33 Skipped windows due to 15 gaps of 32s or more:  31 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.737382378100941\n",
      "UKDALE_5 1425706\n",
      "1425706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 61/64 [01:11<00:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  530 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.37735849056603776\n",
      "UKDALE_2 2156879\n",
      "2156879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 62/64 [01:13<00:04,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  802 Skipped windows due to 30min gap:  8 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.997506234413965\n",
      "UKDALE_1 17064863\n",
      "17064863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 64/64 [01:34<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  6346 Skipped windows due to 30min gap:  35 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5515285219035614\n",
      "UKDALE_3 396227\n",
      "396227\n",
      "Total windows:  147 Skipped windows due to 30min gap:  7 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  4.761904761904762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data.keys()\n",
    "processed = {}\n",
    "\n",
    "for key in tqdm(data.keys()):\n",
    "    # if \"UKDALE\" not in key: \n",
    "    #     continue\n",
    "    print(key, len(data[key]))\n",
    "    processed[key] = create_windows({key : data[key]}, labels_path, save_path, time_window, upper_bound, max_gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggregate</th>\n",
       "      <th>kettle</th>\n",
       "      <th>projector</th>\n",
       "      <th>laptop</th>\n",
       "      <th>electric space heater</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:36</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:44</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:32</th>\n",
       "      <td>176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:40</th>\n",
       "      <td>174.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:56</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:15:04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425100 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aggregate  kettle  projector  laptop  \\\n",
       "0                                                           \n",
       "2013-02-27 20:35:12        5.0       0          0       0   \n",
       "2013-02-27 20:35:20        4.0       0          1       0   \n",
       "2013-02-27 20:35:28        5.0       0          1       0   \n",
       "2013-02-27 20:35:36        4.0       0          0       0   \n",
       "2013-02-27 20:35:44        4.0       0          0       0   \n",
       "...                        ...     ...        ...     ...   \n",
       "2013-04-08 05:14:32      176.0       0          0       0   \n",
       "2013-04-08 05:14:40      174.0       0          0       0   \n",
       "2013-04-08 05:14:48        0.0       0          0       0   \n",
       "2013-04-08 05:14:56        0.0       1          0       0   \n",
       "2013-04-08 05:15:04        0.0       0          0       0   \n",
       "\n",
       "                     electric space heater  \n",
       "0                                           \n",
       "2013-02-27 20:35:12                      0  \n",
       "2013-02-27 20:35:20                      0  \n",
       "2013-02-27 20:35:28                      0  \n",
       "2013-02-27 20:35:36                      0  \n",
       "2013-02-27 20:35:44                      0  \n",
       "...                                    ...  \n",
       "2013-04-08 05:14:32                      0  \n",
       "2013-04-08 05:14:40                      0  \n",
       "2013-04-08 05:14:48                      0  \n",
       "2013-04-08 05:14:56                      0  \n",
       "2013-04-08 05:15:04                      0  \n",
       "\n",
       "[425100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_devices(data):\n",
    "\n",
    "    devices = set()\n",
    "    real_str = \" \"\n",
    "    for key in data.keys():\n",
    "        if key == \"aggregate\":\n",
    "            continue\n",
    "        real_str+= key + \", \"\n",
    "        devices.add(preprocess_string(key))\n",
    "        \n",
    "\n",
    "    return list(devices), real_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../Energy_graph/data/training_data/real_house.pkl\", \"wb\") as f:\n",
    "    pickle.dump(processed, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAWE_1\n",
      "2\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "0.5008752495756639 0.6483688948976613 0.46766169154228854\n",
      "HES_1\n",
      "2\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "0.9336485220415739 1.0 0.8800236406619385\n",
      "REDD_1\n",
      "2\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "0.19309986010022545 0.4746600741656366 0.16563658838071693\n",
      "REDD_2\n",
      "2\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "0.48504702660240756 0.582089552238806 0.43656716417910446\n",
      "REDD_3\n",
      "2\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 27ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "0.19243143563985396 0.43884892086330934 0.1630695443645084\n",
      "REDD_4\n",
      "2\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "3/3 [==============================] - 0s 25ms/step\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "0.09473283976124885 0.37109375 0.05859375\n",
      "REDD_5\n",
      "2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0.4062575794324521 0.4838709677419355 0.3709677419354839\n",
      "REDD_6\n",
      "2\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "0.19418426691153964 0.3501683501683502 0.18518518518518517\n",
      "DRED_1\n",
      "2\n",
      "20/20 [==============================] - 1s 26ms/step\n",
      "20/20 [==============================] - 0s 25ms/step\n",
      "20/20 [==============================] - 1s 25ms/step\n",
      "20/20 [==============================] - 0s 25ms/step\n",
      "20/20 [==============================] - 1s 26ms/step\n",
      "0.3613230390192975 0.6204464870610003 0.32207407407407407\n",
      "HEART_33\n",
      "2\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "0.3430030937477516 0.33018970189701896 0.36829268292682926\n",
      "HEART_7\n",
      "2\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "0.35986203191925475 0.35865287188359446 0.3611342785654712\n",
      "DEDDIAG_8\n",
      "2\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "53/53 [==============================] - 1s 26ms/step\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "0.5356829900571298 0.8939638012865683 0.43100065402223675\n",
      "ECO_1\n",
      "2\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "0.27352528289181305 0.8616935142537381 0.22764227642276422\n",
      "ECO_6\n",
      "2\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "0.45390835202627133 0.6848777000471161 0.3463541666666667\n",
      "ECO_2\n",
      "2\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "0.2957932328210172 0.6231134045085699 0.278561209891682\n",
      "ECO_5\n",
      "2\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "0.47195018658088017 0.9077831547647703 0.43652371485373476\n",
      "ECO_4\n",
      "2\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "0.7032456460306385 0.7452192998889746 0.6678032148075986\n",
      "ECO_3\n",
      "2\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 25ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 25ms/step\n",
      "0.6029031141997384 0.7598817009248773 0.6286644951140065\n",
      "ENERTALK_1\n",
      "2\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "0.5368382177333908 0.9717026819888461 0.4714537963507946\n",
      "ENERTALK_18\n",
      "2\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "0.6620204606809102 0.75 0.5997340425531915\n",
      "ENERTALK_12\n",
      "2\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "0.4739489736147738 0.5833426086316169 0.44888108819657746\n",
      "ENERTALK_20\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "0.31736878506109273 0.33426573426573425 0.3020979020979021\n",
      "ENERTALK_15\n",
      "2\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "0.938832021005673 0.9875956027108486 0.8947368421052632\n",
      "ENERTALK_6\n",
      "2\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "0.47444418120802656 0.7590144230769231 0.43345543345543347\n",
      "ENERTALK_8\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.3234914276434333 0.5 0.24074074074074073\n",
      "ENERTALK_2\n",
      "2\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 24ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "0.6806172047162045 0.9927098186492077 0.6247464503042597\n",
      "ENERTALK_11\n",
      "2\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "0.7878276003276004 1.0 0.6722689075630253\n",
      "ENERTALK_16\n",
      "2\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "0.9436090225563909 1.0 0.8932384341637011\n",
      "ENERTALK_5\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.4966494157471601 0.5063011015911874 0.511578947368421\n",
      "ENERTALK_7\n",
      "2\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.5 0.5 0.5\n",
      "ENERTALK_14\n",
      "2\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "0.15347721822541965 1.0 0.08311688311688312\n",
      "ENERTALK_13\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "0.3365146100483498 0.3501006036217304 0.323943661971831\n",
      "ENERTALK_19\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.6481948714749804 0.9740255257239991 0.48577680525164113\n",
      "ENERTALK_21\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "0.33072191433779696 0.5159235668789809 0.2791932059447983\n",
      "ENERTALK_0\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "0.5085194757098893 0.6464675913862539 0.4950805008944544\n",
      "ENERTALK_4\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "0.7128795064755322 0.9353220825833164 0.6495934959349593\n",
      "ENERTALK_17\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "0.5552453930852128 0.7818407279385825 0.5315822388993121\n",
      "ENERTALK_10\n",
      "2\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "0.4207670310715854 1.0 0.3534675615212528\n",
      "ENERTALK_3\n",
      "2\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "0.04211619543650794 0.4965277777777778 0.02199074074074074\n",
      "ENERTALK_9\n",
      "2\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "0.4397902574645901 0.9538688210745934 0.40801354401805867\n",
      "REFIT_13\n",
      "2\n",
      "43/43 [==============================] - 1s 26ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 26ms/step\n",
      "43/43 [==============================] - 1s 26ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "0.4794494784860013 0.7986656579340571 0.40770725388601037\n",
      "REFIT_6\n",
      "2\n",
      "60/60 [==============================] - 2s 25ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "0.6749540826097389 0.9081763593824688 0.593030242339275\n",
      "REFIT_1\n",
      "2\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "69/69 [==============================] - 2s 26ms/step\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "0.3980972921759798 0.766385631188681 0.4018416677324466\n",
      "REFIT_21\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "0.45505766917916557 0.6217043220527281 0.4354744808849214\n",
      "REFIT_8\n",
      "2\n",
      "62/62 [==============================] - 2s 25ms/step\n",
      "62/62 [==============================] - 2s 25ms/step\n",
      "62/62 [==============================] - 2s 25ms/step\n",
      "62/62 [==============================] - 2s 26ms/step\n",
      "62/62 [==============================] - 2s 26ms/step\n",
      "0.6186423292351364 0.897914435512884 0.5729982887507881\n",
      "REFIT_9\n",
      "2\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 25ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "0.498968870074634 0.903633554551245 0.4430954272098037\n",
      "REFIT_20\n",
      "2\n",
      "54/54 [==============================] - 1s 26ms/step\n",
      "54/54 [==============================] - 1s 26ms/step\n",
      "54/54 [==============================] - 1s 26ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "0.6125393778774827 0.8743209111549864 0.5311355311355311\n",
      "REFIT_7\n",
      "2\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "0.6184004394121984 0.8309242965030035 0.5740226986128626\n",
      "REFIT_15\n",
      "2\n",
      "60/60 [==============================] - 2s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "0.4571135315464248 0.8047035226950325 0.4426062980699463\n",
      "REFIT_12\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 26ms/step\n",
      "0.6213138032576755 0.9043113210512488 0.5349623482403565\n",
      "REFIT_4\n",
      "2\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "67/67 [==============================] - 2s 26ms/step\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "0.5927398444822197 0.8935679968195096 0.511986301369863\n",
      "REFIT_3\n",
      "2\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "68/68 [==============================] - 2s 26ms/step\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "0.5835016399019964 0.742024950596074 0.5790172642762285\n",
      "REFIT_18\n",
      "2\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "0.6613141886259535 0.9273352337435147 0.5542554673352834\n",
      "REFIT_11\n",
      "2\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "0.3239436741091002 0.7806991503168546 0.28835462058602557\n",
      "REFIT_16\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "0.608615766294815 0.6987664035047385 0.5841446453407511\n",
      "REFIT_17\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "0.6129264919242996 0.9148831072099401 0.5414779631061527\n",
      "REFIT_10\n",
      "2\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "0.49508534801038906 0.7048704588249881 0.47951558876578204\n",
      "REFIT_19\n",
      "2\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "0.47783858611386104 0.6254716616733725 0.41051616015436565\n",
      "REFIT_2\n",
      "2\n",
      "57/57 [==============================] - 1s 26ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "0.4822822485346139 0.7896359941103305 0.4353133839511687\n",
      "REFIT_5\n",
      "2\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "72/72 [==============================] - 2s 26ms/step\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "0.5289473319012725 0.7802312454689498 0.4613081873970962\n",
      "UKDALE_5\n",
      "2\n",
      "17/17 [==============================] - 0s 25ms/step\n",
      "17/17 [==============================] - 0s 25ms/step\n",
      "17/17 [==============================] - 0s 26ms/step\n",
      "17/17 [==============================] - 0s 26ms/step\n",
      "17/17 [==============================] - 0s 26ms/step\n",
      "0.29220143640157703 0.5050456606805548 0.24193294506532348\n",
      "UKDALE_2\n",
      "2\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "0.2287783570121012 0.2753419158712448 0.2076704895794796\n",
      "UKDALE_1\n",
      "2\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "0.1896290030041717 0.4496575532812503 0.1608097535532664\n",
      "UKDALE_3\n",
      "2\n",
      "5/5 [==============================] - 0s 26ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "5/5 [==============================] - 0s 26ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "0.033344139304404205 0.2980132450331126 0.017660044150110375\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "labels = pd.read_pickle(labels_path)\n",
    "\n",
    "for f in os.listdir(\"./models/test_IDEAL/\"):\n",
    "        if \"last\" in f:\n",
    "            model = tf.keras.models.load_model(\"./models/test_IDEAL/\"+f, custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "            models.append(model)\n",
    "\n",
    "for h in processed:\n",
    "    print(h)\n",
    "    df = processed[h]\n",
    "    curr_devices, real_str = get_devices(data[h])\n",
    "    X = np.array([i[0] for i in df])\n",
    "    y = np.array([i[1] for i in df])\n",
    "    X= normalize(X)\n",
    "    print(len(X.shape))\n",
    "\n",
    "    if len(X.shape) !=2 or X.shape[1] != 2688:\n",
    "        print(\"wrong shape\", X.shape)\n",
    "        continue\n",
    "    \n",
    "    model_predictions = []\n",
    "    for m in models:\n",
    "            y_pred = m.predict(X)\n",
    "            model_predictions.append(y_pred)\n",
    "\n",
    "\n",
    "    y_pred_avg = np.mean(model_predictions, axis=0)\n",
    "    y_pred_tf = np.where(y_pred_avg > 0.3, 1, 0)\n",
    "\n",
    "    y_pred_num_devices = np.sum(y_pred_tf, axis=1)\n",
    "    y_real_num_devices = np.sum(y, axis=1)\n",
    "\n",
    "    # print(\"average number of devices misspredcited: \", np.mean(y_pred_num_devices - y_real_num_devices)\t)\n",
    "\n",
    "    res = metrics.classification_report(y, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(res[\"weighted avg\"][\"f1-score\"], res[\"weighted avg\"][\"precision\"], res[\"weighted avg\"][\"recall\"])\n",
    "\n",
    "    # barplot y_pred_tf\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(labels, y_pred_tf.sum(axis=0))\n",
    "    # plt.title(h + real_str + \"f1: \"+ str(res[\"weighted avg\"][\"f1-score\"]) + \"precision: \"+ str(res[\"weighted avg\"][\"precision\"]) + \"recall: \"+ str(res[\"weighted avg\"][\"recall\"]))\n",
    "    plt.title(h + \" f1: \"+ str(res[\"weighted avg\"][\"f1-score\"])[:5] + \" average number of devices misspredcited: \" + str(np.mean(y_pred_num_devices - y_real_num_devices))[:5]\t)\n",
    "\n",
    "    # Get current axis\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Loop through label list and set the color to red for highlighted labels\n",
    "    for label in ax.get_xticklabels():\n",
    "        if label.get_text() in curr_devices:\n",
    "            label.set_color('red')\n",
    "\n",
    "    # Rotate all labels\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f\"./plots/real_ideal/{h}_barplot.svg\", format=\"svg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "pc\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:/tmp/nilmtk-zbuuzryk.h5...done\n"
     ]
    }
   ],
   "source": [
    "tb = pd.read_pickle(\"../../Energy_graph/data/tracebase/devices_data.pkl\")\n",
    "device = \"washing machine\"\n",
    "from random import randint\n",
    "i = randint(0, len(tb[device])-1)\n",
    "offset = randint(0, len(tb[device][i])-2688)\n",
    "x = tb[device][i][offset:offset+2668].copy()\n",
    "x[x<5] = 0\n",
    "plt.plot(x)\n",
    "x = x.values.reshape(1, -1)\n",
    "# print(x.values.reshape(1, -1)[0])\n",
    "model_predictions = []\n",
    "for m in models:\n",
    "    y_pred = m.predict(x)\n",
    "    model_predictions.append(y_pred)\n",
    "    \n",
    "y_pred_avg = np.mean(model_predictions, axis=0)\n",
    "y_pred_tf = np.where(y_pred_avg > 0.3, 1, 0)\n",
    "print(y_pred_tf)\n",
    "for i in range(len(labels)):\n",
    "    if y_pred_tf[0][i] == 1:\n",
    "        print(labels[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def preprocess_string(string : str) -> str:\n",
    "    string = string.lower().strip()\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    string = string.replace(\"-\", \" \")\n",
    "    string = string.replace(\"&\", \" \")\n",
    "    string = string.split(\"(\")[0]\n",
    "    string = string.split(\"#\")[0]\n",
    "\n",
    "    string = string.strip()\n",
    "\n",
    "    # handle known synoynms\n",
    "    synonyms = {\n",
    "        \"refrigerator\": \"fridge\",\n",
    "        \"vaccumcleaner\": \"vacuum cleaner\",\n",
    "        \"breadmaker\": \"bread maker\",\n",
    "      \n",
    "        \n",
    "    }\n",
    "    if \"freezer\" in string:\n",
    "        string = \"fridge\"\n",
    "\n",
    "    if string in synonyms:\n",
    "        string = synonyms[string]\n",
    "\n",
    "    if 'hi fi' in string:\n",
    "        string = \"audio system\"\n",
    "\n",
    "    if \"router\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    if \"treadmill\" in string:\n",
    "        string = \"running machine\"\n",
    "        \n",
    "\n",
    "    if \"laptop\" in string:\n",
    "        string = \"laptop\"\n",
    "    \n",
    "    if \"server\" in string:\n",
    "        string = \"server\"\n",
    "\n",
    "    if \"monitor\" in string and not \"baby\" in string:\n",
    "        string = \"monitor\"\n",
    "    # special cases\n",
    "    if \"computer\" in string and \"charger\" not in string:\n",
    "        string = \"pc\"\n",
    "\n",
    "    if \"tv\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"television\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"macbook\" in string:\n",
    "        string = \"laptop\"\n",
    "        \n",
    "    if \"car charger\" == string:\n",
    "        string = \"ev\"\n",
    "    \n",
    "    if \"toast\" in string:\n",
    "        string = \"toaster\"\n",
    "    \n",
    "    if \"modem\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    # we treat all audio devices as speakers so subwoofer is also a speaker\n",
    "    if \"subwoofer\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"speaker\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"iron\" in string and \"soldering\" not in string:\n",
    "        string = \"iron\"\n",
    "\n",
    "    \n",
    "    if \"coffeemachine\" in string:\n",
    "        string = \"coffee machine\"\n",
    "    if \"coffee maker\" in string:\n",
    "        string = \"coffee machine\"\n",
    "\n",
    "    if \"dishwasher\" in string:\n",
    "        string = \"dish washer\"\n",
    "    if \"air conditioner\" in string:\n",
    "        string = \"ac\"\n",
    "\n",
    "    if \"air conditioning\" in string:\n",
    "        string = \"ac\"\n",
    "    \n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = re.sub(r'\\d+', '', string)\n",
    "    return string.strip()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
