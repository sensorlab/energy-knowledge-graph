{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "By downloading and using the cuDNN conda packages, you accept the terms and conditions of the NVIDIA cuDNN EULA -\n",
      "  https://docs.nvidia.com/deeplearning/cudnn/sla/index.html\n",
      "\n",
      "done\n",
      "\u001b[33mWARNING: Skipping nilmtk as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping nilm_metadata as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/nilmtk/nilmtk@master\n",
      "  Cloning https://github.com/nilmtk/nilmtk (to revision master) to /tmp/pip-req-build-fxirbq2g\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilmtk /tmp/pip-req-build-fxirbq2g\n",
      "  Resolved https://github.com/nilmtk/nilmtk to commit b2c514479cef478cab872cb635056da08d5352a1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilmtk\n",
      "  Building wheel for nilmtk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilmtk: filename=nilmtk-0.4.0.dev1+git.b2c5144-py3-none-any.whl size=279177 sha256=4f257c9ba5d11734665c3c4d762c64070b7db516db1089afd9f0cb52d6e7c94e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b7oxzorc/wheels/05/a4/8e/b767c3d1714f61fd30ec991c25780be8cc04c474da8205aeee\n",
      "Successfully built nilmtk\n",
      "Installing collected packages: nilmtk\n",
      "Successfully installed nilmtk-0.4.0.dev1+git.b2c5144\n",
      "Collecting git+https://github.com/nilmtk/nilm_metadata@master\n",
      "  Cloning https://github.com/nilmtk/nilm_metadata (to revision master) to /tmp/pip-req-build-4929t_0k\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilm_metadata /tmp/pip-req-build-4929t_0k\n",
      "  Resolved https://github.com/nilmtk/nilm_metadata to commit 7ed4bab9062d04cb35c6b6000b451715dc5ab4af\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilm-metadata\n",
      "  Building wheel for nilm-metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilm-metadata: filename=nilm_metadata-0.2.5-py3-none-any.whl size=24340 sha256=e543b79d06d029197696a396f6faab3dbfca31c228e6dbbdca930968b8adf2ef\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-btl7zl5g/wheels/7e/2f/7f/e41975c202542dbc11228875ff8473ed4cb95e04a274222bb7\n",
      "Successfully built nilm-metadata\n",
      "Installing collected packages: nilm-metadata\n",
      "Successfully installed nilm-metadata-0.2.5\n"
     ]
    }
   ],
   "source": [
    "#!rm -r /home/jovyan/.local/share/jupyter/kernels/nilmtk\n",
    "!mamba install tensorflow-gpu==2.10 -y -q\n",
    "# !mamba install tensorflow-gpu==2.11.0 -y -q\n",
    "!pip uninstall -y -q nilmtk nilm_metadata\n",
    "#!pip install -q pandas networkx tables scikit-learn hmmlearn pyyaml matplotlib==3.1.3 xgboost pyts\n",
    "# Trick to install NILM regardless of its dependencies\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilm_metadata.git\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilmtk.git@0.4.3\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilmtk@master\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilm_metadata@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from nilmtk import DataSet\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Memory\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "#from nilmtk import DataSet\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "!pip install -q scikit-learn-intelex\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "from sklearn import pipeline, metrics, linear_model, model_selection, multioutput, tree, ensemble, neural_network\n",
    "!pip install xgboost -q\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "#import graphviz\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from ann_visualizer.visualize import ann_viz\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix\n",
    "# GPU goes brrrrrrrrrrrrrrrrrrrr\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "!pip3 install tqdm -q\n",
    "from tqdm.notebook import tqdm\n",
    "import NUK\n",
    "\n",
    "# import garbage collector\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(physical_devices)\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU found, model will train on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "epochs =300\n",
    "window_size = 2700\n",
    "batch_size = 128\n",
    "NmDevices = 78\n",
    "# table_of_options = [1,2,3,4]\n",
    "k = 0.25\n",
    "# Define a learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    return lr\n",
    "    if epoch == 0 or epoch == 1:\n",
    "        return lr\n",
    "    if epoch == 35:\n",
    "        return lr *0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "test_size = 0.2\n",
    "lambda_l2=0\n",
    "function = \"GRU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_pickle(\"../../Energy_graph/data/training_data/processed/X_Y_wsize2700_upper32_gap3600.pkl\")\n",
    "data_syn_ideal = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2700_numW_50000_upper32_gap3600_ideal.pkl\") \n",
    "data_syn_unmetered = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2700_numW_50000_upper32_gap3600_unmetered.pkl\")\n",
    "\n",
    "labels = pd.read_pickle(\"../../Energy_graph/data/labels_new.pkl\")\n",
    "\n",
    "\n",
    "# Separate the tuples into X and y\n",
    "X_syn_ideal = np.array([i[0] for i in data_syn_ideal])\n",
    "y_syn_ideal = np.array([i[1] for i in data_syn_ideal])\n",
    "\n",
    "X_syn_unmetered = np.array([i[0] for i in data_syn_unmetered])\n",
    "y_syn_unmetered = np.array([i[1] for i in data_syn_unmetered])\n",
    "\n",
    "X = np.array([i[0] for i in data])\n",
    "y = np.array([i[1] for i in data])\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "X = np.concatenate((X, X_syn_ideal,X_syn_unmetered), axis=0)\n",
    "y = np.concatenate((y, y_syn_ideal, y_syn_unmetered), axis=0)\n",
    "\n",
    "# X_test = np.array([i[0] for i in data_test])\n",
    "# y_test = np.array([i[1] for i in data_test])\n",
    "def normalize(X):\n",
    "    max_value = 0\n",
    "\n",
    "    for x in X:\n",
    "        v = np.max(x)\n",
    "        if v > max_value:\n",
    "            max_value = v\n",
    "    return X / max_value\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionTime class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weighs_pre = NUK.class_weights_tool(y)\n",
    "\n",
    "class Classifier_INCEPTION:\n",
    "\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, iteration, verbose=False, build=True, batch_size=64,\n",
    "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500, lr=0.001):\n",
    "\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "        self.nb_filters = nb_filters\n",
    "        self.use_residual = use_residual\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size - 1\n",
    "        self.callbacks = None\n",
    "        self.batch_size = batch_size\n",
    "        self.bottleneck_size = 32\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.iteration = iteration\n",
    "\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            # if (verbose == True):\n",
    "                # self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "\n",
    "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
    "\n",
    "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
    "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
    "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
    "\n",
    "        conv_list = []\n",
    "\n",
    "        for i in range(len(kernel_size_s)):\n",
    "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
    "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
    "                input_inception))\n",
    "\n",
    "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
    "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_6)\n",
    "\n",
    "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
    "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
    "                                         padding='same', use_bias=False)(input_tensor)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        x = input_layer\n",
    "        input_res = input_layer\n",
    "\n",
    "        for d in range(self.depth):\n",
    "\n",
    "            x = self._inception_module(x)\n",
    "\n",
    "            if self.use_residual and d % 3 == 2:\n",
    "                x = self._shortcut_layer(input_res, x)\n",
    "                input_res = x\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='sigmoid')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
    "                      metrics=['accuracy', NUK.F1Score, NUK.WeightedF1Score(class_weighs_pre)])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=10, min_lr=0.0001)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor='loss')\n",
    "\n",
    "        file_path = self.output_directory + f'best_model_{self.iteration}.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "        # lr_scheduler = LearningRateScheduler(scheduler)\n",
    "        self.callbacks = [reduce_lr, model_checkpoint, early_stopping]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "       \n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
    "        else:\n",
    "            mini_batch_size = self.batch_size\n",
    "\n",
    "        self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, callbacks=self.callbacks)\n",
    "\n",
    "\n",
    "        self.model.save(self.output_directory + f'last_model_{self.iteration}.hdf5')\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        model_path = self.output_directory + f'last_model_{self.iteration}.hdf5'\n",
    "        model = keras.models.load_model(model_path, custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
    "    #     # if return_df_metrics:\n",
    "    #     #     y_pred = np.argmax(y_pred, axis=1)\n",
    "    #     #     df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
    "    #     #     return df_metrics\n",
    "    #     # else:\n",
    "    #     #     test_duration = time.time() - start_time\n",
    "    #     #     save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
    "    #     #     return y_pred\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "itr = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    # print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    # model = NUK.PC0_reg(NmDevices, window_size, function, 128, k, lambda_l2=lambda_l2)\n",
    "    print(batch_size, epochs)\n",
    "    model = Classifier_INCEPTION(output_directory=\"./models/\", input_shape=(window_size, 1), nb_classes=78, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=6)\n",
    "    model.fit(X_train, y_train)\n",
    "    # model.model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test, y_pred))\n",
    "    t = (y_pred_tf, y_test, y_pred)\n",
    "    with open(f\"./predictions/test_inception_pickle_{itr}.pkl\", 'wb') as f:\n",
    "        pickle.dump(t, f)\n",
    "\n",
    "    itr += 1\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/Inception_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77be7a21e2c7437b8a043d693d7fff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 300\n",
      "Epoch 1/300\n",
      "765/765 [==============================] - 148s 189ms/step - loss: 0.2684 - accuracy: 0.1268 - F1Score: 0.0261 - WeightedF1: 0.0432 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "765/765 [==============================] - 146s 190ms/step - loss: 0.2540 - accuracy: 0.1404 - F1Score: 0.0453 - WeightedF1: 0.0716 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "765/765 [==============================] - 145s 189ms/step - loss: 0.2471 - accuracy: 0.1455 - F1Score: 0.0617 - WeightedF1: 0.0949 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2423 - accuracy: 0.1452 - F1Score: 0.0782 - WeightedF1: 0.1174 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2383 - accuracy: 0.1443 - F1Score: 0.0917 - WeightedF1: 0.1356 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "765/765 [==============================] - 145s 189ms/step - loss: 0.2348 - accuracy: 0.1480 - F1Score: 0.1027 - WeightedF1: 0.1497 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.2318 - accuracy: 0.1486 - F1Score: 0.1133 - WeightedF1: 0.1620 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.2290 - accuracy: 0.1498 - F1Score: 0.1231 - WeightedF1: 0.1731 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2268 - accuracy: 0.1488 - F1Score: 0.1308 - WeightedF1: 0.1819 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "765/765 [==============================] - 144s 189ms/step - loss: 0.2245 - accuracy: 0.1508 - F1Score: 0.1393 - WeightedF1: 0.1908 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.2220 - accuracy: 0.1505 - F1Score: 0.1492 - WeightedF1: 0.2013 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2202 - accuracy: 0.1512 - F1Score: 0.1588 - WeightedF1: 0.2111 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2180 - accuracy: 0.1524 - F1Score: 0.1684 - WeightedF1: 0.2208 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2160 - accuracy: 0.1526 - F1Score: 0.1788 - WeightedF1: 0.2312 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2140 - accuracy: 0.1546 - F1Score: 0.1890 - WeightedF1: 0.2414 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "765/765 [==============================] - 144s 189ms/step - loss: 0.2124 - accuracy: 0.1554 - F1Score: 0.1983 - WeightedF1: 0.2504 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2106 - accuracy: 0.1577 - F1Score: 0.2075 - WeightedF1: 0.2596 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2091 - accuracy: 0.1596 - F1Score: 0.2164 - WeightedF1: 0.2684 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2078 - accuracy: 0.1609 - F1Score: 0.2228 - WeightedF1: 0.2749 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "765/765 [==============================] - 144s 189ms/step - loss: 0.2064 - accuracy: 0.1605 - F1Score: 0.2307 - WeightedF1: 0.2825 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "765/765 [==============================] - 144s 189ms/step - loss: 0.2053 - accuracy: 0.1636 - F1Score: 0.2362 - WeightedF1: 0.2880 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2042 - accuracy: 0.1650 - F1Score: 0.2424 - WeightedF1: 0.2943 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "765/765 [==============================] - 145s 189ms/step - loss: 0.2031 - accuracy: 0.1666 - F1Score: 0.2484 - WeightedF1: 0.3002 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.2020 - accuracy: 0.1673 - F1Score: 0.2532 - WeightedF1: 0.3049 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.2012 - accuracy: 0.1680 - F1Score: 0.2581 - WeightedF1: 0.3097 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.2002 - accuracy: 0.1696 - F1Score: 0.2622 - WeightedF1: 0.3139 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1994 - accuracy: 0.1699 - F1Score: 0.2674 - WeightedF1: 0.3189 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1985 - accuracy: 0.1701 - F1Score: 0.2714 - WeightedF1: 0.3230 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1978 - accuracy: 0.1712 - F1Score: 0.2756 - WeightedF1: 0.3272 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1969 - accuracy: 0.1732 - F1Score: 0.2787 - WeightedF1: 0.3303 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1965 - accuracy: 0.1745 - F1Score: 0.2818 - WeightedF1: 0.3333 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1956 - accuracy: 0.1741 - F1Score: 0.2860 - WeightedF1: 0.3376 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1950 - accuracy: 0.1752 - F1Score: 0.2899 - WeightedF1: 0.3413 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1944 - accuracy: 0.1770 - F1Score: 0.2924 - WeightedF1: 0.3440 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1938 - accuracy: 0.1771 - F1Score: 0.2954 - WeightedF1: 0.3466 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1933 - accuracy: 0.1768 - F1Score: 0.2983 - WeightedF1: 0.3496 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1925 - accuracy: 0.1773 - F1Score: 0.3013 - WeightedF1: 0.3527 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1922 - accuracy: 0.1799 - F1Score: 0.3033 - WeightedF1: 0.3546 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1915 - accuracy: 0.1809 - F1Score: 0.3069 - WeightedF1: 0.3582 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1911 - accuracy: 0.1798 - F1Score: 0.3092 - WeightedF1: 0.3602 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1906 - accuracy: 0.1810 - F1Score: 0.3113 - WeightedF1: 0.3623 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1900 - accuracy: 0.1809 - F1Score: 0.3138 - WeightedF1: 0.3650 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1897 - accuracy: 0.1826 - F1Score: 0.3158 - WeightedF1: 0.3669 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1891 - accuracy: 0.1824 - F1Score: 0.3186 - WeightedF1: 0.3695 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1889 - accuracy: 0.1828 - F1Score: 0.3200 - WeightedF1: 0.3708 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1882 - accuracy: 0.1842 - F1Score: 0.3230 - WeightedF1: 0.3739 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1877 - accuracy: 0.1845 - F1Score: 0.3245 - WeightedF1: 0.3755 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1874 - accuracy: 0.1838 - F1Score: 0.3267 - WeightedF1: 0.3775 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1871 - accuracy: 0.1847 - F1Score: 0.3286 - WeightedF1: 0.3792 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1868 - accuracy: 0.1859 - F1Score: 0.3296 - WeightedF1: 0.3804 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1862 - accuracy: 0.1865 - F1Score: 0.3320 - WeightedF1: 0.3828 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1857 - accuracy: 0.1859 - F1Score: 0.3341 - WeightedF1: 0.3848 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "765/765 [==============================] - 146s 191ms/step - loss: 0.1855 - accuracy: 0.1866 - F1Score: 0.3359 - WeightedF1: 0.3866 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.1852 - accuracy: 0.1873 - F1Score: 0.3361 - WeightedF1: 0.3868 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1848 - accuracy: 0.1863 - F1Score: 0.3383 - WeightedF1: 0.3889 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1844 - accuracy: 0.1870 - F1Score: 0.3399 - WeightedF1: 0.3907 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1841 - accuracy: 0.1901 - F1Score: 0.3414 - WeightedF1: 0.3919 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "765/765 [==============================] - 146s 190ms/step - loss: 0.1837 - accuracy: 0.1895 - F1Score: 0.3429 - WeightedF1: 0.3935 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "765/765 [==============================] - 143s 186ms/step - loss: 0.1834 - accuracy: 0.1891 - F1Score: 0.3439 - WeightedF1: 0.3945 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1831 - accuracy: 0.1889 - F1Score: 0.3461 - WeightedF1: 0.3965 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "765/765 [==============================] - 143s 186ms/step - loss: 0.1828 - accuracy: 0.1909 - F1Score: 0.3465 - WeightedF1: 0.3971 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1826 - accuracy: 0.1896 - F1Score: 0.3483 - WeightedF1: 0.3987 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1822 - accuracy: 0.1892 - F1Score: 0.3494 - WeightedF1: 0.3998 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "765/765 [==============================] - 143s 186ms/step - loss: 0.1819 - accuracy: 0.1902 - F1Score: 0.3501 - WeightedF1: 0.4006 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1816 - accuracy: 0.1915 - F1Score: 0.3513 - WeightedF1: 0.4016 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1812 - accuracy: 0.1910 - F1Score: 0.3532 - WeightedF1: 0.4035 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1815 - accuracy: 0.1906 - F1Score: 0.3535 - WeightedF1: 0.4036 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1813 - accuracy: 0.1897 - F1Score: 0.3536 - WeightedF1: 0.4039 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1804 - accuracy: 0.1921 - F1Score: 0.3567 - WeightedF1: 0.4068 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1803 - accuracy: 0.1917 - F1Score: 0.3570 - WeightedF1: 0.4072 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1803 - accuracy: 0.1899 - F1Score: 0.3570 - WeightedF1: 0.4071 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1797 - accuracy: 0.1920 - F1Score: 0.3596 - WeightedF1: 0.4097 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1796 - accuracy: 0.1913 - F1Score: 0.3604 - WeightedF1: 0.4105 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1791 - accuracy: 0.1919 - F1Score: 0.3622 - WeightedF1: 0.4123 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1788 - accuracy: 0.1922 - F1Score: 0.3635 - WeightedF1: 0.4134 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1786 - accuracy: 0.1931 - F1Score: 0.3644 - WeightedF1: 0.4142 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1785 - accuracy: 0.1922 - F1Score: 0.3649 - WeightedF1: 0.4147 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "765/765 [==============================] - 143s 186ms/step - loss: 0.1781 - accuracy: 0.1935 - F1Score: 0.3657 - WeightedF1: 0.4157 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1779 - accuracy: 0.1930 - F1Score: 0.3674 - WeightedF1: 0.4172 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "765/765 [==============================] - 144s 189ms/step - loss: 0.1776 - accuracy: 0.1937 - F1Score: 0.3681 - WeightedF1: 0.4179 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1773 - accuracy: 0.1940 - F1Score: 0.3692 - WeightedF1: 0.4190 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1771 - accuracy: 0.1931 - F1Score: 0.3697 - WeightedF1: 0.4195 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1770 - accuracy: 0.1939 - F1Score: 0.3703 - WeightedF1: 0.4201 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1767 - accuracy: 0.1945 - F1Score: 0.3713 - WeightedF1: 0.4210 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1764 - accuracy: 0.1947 - F1Score: 0.3726 - WeightedF1: 0.4222 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1761 - accuracy: 0.1945 - F1Score: 0.3736 - WeightedF1: 0.4231 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1762 - accuracy: 0.1950 - F1Score: 0.3742 - WeightedF1: 0.4238 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1757 - accuracy: 0.1952 - F1Score: 0.3761 - WeightedF1: 0.4256 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1756 - accuracy: 0.1948 - F1Score: 0.3758 - WeightedF1: 0.4255 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.1752 - accuracy: 0.1947 - F1Score: 0.3772 - WeightedF1: 0.4267 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1750 - accuracy: 0.1938 - F1Score: 0.3776 - WeightedF1: 0.4271 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1748 - accuracy: 0.1961 - F1Score: 0.3789 - WeightedF1: 0.4285 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1745 - accuracy: 0.1951 - F1Score: 0.3798 - WeightedF1: 0.4291 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1745 - accuracy: 0.1975 - F1Score: 0.3802 - WeightedF1: 0.4297 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1743 - accuracy: 0.1961 - F1Score: 0.3808 - WeightedF1: 0.4302 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1740 - accuracy: 0.1955 - F1Score: 0.3824 - WeightedF1: 0.4318 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1737 - accuracy: 0.1961 - F1Score: 0.3836 - WeightedF1: 0.4327 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.1735 - accuracy: 0.1965 - F1Score: 0.3836 - WeightedF1: 0.4328 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1734 - accuracy: 0.1976 - F1Score: 0.3837 - WeightedF1: 0.4330 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1731 - accuracy: 0.1969 - F1Score: 0.3849 - WeightedF1: 0.4340 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1729 - accuracy: 0.1980 - F1Score: 0.3862 - WeightedF1: 0.4354 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1726 - accuracy: 0.1978 - F1Score: 0.3870 - WeightedF1: 0.4362 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "765/765 [==============================] - 147s 192ms/step - loss: 0.1726 - accuracy: 0.1983 - F1Score: 0.3870 - WeightedF1: 0.4362 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "765/765 [==============================] - 145s 189ms/step - loss: 0.1724 - accuracy: 0.1973 - F1Score: 0.3884 - WeightedF1: 0.4375 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1721 - accuracy: 0.1981 - F1Score: 0.3894 - WeightedF1: 0.4384 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1718 - accuracy: 0.1997 - F1Score: 0.3902 - WeightedF1: 0.4391 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1718 - accuracy: 0.1990 - F1Score: 0.3907 - WeightedF1: 0.4397 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1715 - accuracy: 0.1974 - F1Score: 0.3918 - WeightedF1: 0.4407 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "765/765 [==============================] - 145s 189ms/step - loss: 0.1713 - accuracy: 0.1987 - F1Score: 0.3924 - WeightedF1: 0.4413 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1710 - accuracy: 0.1985 - F1Score: 0.3931 - WeightedF1: 0.4421 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1709 - accuracy: 0.1993 - F1Score: 0.3943 - WeightedF1: 0.4432 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "765/765 [==============================] - 147s 192ms/step - loss: 0.1708 - accuracy: 0.1982 - F1Score: 0.3939 - WeightedF1: 0.4429 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1709 - accuracy: 0.1988 - F1Score: 0.3945 - WeightedF1: 0.4432 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1703 - accuracy: 0.1986 - F1Score: 0.3957 - WeightedF1: 0.4446 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1701 - accuracy: 0.1994 - F1Score: 0.3970 - WeightedF1: 0.4457 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1700 - accuracy: 0.1975 - F1Score: 0.3973 - WeightedF1: 0.4460 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1699 - accuracy: 0.1990 - F1Score: 0.3980 - WeightedF1: 0.4469 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1697 - accuracy: 0.2017 - F1Score: 0.3988 - WeightedF1: 0.4474 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.1696 - accuracy: 0.2012 - F1Score: 0.3994 - WeightedF1: 0.4479 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.1693 - accuracy: 0.2001 - F1Score: 0.3994 - WeightedF1: 0.4480 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "765/765 [==============================] - 145s 190ms/step - loss: 0.1691 - accuracy: 0.1992 - F1Score: 0.4008 - WeightedF1: 0.4493 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1690 - accuracy: 0.2008 - F1Score: 0.4008 - WeightedF1: 0.4494 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "765/765 [==============================] - 144s 189ms/step - loss: 0.1687 - accuracy: 0.2012 - F1Score: 0.4023 - WeightedF1: 0.4508 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "765/765 [==============================] - 146s 191ms/step - loss: 0.1686 - accuracy: 0.2006 - F1Score: 0.4027 - WeightedF1: 0.4512 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1685 - accuracy: 0.1998 - F1Score: 0.4026 - WeightedF1: 0.4510 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1682 - accuracy: 0.2020 - F1Score: 0.4040 - WeightedF1: 0.4524 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1681 - accuracy: 0.1998 - F1Score: 0.4046 - WeightedF1: 0.4530 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1678 - accuracy: 0.2025 - F1Score: 0.4057 - WeightedF1: 0.4540 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "765/765 [==============================] - 143s 188ms/step - loss: 0.1677 - accuracy: 0.2015 - F1Score: 0.4056 - WeightedF1: 0.4539 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "765/765 [==============================] - 144s 188ms/step - loss: 0.1676 - accuracy: 0.2006 - F1Score: 0.4067 - WeightedF1: 0.4550 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1673 - accuracy: 0.2010 - F1Score: 0.4071 - WeightedF1: 0.4556 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1672 - accuracy: 0.2030 - F1Score: 0.4075 - WeightedF1: 0.4558 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1670 - accuracy: 0.2013 - F1Score: 0.4083 - WeightedF1: 0.4567 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1669 - accuracy: 0.2020 - F1Score: 0.4083 - WeightedF1: 0.4566 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1666 - accuracy: 0.2015 - F1Score: 0.4094 - WeightedF1: 0.4577 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1665 - accuracy: 0.2033 - F1Score: 0.4100 - WeightedF1: 0.4583 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1663 - accuracy: 0.2012 - F1Score: 0.4105 - WeightedF1: 0.4587 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1664 - accuracy: 0.2025 - F1Score: 0.4100 - WeightedF1: 0.4583 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1660 - accuracy: 0.2021 - F1Score: 0.4117 - WeightedF1: 0.4602 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1659 - accuracy: 0.2020 - F1Score: 0.4121 - WeightedF1: 0.4604 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1659 - accuracy: 0.2023 - F1Score: 0.4128 - WeightedF1: 0.4609 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "765/765 [==============================] - 143s 187ms/step - loss: 0.1656 - accuracy: 0.2028 - F1Score: 0.4128 - WeightedF1: 0.4609 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "604/765 [======================>.......] - ETA: 30s - loss: 0.1653 - accuracy: 0.2026 - F1Score: 0.4137 - WeightedF1: 0.4618"
     ]
    }
   ],
   "source": [
    "num_splits = 3\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "for fold,(train_index, test_index) in tqdm(enumerate(kf.split(X))):\n",
    "    # print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # normalization\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    model_predictions = []\n",
    "\n",
    "    print(batch_size, epochs)\n",
    "    for i in range(5):\n",
    "\n",
    "        model = Classifier_INCEPTION(output_directory=f\"./models/{fold}/\", input_shape=(window_size, 1), nb_classes=78, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=6, iteration=i)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(X_test)\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    y_pred = np.mean(model_predictions, axis=0)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test, y_pred))\n",
    "    t = (y_pred_tf, y_test, y_pred)\n",
    "    with open(f\"./predictions/test_inception_pickle_{fold}.pkl\", 'wb') as f:\n",
    "        pickle.dump(t, f)\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    \n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / num_splits\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_3fold.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109293f5d594455ca8e2e7bc9fa131ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [    0     1     2 ... 49997 49998 49999] Test indices: [    4     6     7 ... 49988 49990 49992]\n",
      "Epoch 1/20\n",
      "313/313 [==============================] - 8s 17ms/step - loss: 0.3389 - F1Score: 4.3002e-05 - WeightedF1: 4.3906e-05 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 5s 18ms/step - loss: 0.3194 - F1Score: 0.0114 - WeightedF1: 0.0118 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.3113 - F1Score: 0.0300 - WeightedF1: 0.0309 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.3057 - F1Score: 0.0460 - WeightedF1: 0.0472 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 5s 18ms/step - loss: 0.2994 - F1Score: 0.0646 - WeightedF1: 0.0662 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2919 - F1Score: 0.0894 - WeightedF1: 0.0915 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2800 - F1Score: 0.1302 - WeightedF1: 0.1326 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2580 - F1Score: 0.2089 - WeightedF1: 0.2122 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.2140 - F1Score: 0.3957 - WeightedF1: 0.4002 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.1441 - F1Score: 0.6630 - WeightedF1: 0.6660 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0808 - F1Score: 0.8384 - WeightedF1: 0.8399 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 5s 18ms/step - loss: 0.0474 - F1Score: 0.9130 - WeightedF1: 0.9137 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0308 - F1Score: 0.9472 - WeightedF1: 0.9476 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0221 - F1Score: 0.9634 - WeightedF1: 0.9637 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0179 - F1Score: 0.9708 - WeightedF1: 0.9710 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0150 - F1Score: 0.9761 - WeightedF1: 0.9763 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0137 - F1Score: 0.9786 - WeightedF1: 0.9787 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0119 - F1Score: 0.9811 - WeightedF1: 0.9813 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0111 - F1Score: 0.9828 - WeightedF1: 0.9830 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0100 - F1Score: 0.9845 - WeightedF1: 0.9846 - lr: 3.0000e-04\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "{'precision': 0.28977932327533545, 'recall': 0.19972602739726028, 'f1-score': 0.2330580581913171, 'support': 80300}\n",
      "Train indices: [    2     3     4 ... 49997 49998 49999] Test indices: [    0     1    11 ... 49979 49986 49993]\n",
      "Epoch 1/20\n",
      "313/313 [==============================] - 7s 18ms/step - loss: 0.3376 - F1Score: 2.4736e-04 - WeightedF1: 2.6942e-04 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.3190 - F1Score: 0.0135 - WeightedF1: 0.0146 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.3108 - F1Score: 0.0320 - WeightedF1: 0.0341 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.3051 - F1Score: 0.0471 - WeightedF1: 0.0495 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.2984 - F1Score: 0.0677 - WeightedF1: 0.0705 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.2902 - F1Score: 0.0936 - WeightedF1: 0.0966 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      " 73/313 [=====>........................] - ETA: 4s - loss: 0.2772 - F1Score: 0.1311 - WeightedF1: 0.1342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\foglo\\Documents\\IJS\\energy-knowledge-graph\\scripts\\notebooks\\multi-label classification\\models.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m0.0000002\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# for class weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights_UKD, callbacks=[lr_scheduler])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# class_weights = NUK.class_weights_tool(y_train)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, callbacks\u001b[39m=\u001b[39;49m[lr_scheduler])\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    # break\n",
    "\n",
    "    model = NUK.PC0_reg(NmDevices, window_size, function, 128, k, lambda_l2=lambda_l2)\n",
    "    # model = NUK.VGG11_1D(NmDevices, window_size)\n",
    "    model.build((len(y_train) + len(y_test), window_size, 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1,patience=2, min_lr=0.0000002)\n",
    "    # for class weights\n",
    "    # model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights_UKD, callbacks=[lr_scheduler])\n",
    "    # class_weights = NUK.class_weights_tool(y_train)\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[lr_scheduler])\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test))\n",
    "\n",
    "    # n_labels = y_test.shape[1]\n",
    "    # co_occurrence_matrix = np.zeros((n_labels, n_labels))\n",
    "\n",
    "    # for true, pred in zip(y_test, y_pred_tf):\n",
    "    #     fn_labels = np.where((true == 1) & (pred == 0))[0]  # False negatives\n",
    "    #     fp_labels = np.where((true == 0) & (pred == 1))[0]  # False positives\n",
    "\n",
    "    #     for fn in fn_labels:\n",
    "    #         for fp in fp_labels:\n",
    "    #             co_occurrence_matrix[fn, fp] += 1\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c6686f14b44b6183215e04355a9509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan:  0 neg:  0 inf:  0 zero:  824442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = X\n",
    "\n",
    "\n",
    "count_zero = 0\n",
    "count_nan = 0\n",
    "count_neg = 0\n",
    "count_inf = 0\n",
    "n = range(len(X))\n",
    "for i in tqdm(n):\n",
    "    for j in range(len(data[i])):\n",
    "        if math.isnan(data[i][j]) or np.isnan(data[i][j]):\n",
    "            count_nan += 1\n",
    "        elif data[i][j] < 0:\n",
    "            count_neg += 1\n",
    "        elif math.isinf(data[i][j]):\n",
    "            count_inf += 1\n",
    "        elif data[i][j] == 0:\n",
    "            count_zero += 1\n",
    "\n",
    "    \n",
    "print(\"nan: \", count_nan, \"neg: \", count_neg, \"inf: \", count_inf, \"zero: \", count_zero)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48642.98, 0.0, 484.20068986207036, 950.5494532852276)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(), X.min(), X.mean(), X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save with pickle\n",
    "with open('predictions/predictions_real_normal.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [     1      2      3 ... 147053 147054 147055] Test indices: [     0      4     12 ... 147049 147050 147052]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "jobs = 110\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize the Random Forest classifier wrapped in a MultiOutputClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=jobs)\n",
    "    # classifier = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "    evaluation_results.append(report)\n",
    "\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "# df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_staticLR_LSTM_watts_mixed100k.csv\")\n",
    "df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_classes{NmDevices}_watts_RF_mixed50k.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "model = NUK.PC0(NmDevices, window_size, 'GRU',128, k)\n",
    "model.build((len(y), window_size, 1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=[lr_scheduler])\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_tf = (y_pred > 0.5)\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "# report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "# report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "df = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.17.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.5.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2022.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "import catboost as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58177334d95422290ad96ca8b871f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [    0     1     2 ... 49997 49998 49999] Test indices: [    4     6     7 ... 49988 49990 49992]\n",
      "Learning rate set to 0.051213\n",
      "0:\tlearn: 0.6478853\ttotal: 545ms\tremaining: 4m 32s\n",
      "499:\tlearn: 0.2732736\ttotal: 3m 57s\tremaining: 0us\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "{'precision': 0.12109148155906953, 'recall': 0.021307596513075965, 'f1-score': 0.03282443454939719, 'support': 80300}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    # break\n",
    "\n",
    "    # Initialize CatBoost multilabel classifier\n",
    "    model = cb.CatBoostClassifier(\n",
    "        loss_function='MultiLogloss',\n",
    "        verbose=500,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        task_type=\"GPU\",\n",
    "        devices='0:1',\n",
    "        # class_weights=class_weights_all,\n",
    "        iterations=500,\n",
    "\n",
    "        )  # Add other hyperparameters if needed\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert predictions to boolean format similar to y_test for evaluation\n",
    "    y_pred_tf = (y_pred == 1)\n",
    "    print(y_pred)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    break\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/Inception_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_real_norm.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "# df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_classes{NmDevices}_watts_RF_mixed50k.csv\")\n",
    "    \n",
    "# evaluation_results+ evaluation_results[1].to_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
