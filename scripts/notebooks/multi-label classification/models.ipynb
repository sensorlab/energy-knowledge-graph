{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "By downloading and using the cuDNN conda packages, you accept the terms and conditions of the NVIDIA cuDNN EULA -\n",
      "  https://docs.nvidia.com/deeplearning/cudnn/sla/index.html\n",
      "\n",
      "done\n",
      "\u001b[33mWARNING: Skipping nilmtk as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping nilm_metadata as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/nilmtk/nilmtk@master\n",
      "  Cloning https://github.com/nilmtk/nilmtk (to revision master) to /tmp/pip-req-build-nhisx402\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilmtk /tmp/pip-req-build-nhisx402\n",
      "  Resolved https://github.com/nilmtk/nilmtk to commit b2c514479cef478cab872cb635056da08d5352a1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilmtk\n",
      "  Building wheel for nilmtk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilmtk: filename=nilmtk-0.4.0.dev1+git.b2c5144-py3-none-any.whl size=279177 sha256=7548499459516df5fececd9d2c607ef298afc414c183b34778c02bcb931728f6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1wvtmbs3/wheels/05/a4/8e/b767c3d1714f61fd30ec991c25780be8cc04c474da8205aeee\n",
      "Successfully built nilmtk\n",
      "Installing collected packages: nilmtk\n",
      "Successfully installed nilmtk-0.4.0.dev1+git.b2c5144\n",
      "Collecting git+https://github.com/nilmtk/nilm_metadata@master\n",
      "  Cloning https://github.com/nilmtk/nilm_metadata (to revision master) to /tmp/pip-req-build-7hg_g19c\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nilmtk/nilm_metadata /tmp/pip-req-build-7hg_g19c\n",
      "  Resolved https://github.com/nilmtk/nilm_metadata to commit 7ed4bab9062d04cb35c6b6000b451715dc5ab4af\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nilm-metadata\n",
      "  Building wheel for nilm-metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nilm-metadata: filename=nilm_metadata-0.2.5-py3-none-any.whl size=24340 sha256=7bc4f3e589a4894d5ee224897a776c9d024ed8c4776c6120505ba6e655955d0d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8386wna3/wheels/7e/2f/7f/e41975c202542dbc11228875ff8473ed4cb95e04a274222bb7\n",
      "Successfully built nilm-metadata\n",
      "Installing collected packages: nilm-metadata\n",
      "Successfully installed nilm-metadata-0.2.5\n"
     ]
    }
   ],
   "source": [
    "#!rm -r /home/jovyan/.local/share/jupyter/kernels/nilmtk\n",
    "!mamba install tensorflow-gpu==2.10 -y -q\n",
    "# !mamba install tensorflow-gpu==2.11.0 -y -q\n",
    "!pip uninstall -y -q nilmtk nilm_metadata\n",
    "#!pip install -q pandas networkx tables scikit-learn hmmlearn pyyaml matplotlib==3.1.3 xgboost pyts\n",
    "# Trick to install NILM regardless of its dependencies\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilm_metadata.git\n",
    "#!pip install --no-cache -U git+https://github.com/nilmtk/nilmtk.git@0.4.3\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilmtk@master\n",
    "!python3 -m pip install --no-deps git+https://github.com/nilmtk/nilm_metadata@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 08:45:25.289376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100 80GB PCIe MIG 3g.40gb, compute capability 8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from nilmtk import DataSet\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Memory\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "#from nilmtk import DataSet\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "!pip install -q scikit-learn-intelex\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "from sklearn import pipeline, metrics, linear_model, model_selection, multioutput, tree, ensemble, neural_network\n",
    "!pip install xgboost -q\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "#import graphviz\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from ann_visualizer.visualize import ann_viz\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix\n",
    "# GPU goes brrrrrrrrrrrrrrrrrrrr\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "!pip3 install tqdm -q\n",
    "from tqdm.notebook import tqdm\n",
    "import NUK\n",
    "\n",
    "# import garbage collector\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    print(physical_devices)\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU found, model will train on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "epochs =1200\n",
    "window_size = 2688\n",
    "batch_size = 512\n",
    "NmDevices = 64\n",
    "# table_of_options = [1,2,3,4]\n",
    "k = 0.125\n",
    "# Define a learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    return lr\n",
    "    if epoch == 0 or epoch == 1:\n",
    "        return lr\n",
    "    if epoch == 35:\n",
    "        return lr *0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "test_size = 0.2\n",
    "lambda_l2=0\n",
    "function = \"GRU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_100000_upper32_gap3600_numD64_ideal.pkl\")\n",
    "# data = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_50000_upper32_gap3600_numD64_ideal.pkl\") \n",
    "# data_syn_ideal = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_50000_upper32_gap3600_numD64_ideal.pkl\") \n",
    "# data_syn_unmetered = pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_50000_upper32_gap3600_numD64_unmetered.pkl\")\n",
    "\n",
    "labels = pd.read_pickle(\"../../Energy_graph/data/labels_new.pkl\")\n",
    "\n",
    "\n",
    "# Separate the tuples into X and y\n",
    "# X_syn_ideal = np.array([i[0] for i in data_syn_ideal])\n",
    "# y_syn_ideal = np.array([i[1] for i in data_syn_ideal])\n",
    "\n",
    "# X_syn_unmetered = np.array([i[0] for i in data_syn_unmetered])\n",
    "# y_syn_unmetered = np.array([i[1] for i in data_syn_unmetered])\n",
    "\n",
    "X = np.array([i[0] for i in data])\n",
    "y = np.array([i[1] for i in data])\n",
    "\n",
    "\n",
    "# X = np.concatenate((X, X_syn_ideal,X_syn_unmetered), axis=0)\n",
    "# y = np.concatenate((y, y_syn_ideal, y_syn_unmetered), axis=0)\n",
    "\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "# X_test = np.array([i[0] for i in data_test])\n",
    "# y_test = np.array([i[1] for i in data_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(X):\n",
    "    max_value = 0\n",
    "\n",
    "    for x in X:\n",
    "        v = np.max(x)\n",
    "        if v > max_value:\n",
    "            max_value = v\n",
    "\n",
    "    if max_value == 0:\n",
    "        return X\n",
    "    return X / max_value\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionTime class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2= pd.read_pickle(\"../../Energy_graph/data/training_data/synthetic/X_Y_wsize2688_numW_100000_upper32_gap3600_numD64_ideal.pkl\")\n",
    "# y2 = np.array([i[1] for i in data])\n",
    "class_weighs_pre = NUK.class_weights_tool(y)\n",
    "\n",
    "class Classifier_INCEPTION:\n",
    "\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, iteration, verbose=False, build=True, batch_size=64,\n",
    "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500, lr=0.001):\n",
    "\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "        self.nb_filters = nb_filters\n",
    "        self.use_residual = use_residual\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size - 1\n",
    "        self.callbacks = None\n",
    "        self.batch_size = batch_size\n",
    "        self.bottleneck_size = 32\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.iteration = iteration\n",
    "\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            # if (verbose == True):\n",
    "                # self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "\n",
    "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
    "\n",
    "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
    "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
    "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
    "\n",
    "        conv_list = []\n",
    "\n",
    "        for i in range(len(kernel_size_s)):\n",
    "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
    "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
    "                input_inception))\n",
    "\n",
    "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
    "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_6)\n",
    "\n",
    "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
    "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
    "                                         padding='same', use_bias=False)(input_tensor)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        x = input_layer\n",
    "        input_res = input_layer\n",
    "\n",
    "        for d in range(self.depth):\n",
    "\n",
    "            x = self._inception_module(x)\n",
    "\n",
    "            if self.use_residual and d % 3 == 2:\n",
    "                x = self._shortcut_layer(input_res, x)\n",
    "                input_res = x\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='sigmoid')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
    "                      metrics=['accuracy', NUK.F1Score, NUK.WeightedF1Score(class_weighs_pre)])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=7, min_lr=0.0001)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=9, restore_best_weights=True, monitor='loss')\n",
    "\n",
    "        file_path = self.output_directory + f'best_model_{self.iteration}.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "        # lr_scheduler = LearningRateScheduler(scheduler)\n",
    "        self.callbacks = [reduce_lr, model_checkpoint, early_stopping]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "       \n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
    "        else:\n",
    "            mini_batch_size = self.batch_size\n",
    "\n",
    "        self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, callbacks=self.callbacks, validation_split=0.1)\n",
    "\n",
    "\n",
    "        self.model.save(self.output_directory + f'last_model_{self.iteration}.hdf5')\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        model_path = self.output_directory + f'last_model_{self.iteration}.hdf5'\n",
    "        model = keras.models.load_model(model_path, custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
    "    #     # if return_df_metrics:\n",
    "    #     #     y_pred = np.argmax(y_pred, axis=1)\n",
    "    #     #     df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
    "    #     #     return df_metrics\n",
    "    #     # else:\n",
    "    #     #     test_duration = time.time() - start_time\n",
    "    #     #     save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
    "    #     #     return y_pred\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "itr = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    # print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    # model = NUK.PC0_reg(NmDevices, window_size, function, 128, k, lambda_l2=lambda_l2)\n",
    "    print(batch_size, epochs)\n",
    "    model = Classifier_INCEPTION(output_directory=\"./models/\", input_shape=(window_size, 1), nb_classes=78, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=6)\n",
    "    model.fit(X_train, y_train)\n",
    "    # model.model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test, y_pred))\n",
    "    t = (y_pred_tf, y_test, y_pred)\n",
    "    with open(f\"./predictions/test_inception_pickle_{itr}.pkl\", 'wb') as f:\n",
    "        pickle.dump(t, f)\n",
    "\n",
    "    itr += 1\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/Inception_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\foglo\\Documents\\IJS\\energy-knowledge-graph\\scripts\\notebooks\\multi-label classification\\models.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m evaluation_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m predictions \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m class_weights_all \u001b[39m=\u001b[39m NUK\u001b[39m.\u001b[39;49mclass_weights_tool(y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold,(train_index, test_index) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(kf\u001b[39m.\u001b[39msplit(X))):\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# print(\"Train indices:\", train_index, \"Test indices:\", test_index)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/foglo/Documents/IJS/energy-knowledge-graph/scripts/notebooks/multi-label%20classification/models.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m X[train_index], X[test_index]\n",
      "File \u001b[0;32m~/shared/to_vid/NUK.py:550\u001b[0m, in \u001b[0;36mclass_weights_tool\u001b[0;34m(y_test)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39m# this loop goes over the rows in the y_test dataset\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(y_test)):\n\u001b[1;32m    548\u001b[0m     \n\u001b[1;32m    549\u001b[0m     \u001b[39m# we count Trues in the whole column of y_test dataset\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mif\u001b[39;00m y_test[i][j] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m: count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m     \n\u001b[1;32m    552\u001b[0m \u001b[39m# we append Trues for the column of y_test dataset to the list light_weight\u001b[39;00m\n\u001b[1;32m    553\u001b[0m light_weight\u001b[39m.\u001b[39mappend(count)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_splits = 3\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "for fold,(train_index, test_index) in tqdm(enumerate(kf.split(X))):\n",
    "    # print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # normalization\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    model_predictions = []\n",
    "\n",
    "    print(batch_size, epochs)\n",
    "    for i in range(5):\n",
    "\n",
    "        model = Classifier_INCEPTION(output_directory=f\"./models/{fold}/\", input_shape=(window_size, 1), nb_classes=78, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=8, iteration=i)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(X_test)\n",
    "        model_predictions.append(y_pred)\n",
    "\n",
    "    y_pred = np.mean(model_predictions, axis=0)\n",
    "    y_pred_tf = (y_pred > 0.5)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test, y_pred))\n",
    "    t = (y_pred_tf, y_test, y_pred)\n",
    "    with open(f\"./predictions/test_inception_pickle_{fold}.pkl\", 'wb') as f:\n",
    "        pickle.dump(t, f)\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    \n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / num_splits\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_3fold.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "model_predictions = []\n",
    "\n",
    "print(batch_size, epochs)\n",
    "for i in range(5):\n",
    "\n",
    "    model = Classifier_INCEPTION(output_directory=f\"./models/performance/\", input_shape=(window_size, 1), nb_classes=NmDevices, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=10, iteration=i, kernel_size=128, kernel_size=128)\n",
    "    # model = keras.models.load_model(f\"./models/test/last_model_{i}.hdf5\", custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_predictions.append(y_pred)\n",
    "\n",
    "y_pred = np.mean(model_predictions, axis=0)\n",
    "y_pred_tf = (y_pred > 0.3)\n",
    "\n",
    "predictions.append((y_pred_tf, y_test, y_pred))\n",
    "t = (y_pred_tf, y_test, y_pred)\n",
    "with open(f\"./predictions/test_inception_pickle_performance_testing.pkl\", 'wb') as f:\n",
    "    pickle.dump(t, f)\n",
    "\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "print(report[\"weighted avg\"])\n",
    "\n",
    "evaluation_results.append(report)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_performance_testing.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 5\n",
      "Epoch 1/5\n",
      "207/207 [==============================] - 330s 2s/step - loss: 0.3249 - accuracy: 0.0859 - F1Score: 0.0203 - WeightedF1: 0.1213 - val_loss: 0.3521 - val_accuracy: 5.9544e-04 - val_F1Score: 0.0026 - val_WeightedF1: 0.0015 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2847 - accuracy: 0.1037 - F1Score: 0.0501 - WeightedF1: 0.2598 - val_loss: 0.3316 - val_accuracy: 0.0115 - val_F1Score: 0.0057 - val_WeightedF1: 0.0091 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2765 - accuracy: 0.1190 - F1Score: 0.0766 - WeightedF1: 0.3217 - val_loss: 0.3225 - val_accuracy: 0.0249 - val_F1Score: 0.0224 - val_WeightedF1: 0.0388 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2699 - accuracy: 0.1256 - F1Score: 0.1004 - WeightedF1: 0.3611 - val_loss: 0.2979 - val_accuracy: 0.0947 - val_F1Score: 0.1027 - val_WeightedF1: 0.2987 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "207/207 [==============================] - 286s 1s/step - loss: 0.2641 - accuracy: 0.1311 - F1Score: 0.1260 - WeightedF1: 0.3894 - val_loss: 0.3055 - val_accuracy: 0.0885 - val_F1Score: 0.1028 - val_WeightedF1: 0.3224 - lr: 0.0010\n",
      "58/58 [==============================] - 20s 340ms/step\n",
      "{'precision': 0.3441303811150662, 'recall': 0.18797617912282374, 'f1-score': 0.18487712925876545, 'support': 188742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:/tmp/nilmtk-jgsyvjt5.h5...done\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "window_size = 2688\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "model_predictions = []\n",
    "\n",
    "print(batch_size, epochs)\n",
    "for i in range(1):\n",
    "\n",
    "    model = Classifier_INCEPTION(output_directory=f\"./models/performance/\", input_shape=(window_size, 1), nb_classes=NmDevices, verbose=True, build=True, batch_size=batch_size, nb_epochs=epochs, lr=0.001, depth=10, iteration=i, kernel_size=128)\n",
    "    # model = keras.models.load_model(f\"./models/test/last_model_{i}.hdf5\", custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_predictions.append(y_pred)\n",
    "\n",
    "y_pred = np.mean(model_predictions, axis=0)\n",
    "y_pred_tf = (y_pred > 0.3)\n",
    "\n",
    "predictions.append((y_pred_tf, y_test, y_pred))\n",
    "t = (y_pred_tf, y_test, y_pred)\n",
    "with open(f\"./predictions/test_inception_pickle_performance_testing.pkl\", 'wb') as f:\n",
    "    pickle.dump(t, f)\n",
    "\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "print(report[\"weighted avg\"])\n",
    "\n",
    "evaluation_results.append(report)\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/NNE_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_performance_testing.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = keras.layers.Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv1D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
    "    y = keras.layers.Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(y)\n",
    "    out = keras.layers.Dense(nb_classes, activation='sigmoid')(full)\n",
    "    print ('        -- model was built.')\n",
    "    # input = keras.layers.Input(shape=(input_shape))\n",
    "    model = keras.models.Model(inputs=x, outputs=out, name=\"Resnet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "treshold = 0.5\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train[:, 0] = normalize(X_train[:, 0])\n",
    "    X_test[:,0] = normalize(X_test[:, 0])\n",
    "\n",
    "    model = build_resnet((window_size, 1), 32, NmDevices)\n",
    "    # model.build((len(y_train) + len(y_test), window_size, 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=3, min_lr=0.000001)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True, monitor='loss')\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduce_lr,early_stopping])\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > treshold)\n",
    "\n",
    "    predictions.append((y_pred, y_pred_tf, y_test))\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"], \"|||\", report[\"samples avg\"])\n",
    "\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "df = df / num_splits\n",
    "import os\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/ResNet_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_mixed_t0.3_d10.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PC0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "treshold = 0.5\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train[:, 0] = normalize(X_train[:, 0])\n",
    "    X_test[:,0] = normalize(X_test[:, 0])\n",
    "\n",
    "    # break\n",
    "    # class_weights_all = NUK.class_weights_tool(y_train)\n",
    "    # for k in class_weights_all:\n",
    "    #     class_weights_all[k] += 1\n",
    "    \n",
    "    # print(class_weights_all)\n",
    "    # break\n",
    "    model = NUK.PC0_reg(NmDevices, window_size, function, 128, k, lambda_l2=lambda_l2)\n",
    "    # model = NUK.VGG11_1D(NmDevices, window_size)\n",
    "    model.build((len(y_train) + len(y_test), window_size, 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "    # lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=3, min_lr=0.000001)\n",
    "    # callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1,patience=2, min_lr=0.0000002)\n",
    "    # for class weights\n",
    "    # model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weights_UKD, callbacks=[lr_scheduler])\n",
    "    # class_weights = NUK.class_weights_tool(y_train)\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduce_lr])\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tf = (y_pred > treshold)\n",
    "\n",
    "    predictions.append((y_pred, y_pred_tf, y_test))\n",
    "\n",
    "    # n_labels = y_test.shape[1]\n",
    "    # co_occurrence_matrix = np.zeros((n_labels, n_labels))\n",
    "\n",
    "    # for true, pred in zip(y_test, y_pred_tf):\n",
    "    #     fn_labels = np.where((true == 1) & (pred == 0))[0]  # False negatives\n",
    "    #     fp_labels = np.where((true == 0) & (pred == 1))[0]  # False positives\n",
    "\n",
    "    #     for fn in fn_labels:\n",
    "    #         for fp in fp_labels:\n",
    "    #             co_occurrence_matrix[fn, fp] += 1\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"], \"|||\", report[\"samples avg\"])\n",
    "\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSMixer\n",
    "https://github.com/google-research/google-research/tree/master/tsmixer/tsmixer_basic/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement TSMixer for multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [     1      2      3 ... 147053 147054 147055] Test indices: [     0      4     12 ... 147049 147050 147052]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "jobs = 110\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize the Random Forest classifier wrapped in a MultiOutputClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=jobs)\n",
    "    # classifier = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "    evaluation_results.append(report)\n",
    "\n",
    "\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "# df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_staticLR_LSTM_watts_mixed100k.csv\")\n",
    "df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_classes{NmDevices}_watts_RF_mixed50k.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "model = NUK.PC0(NmDevices, window_size, 'GRU',128, k)\n",
    "model.build((len(y), window_size, 1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=[NUK.F1Score, NUK.WeightedF1Score(NUK.class_weights_tool(y_test))])\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=[lr_scheduler])\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_tf = (y_pred > 0.5)\n",
    "report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "# report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "# report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "df = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.17.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.5.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2022.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "import catboost as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58177334d95422290ad96ca8b871f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [    0     1     2 ... 49997 49998 49999] Test indices: [    4     6     7 ... 49988 49990 49992]\n",
      "Learning rate set to 0.051213\n",
      "0:\tlearn: 0.6478853\ttotal: 545ms\tremaining: 4m 32s\n",
      "499:\tlearn: 0.2732736\ttotal: 3m 57s\tremaining: 0us\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "{'precision': 0.12109148155906953, 'recall': 0.021307596513075965, 'f1-score': 0.03282443454939719, 'support': 80300}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "evaluation_results = []\n",
    "predictions = []\n",
    "class_weights_all = NUK.class_weights_tool(y)\n",
    "debug = 0\n",
    "for train_index, test_index in tqdm(kf.split(X)):\n",
    "    print(\"Train indices:\", train_index, \"Test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    # break\n",
    "\n",
    "    # Initialize CatBoost multilabel classifier\n",
    "    model = cb.CatBoostClassifier(\n",
    "        loss_function='MultiLogloss',\n",
    "        verbose=500,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        task_type=\"GPU\",\n",
    "        devices='0:1',\n",
    "        # class_weights=class_weights_all,\n",
    "        iterations=500,\n",
    "\n",
    "        )  # Add other hyperparameters if needed\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert predictions to boolean format similar to y_test for evaluation\n",
    "    y_pred_tf = (y_pred == 1)\n",
    "    print(y_pred)\n",
    "\n",
    "    predictions.append((y_pred_tf, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    report = metrics.classification_report(y_test, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(report[\"weighted avg\"])\n",
    "    break\n",
    "    # report_DF = NUK.ClassificationReportToDF(report, labels)  \n",
    "    # report_DF.rename(columns={report_DF.columns[0]: \"device\"}, inplace=True)\n",
    "    evaluation_results.append(report)\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in range(len(evaluation_results)):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(evaluation_results[i]).T\n",
    "    else:\n",
    "        df = df + pd.DataFrame(evaluation_results[i]).T\n",
    "\n",
    "\n",
    "df = df / 5\n",
    "# check if directory exsits\n",
    "if not os.path.exists(f\"../../Energy_graph/data/results/{window_size}\"):\n",
    "    os.makedirs(f\"../../Energy_graph/data/results/{window_size}\")\n",
    "if lambda_l2 == 0:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/{window_size}/PC0_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_mix.csv\")\n",
    "else:\n",
    "    df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_epochs{epochs}_batch_size{batch_size}_k{k}_classes{NmDevices}_{function}_watts_syn_reg{lambda_l2}.csv\")\n",
    "# df.to_csv(f\"../../Energy_graph/data/results/w_size{window_size}_classes{NmDevices}_watts_RF_mixed50k.csv\")\n",
    "    \n",
    "# evaluation_results+ evaluation_results[1].to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2688, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 2688, 1)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 2688, 32)     1280        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 2688, 32)     640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 2688, 32)     320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 2688, 32)     32          ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2688, 128)    0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 2688, 128)   512         ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 2688, 128)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 2688, 32)     4096        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 2688, 128)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 2688, 32)     40960       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 2688, 32)     20480       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 2688, 32)     10240       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 2688, 32)     4096        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2688, 128)    0           ['conv1d_5[0][0]',               \n",
      "                                                                  'conv1d_6[0][0]',               \n",
      "                                                                  'conv1d_7[0][0]',               \n",
      "                                                                  'conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 2688, 128)   512         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 2688, 128)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 2688, 32)     4096        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 2688, 128)    0           ['conv1d_10[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]',              \n",
      "                                                                  'conv1d_12[0][0]',              \n",
      "                                                                  'conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 2688, 128)    128         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 2688, 128)   512         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 2688, 128)   512         ['conv1d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 2688, 128)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 2688, 128)    0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 2688, 128)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 2688, 32)     4096        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 2688, 128)    0           ['conv1d_16[0][0]',              \n",
      "                                                                  'conv1d_17[0][0]',              \n",
      "                                                                  'conv1d_18[0][0]',              \n",
      "                                                                  'conv1d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 2688, 128)   512         ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 2688, 128)    0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 2688, 32)     4096        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 2688, 128)    0           ['conv1d_21[0][0]',              \n",
      "                                                                  'conv1d_22[0][0]',              \n",
      "                                                                  'conv1d_23[0][0]',              \n",
      "                                                                  'conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 2688, 128)   512         ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 2688, 128)    0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 2688, 32)     4096        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 2688, 128)    0           ['conv1d_26[0][0]',              \n",
      "                                                                  'conv1d_27[0][0]',              \n",
      "                                                                  'conv1d_28[0][0]',              \n",
      "                                                                  'conv1d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 2688, 128)    16384       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 2688, 128)   512         ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 2688, 128)   512         ['conv1d_30[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 2688, 128)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 2688, 128)    0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 2688, 128)    0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 2688, 32)     4096        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 2688, 128)    0           ['conv1d_32[0][0]',              \n",
      "                                                                  'conv1d_33[0][0]',              \n",
      "                                                                  'conv1d_34[0][0]',              \n",
      "                                                                  'conv1d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 2688, 128)   512         ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 2688, 128)    0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 2688, 32)     4096        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_7[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 2688, 128)    0           ['conv1d_37[0][0]',              \n",
      "                                                                  'conv1d_38[0][0]',              \n",
      "                                                                  'conv1d_39[0][0]',              \n",
      "                                                                  'conv1d_40[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 2688, 128)   512         ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 2688, 128)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)             (None, 2688, 32)     4096        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_42 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_45 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 2688, 128)    0           ['conv1d_42[0][0]',              \n",
      "                                                                  'conv1d_43[0][0]',              \n",
      "                                                                  'conv1d_44[0][0]',              \n",
      "                                                                  'conv1d_45[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_46 (Conv1D)             (None, 2688, 128)    16384       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 2688, 128)   512         ['concatenate_8[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 2688, 128)   512         ['conv1d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 2688, 128)    0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 2688, 128)    0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 2688, 128)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)             (None, 2688, 32)     4096        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 2688, 128)   0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_48 (Conv1D)             (None, 2688, 32)     40960       ['conv1d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_49 (Conv1D)             (None, 2688, 32)     20480       ['conv1d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_50 (Conv1D)             (None, 2688, 32)     10240       ['conv1d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_51 (Conv1D)             (None, 2688, 32)     4096        ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2688, 128)    0           ['conv1d_48[0][0]',              \n",
      "                                                                  'conv1d_49[0][0]',              \n",
      "                                                                  'conv1d_50[0][0]',              \n",
      "                                                                  'conv1d_51[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 2688, 128)   512         ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 2688, 128)    0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['activation_12[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 768,928\n",
      "Trainable params: 765,600\n",
      "Non-trainable params: 3,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"./models/test/last_model_4.hdf5\", custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../Energy_graph/data/processed_watts/REFIT_clean.pkl\")\n",
    "df[\"REFIT_1\"]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../../Energy_graph/data/processed_watts/refit/REFIT1_clean.pkl\", 'wb') as f:\n",
    "    pickle.dump({\"REFIT_1\" : df[\"REFIT_1\"]}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "# from helper import preprocess_string\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_string(string : str) -> str:\n",
    "    string = string.lower().strip()\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    string = string.replace(\"-\", \" \")\n",
    "    string = string.replace(\"&\", \" \")\n",
    "    string = string.split(\"(\")[0]\n",
    "    string = string.split(\"#\")[0]\n",
    "\n",
    "    string = string.strip()\n",
    "\n",
    "    # handle known synoynms\n",
    "    synonyms = {\n",
    "        \"refrigerator\": \"fridge\",\n",
    "        \"vaccumcleaner\": \"vacuum cleaner\",\n",
    "        \"breadmaker\": \"bread maker\",\n",
    "      \n",
    "        \n",
    "    }\n",
    "    if \"freezer\" in string:\n",
    "        string = \"fridge\"\n",
    "\n",
    "    if string in synonyms:\n",
    "        string = synonyms[string]\n",
    "\n",
    "    if 'hi fi' in string:\n",
    "        string = \"audio system\"\n",
    "\n",
    "    if \"router\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    if \"treadmill\" in string:\n",
    "        string = \"running machine\"\n",
    "        \n",
    "\n",
    "    if \"laptop\" in string:\n",
    "        string = \"laptop\"\n",
    "    \n",
    "    if \"server\" in string:\n",
    "        string = \"server\"\n",
    "\n",
    "    if \"monitor\" in string and not \"baby\" in string:\n",
    "        string = \"monitor\"\n",
    "    # special cases\n",
    "    if \"computer\" in string and \"charger\" not in string:\n",
    "        string = \"pc\"\n",
    "\n",
    "    if \"tv\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"television\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"macbook\" in string:\n",
    "        string = \"laptop\"\n",
    "        \n",
    "    if \"car charger\" == string:\n",
    "        string = \"ev\"\n",
    "    \n",
    "    if \"toast\" in string:\n",
    "        string = \"toaster\"\n",
    "    \n",
    "    if \"modem\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    # we treat all audio devices as speakers so subwoofer is also a speaker\n",
    "    if \"subwoofer\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"speaker\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"iron\" in string and \"soldering\" not in string:\n",
    "        string = \"iron\"\n",
    "\n",
    "    \n",
    "    if \"coffeemachine\" in string:\n",
    "        string = \"coffee machine\"\n",
    "    if \"coffee maker\" in string:\n",
    "        string = \"coffee machine\"\n",
    "\n",
    "    if \"dishwasher\" in string:\n",
    "        string = \"dish washer\"\n",
    "    if \"air conditioner\" in string:\n",
    "        string = \"ac\"\n",
    "\n",
    "    if \"air conditioning\" in string:\n",
    "        string = \"ac\"\n",
    "    \n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = re.sub(r'\\d+', '', string)\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_dictionary(data: dict, values=0) -> pd.DataFrame:\n",
    "\n",
    "    ignored_devices = [\n",
    "        \"light\",\n",
    "        \"outlet\",\n",
    "        \"sockets\",\n",
    "        \"lamp\",\n",
    "        \"plug\",\n",
    "        'CE appliance'\n",
    "        'kettle/toaster',\n",
    "        'dehumidifier/heater',\n",
    "        'HairDryer-Straightener',\n",
    "        'Office Desk',\n",
    "        'heat basement',\n",
    "        'set top box',\n",
    "        'subpanel',\n",
    "    ]\n",
    "    dfs = []\n",
    "    for device in data:\n",
    "        # ignore devices\n",
    "        if any(ignored_device in device.lower() for ignored_device in ignored_devices):\n",
    "            continue\n",
    "        if device == \"aggregate\":\n",
    "            continue\n",
    "        # preprocess device name\n",
    "        device_name = preprocess_string(device)\n",
    "        \n",
    "        df = data[device]\n",
    "        df = df.resample(\"8s\").mean()\n",
    "\n",
    "        # rename column to standardized device name\n",
    "        df.columns = [device_name]\n",
    "        if df.max().max() < 2:\n",
    "            print(\"device with zeros: \", device_name)\n",
    "            continue\n",
    "\n",
    "        time_diffs = df.index.to_series().diff()\n",
    "        median_interval = time_diffs.median()\n",
    "\n",
    "        # if there is less than 3 days of data drop the device\n",
    "        if len(df) < (3*24 * 60 * 60) / median_interval.total_seconds():\n",
    "            print(\"less than 3 days of data for device: \", device_name)\n",
    "            continue\n",
    "        df.dropna(inplace=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # concatenate all dataframes\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # handle missing values\n",
    "    df = df.ffill(limit=6)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # handle negative values\n",
    "    df[df<0] = 0\n",
    "\n",
    "    df[\"aggregate\"] = df.sum(axis=1)\n",
    "    # df.drop(columns=[\"aggregate\"], inplace=True)\n",
    "\n",
    "    # df.rename(columns={\"sum_ideal\": \"aggregate\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # treshold in watts\n",
    "    treshold = 5\n",
    "    if values == 0:\n",
    "        # put 1 if device is on and 0 if device is off\n",
    "        for c in df.columns:\n",
    "            if c == \"aggregate\":\n",
    "                continue\n",
    "            # if power is less than treshold device is off\n",
    "            df[c] = (df[c] > treshold).astype(int)\n",
    "\n",
    "    # find duplicate columns\n",
    "    column_counts = Counter(df.columns)\n",
    "    duplicates = [col for col, count in column_counts.items() if count > 1]\n",
    "    # Sum duplicate columns\n",
    "    for duplicate in duplicates:\n",
    "        duplicate_cols = [col for i, col in enumerate(df.columns) if col == duplicate]\n",
    "        df[duplicate] = df[duplicate_cols].sum(axis=1)\n",
    "        # Drop other duplicate columns if needed\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep='last')]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def process_dataset(dataset_path):\n",
    "    data = pd.read_pickle(dataset_path)\n",
    "    # print(dataset_path)\n",
    "    for house in data:\n",
    "        data[house] = process_dictionary(data[house], house)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data(path : str, labels_path : str, values=0):\n",
    "        \n",
    "    # path = \"./Energy_graph/data/processed_watts/\"\n",
    "    dataset_paths = [os.path.join(path, dataset) for dataset in os.listdir(path) if dataset.endswith('.pkl')]\n",
    "        \n",
    "    cpu_count = int(os.cpu_count() / 2)\n",
    "    data_dict = {}\n",
    "\n",
    "    with tqdm(total=len(dataset_paths), desc=\"Processing datasets\", unit=\"dataset\") as progress_bar:\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count) as executor:\n",
    "            futures = {executor.submit(process_dataset, dataset_path): dataset_path for dataset_path in dataset_paths}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                dataset_path = futures[future]\n",
    "                try:\n",
    "                    processed_data = future.result()\n",
    "                    data_dict.update(processed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Dataset {dataset_path} generated an exception: {e}\")\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "\n",
    "    labels = pd.read_pickle(labels_path)\n",
    "    labels.sort()\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def create_windows(data : dict, labels_path: str, save_path : str, time_window=2700, upper_bound=pd.Timedelta(seconds=32), max_gap = pd.Timedelta(seconds=3600)):\n",
    "    \"\"\"Creates windows of time_window seconds from the data and discards windows with gaps of more than 1h or 15 gaps of 32 seconds or more\"\"\"\n",
    "    \n",
    "    labels = pd.read_pickle(labels_path)\n",
    "    # windows = []\n",
    "    X_Y = [] # list of tuples (X, Y)\n",
    "    skip_count_1 = 0\n",
    "    skip_count_2 = 0\n",
    "    skip_count_3 = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for df in data.values():\n",
    "        print(len(df[\"aggregate\"]))\n",
    "        for i in range(0, len(df) - time_window, time_window + 1):\n",
    "            window = df.iloc[i:i + time_window]\n",
    "            total_count += 1\n",
    "            # if there is a gap of more than max_gap skip the window\n",
    "            time_diffs = window.index.to_series().diff().dropna()\n",
    "            if  (time_diffs >= max_gap).any():\n",
    "                skip_count_1 += 1\n",
    "                continue\n",
    "            # if there are more than 15 gaps of upper_bound or more skip the window\n",
    "            if len(time_diffs[time_diffs > upper_bound]) > 15:\n",
    "                skip_count_2 += 1\n",
    "                continue\n",
    "\n",
    "            x = window[\"aggregate\"].values\n",
    "            # if there is a value bigger than 50000 skip the window\n",
    "            if (x > 50000).any():\n",
    "                skip_count_3 += 1\n",
    "                continue\n",
    "            devices = [False] * len(labels)\n",
    "            # check if device is on in the window\n",
    "            for c in window.columns:\n",
    "                if c == \"aggregate\":\n",
    "                    continue\n",
    "                on = (window[c] > 0)\n",
    "                ix = labels.index(c)\n",
    "                devices[ix] = on.any()\n",
    "\n",
    "            X_Y.append((x, devices))\n",
    "            \n",
    "\n",
    "\n",
    "            # windows.append(window)\n",
    "    print(\"Total windows: \", total_count, \"Skipped windows due to 30min gap: \", skip_count_1, \"Skipped windows due to 15 gaps of 32s or more: \", skip_count_2 ,\"Skipped windows due to values larger than 50k: \", skip_count_3 ,\"Procentage skipped: \", (skip_count_1+skip_count_2+ skip_count_3) / total_count * 100)\n",
    "    return X_Y\n",
    "    # with open(save_path+ f\"/X_Y_wsize{time_window}_upper{int(upper_bound.total_seconds())}_gap{int(max_gap.total_seconds())}_numD{len(labels)}.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(X_Y, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to base folder:  ../../Energy_graph/\n",
      "Path to save windows:  ../../Energy_graph//data/training_data/processed/\n",
      "Time window:  2688 rows |  21504 seconds\n",
      "Upper bound:  0 days 00:00:32\n",
      "Max gap:  0 days 01:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/10 [00:00<?, ?dataset/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  50%|     | 5/10 [00:06<00:04,  1.04dataset/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 3 days of data for device:  games console\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|| 10/10 [02:10<00:00, 13.07s/dataset]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = \"\"\n",
    "# Initialize paths\n",
    "path_to_base = \"../../Energy_graph/\"\n",
    "path = path_to_base + \"/data/processed_watts/\"+folder\n",
    "labels_path = path_to_base + \"/data/labels_new.pkl\"\n",
    "save_path = path_to_base + \"/data/training_data/processed/\"+folder\n",
    "\n",
    "# Initialize parameters\n",
    "time_window = 2688\n",
    "upper_bound = pd.Timedelta(seconds=32)\n",
    "max_gap = pd.Timedelta(seconds=3600)\n",
    "\n",
    "# Print parameters\n",
    "print(\"Path to base folder: \", path_to_base)\n",
    "print(\"Path to save windows: \", save_path)\n",
    "print(\"Time window: \", time_window, \"rows | \", time_window*8, \"seconds\")\n",
    "print(\"Upper bound: \", upper_bound)\n",
    "print(\"Max gap: \", max_gap)\n",
    "\n",
    "# Get data and create windows\n",
    "data = get_data(path, labels_path)\n",
    "# create_windows(data, labels_path, save_path, time_window, upper_bound, max_gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAWE_1 620447\n",
      "620447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 1/64 [00:00<00:20,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  230 Skipped windows due to 30min gap:  14 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  6.086956521739131\n",
      "HES_1 1152268\n",
      "1152268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 3/64 [00:01<00:24,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  428 Skipped windows due to 30min gap:  5 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.1682242990654206\n",
      "REDD_1 358273\n",
      "358273\n",
      "Total windows:  133 Skipped windows due to 30min gap:  5 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.7593984962406015\n",
      "REDD_2 148341\n",
      "148341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 5/64 [00:01<00:12,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  55 Skipped windows due to 30min gap:  3 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  5.454545454545454\n",
      "REDD_3 192499\n",
      "192499\n",
      "Total windows:  71 Skipped windows due to 30min gap:  9 Skipped windows due to 15 gaps of 32s or more:  1 Skipped windows due to values larger than 50k:  0 Procentage skipped:  14.084507042253522\n",
      "REDD_4 272058\n",
      "272058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 6/64 [00:01<00:10,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  101 Skipped windows due to 30min gap:  6 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  5.9405940594059405\n",
      "REDD_5 39395\n",
      "39395\n",
      "Total windows:  14 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  28.57142857142857\n",
      "REDD_6 152445\n",
      "152445\n",
      "Total windows:  56 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  7.142857142857142\n",
      "DRED_1 1657798\n",
      "1657798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 9/64 [00:02<00:15,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  616 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "HEART_33 1004400\n",
      "1004400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 10/64 [00:03<00:18,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  373 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "HEART_7 1004400\n",
      "1004400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 11/64 [00:03<00:21,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  373 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "DEDDIAG_8 4500232\n",
      "4500232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 12/64 [00:05<00:37,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1673 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "ECO_1 2494800\n",
      "2494800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 13/64 [00:06<00:41,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  927 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.10787486515641855\n",
      "ECO_6 1998000\n",
      "1998000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 14/64 [00:07<00:41,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  743 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.2691790040376851\n",
      "ECO_2 2592000\n",
      "2592000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 15/64 [00:08<00:47,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  963 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.20768431983385255\n",
      "ECO_5 2354400\n",
      "2354400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 16/64 [00:09<00:47,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  875 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.1142857142857143\n",
      "ECO_4 2106000\n",
      "2106000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 17/64 [00:10<00:42,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  783 Skipped windows due to 30min gap:  3 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.38314176245210724\n",
      "ECO_3 1047600\n",
      "1047600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 18/64 [00:10<00:35,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  389 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5141388174807198\n",
      "ENERTALK_1 1211429\n",
      "1211429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|      | 20/64 [00:11<00:23,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  450 Skipped windows due to 30min gap:  14 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.111111111111111\n",
      "ENERTALK_18 509433\n",
      "509433\n",
      "Total windows:  189 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5291005291005291\n",
      "ENERTALK_12 1282152\n",
      "1282152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 21/64 [00:12<00:23,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  476 Skipped windows due to 30min gap:  6 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.2605042016806722\n",
      "ENERTALK_20 647946\n",
      "647946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 23/64 [00:12<00:14,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  240 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4166666666666667\n",
      "ENERTALK_15 495183\n",
      "495183\n",
      "Total windows:  184 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5434782608695652\n",
      "ENERTALK_6 464668\n",
      "464668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 24/64 [00:12<00:12,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  172 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5813953488372093\n",
      "ENERTALK_8 657789\n",
      "657789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 26/64 [00:13<00:09,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  244 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4098360655737705\n",
      "ENERTALK_2 334755\n",
      "334755\n",
      "Total windows:  124 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.0\n",
      "ENERTALK_11 322716\n",
      "322716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 27/64 [00:13<00:07,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  120 Skipped windows due to 30min gap:  0 Skipped windows due to 15 gaps of 32s or more:  1 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.8333333333333334\n",
      "ENERTALK_16 767570\n",
      "767570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 28/64 [00:13<00:07,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  285 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.4035087719298245\n",
      "ENERTALK_5 617543\n",
      "617543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 29/64 [00:13<00:07,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  229 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.43668122270742354\n",
      "ENERTALK_7 657441\n",
      "657441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 30/64 [00:13<00:07,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  244 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4098360655737705\n",
      "ENERTALK_14 1062395\n",
      "1062395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 31/64 [00:14<00:07,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  395 Skipped windows due to 30min gap:  10 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.5316455696202533\n",
      "ENERTALK_13 947873\n",
      "947873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 32/64 [00:14<00:08,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  352 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.1363636363636365\n",
      "ENERTALK_19 637122\n",
      "637122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 33/64 [00:14<00:07,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  236 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.847457627118644\n",
      "ENERTALK_21 657552\n",
      "657552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 34/64 [00:15<00:07,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  244 Skipped windows due to 30min gap:  1 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.4098360655737705\n",
      "ENERTALK_0 936253\n",
      "936253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 35/64 [00:15<00:09,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  348 Skipped windows due to 30min gap:  6 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.7241379310344827\n",
      "ENERTALK_4 906831\n",
      "906831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 36/64 [00:15<00:09,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  337 Skipped windows due to 30min gap:  7 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.0771513353115725\n",
      "ENERTALK_17 912460\n",
      "912460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 37/64 [00:16<00:09,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  339 Skipped windows due to 30min gap:  7 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.0648967551622417\n",
      "ENERTALK_10 1249051\n",
      "1249051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 38/64 [00:16<00:09,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  464 Skipped windows due to 30min gap:  8 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.7241379310344827\n",
      "ENERTALK_3 1215991\n",
      "1215991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 39/64 [00:16<00:09,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  452 Skipped windows due to 30min gap:  11 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.433628318584071\n",
      "ENERTALK_9 1293445\n",
      "1293445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 40/64 [00:17<00:09,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  481 Skipped windows due to 30min gap:  4 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.8316008316008316\n",
      "REFIT_13 4075515\n",
      "4075515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 41/64 [00:19<00:19,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1515 Skipped windows due to 30min gap:  35 Skipped windows due to 15 gaps of 32s or more:  124 Skipped windows due to values larger than 50k:  0 Procentage skipped:  10.495049504950495\n",
      "REFIT_6 5246422\n",
      "5246422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 42/64 [00:22<00:32,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1951 Skipped windows due to 30min gap:  22 Skipped windows due to 15 gaps of 32s or more:  32 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.7678113787801126\n",
      "REFIT_1 6003014\n",
      "6003014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 43/64 [00:25<00:40,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2232 Skipped windows due to 30min gap:  25 Skipped windows due to 15 gaps of 32s or more:  13 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.702508960573477\n",
      "REFIT_21 4770403\n",
      "4770403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 44/64 [00:27<00:41,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1774 Skipped windows due to 30min gap:  18 Skipped windows due to 15 gaps of 32s or more:  5 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.2965050732807215\n",
      "REFIT_8 5346837\n",
      "5346837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 45/64 [00:30<00:43,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1988 Skipped windows due to 30min gap:  18 Skipped windows due to 15 gaps of 32s or more:  11 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.4587525150905434\n",
      "REFIT_9 5161252\n",
      "5161252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 46/64 [00:33<00:44,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1919 Skipped windows due to 30min gap:  16 Skipped windows due to 15 gaps of 32s or more:  9 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.3027618551328817\n",
      "REFIT_20 4651687\n",
      "4651687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 47/64 [00:35<00:42,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1729 Skipped windows due to 30min gap:  12 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.156737998843262\n",
      "REFIT_7 5758682\n",
      "5758682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 48/64 [00:38<00:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2141 Skipped windows due to 30min gap:  24 Skipped windows due to 15 gaps of 32s or more:  10 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.5880429705744978\n",
      "REFIT_15 5262277\n",
      "5262277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 49/64 [00:41<00:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1956 Skipped windows due to 30min gap:  30 Skipped windows due to 15 gaps of 32s or more:  9 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.9938650306748467\n",
      "REFIT_12 4884120\n",
      "4884120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 50/64 [00:43<00:34,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1816 Skipped windows due to 30min gap:  16 Skipped windows due to 15 gaps of 32s or more:  43 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.248898678414097\n",
      "REFIT_4 5881600\n",
      "5881600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 51/64 [00:46<00:33,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2187 Skipped windows due to 30min gap:  40 Skipped windows due to 15 gaps of 32s or more:  27 Skipped windows due to values larger than 50k:  0 Procentage skipped:  3.063557384545039\n",
      "REFIT_3 5858072\n",
      "5858072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 52/64 [00:49<00:33,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2178 Skipped windows due to 30min gap:  20 Skipped windows due to 15 gaps of 32s or more:  11 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.423324150596878\n",
      "REFIT_18 4488860\n",
      "4488860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 53/64 [00:51<00:28,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1669 Skipped windows due to 30min gap:  16 Skipped windows due to 15 gaps of 32s or more:  4 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.1983223487118035\n",
      "REFIT_11 3745057\n",
      "3745057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 54/64 [00:53<00:24,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1392 Skipped windows due to 30min gap:  15 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.6522988505747127\n",
      "REFIT_16 4829646\n",
      "4829646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 55/64 [00:56<00:21,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1796 Skipped windows due to 30min gap:  32 Skipped windows due to 15 gaps of 32s or more:  6 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.1158129175946545\n",
      "REFIT_17 4733420\n",
      "4733420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 56/64 [00:58<00:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1760 Skipped windows due to 30min gap:  17 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.4204545454545454\n",
      "REFIT_10 5573669\n",
      "5573669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 57/64 [01:01<00:17,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2072 Skipped windows due to 30min gap:  32 Skipped windows due to 15 gaps of 32s or more:  51 Skipped windows due to values larger than 50k:  0 Procentage skipped:  4.005791505791506\n",
      "REFIT_19 4720415\n",
      "4720415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 58/64 [01:03<00:14,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1755 Skipped windows due to 30min gap:  15 Skipped windows due to 15 gaps of 32s or more:  20 Skipped windows due to values larger than 50k:  0 Procentage skipped:  1.9943019943019942\n",
      "REFIT_2 4999158\n",
      "4999158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 59/64 [01:06<00:12,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  1859 Skipped windows due to 30min gap:  33 Skipped windows due to 15 gaps of 32s or more:  8 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.205486820871436\n",
      "REFIT_5 6287475\n",
      "6287475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 60/64 [01:10<00:11,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  2338 Skipped windows due to 30min gap:  33 Skipped windows due to 15 gaps of 32s or more:  31 Skipped windows due to values larger than 50k:  0 Procentage skipped:  2.737382378100941\n",
      "UKDALE_5 1425706\n",
      "1425706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 61/64 [01:11<00:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  530 Skipped windows due to 30min gap:  2 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.37735849056603776\n",
      "UKDALE_2 2156879\n",
      "2156879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 62/64 [01:13<00:04,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  802 Skipped windows due to 30min gap:  8 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.997506234413965\n",
      "UKDALE_1 17064863\n",
      "17064863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 64/64 [01:34<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows:  6346 Skipped windows due to 30min gap:  35 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  0.5515285219035614\n",
      "UKDALE_3 396227\n",
      "396227\n",
      "Total windows:  147 Skipped windows due to 30min gap:  7 Skipped windows due to 15 gaps of 32s or more:  0 Skipped windows due to values larger than 50k:  0 Procentage skipped:  4.761904761904762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data.keys()\n",
    "processed = {}\n",
    "\n",
    "for key in tqdm(data.keys()):\n",
    "    # if \"UKDALE\" not in key: \n",
    "    #     continue\n",
    "    print(key, len(data[key]))\n",
    "    processed[key] = create_windows({key : data[key]}, labels_path, save_path, time_window, upper_bound, max_gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggregate</th>\n",
       "      <th>kettle</th>\n",
       "      <th>projector</th>\n",
       "      <th>laptop</th>\n",
       "      <th>electric space heater</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:36</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-27 20:35:44</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:32</th>\n",
       "      <td>176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:40</th>\n",
       "      <td>174.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:14:56</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-08 05:15:04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425100 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aggregate  kettle  projector  laptop  \\\n",
       "0                                                           \n",
       "2013-02-27 20:35:12        5.0       0          0       0   \n",
       "2013-02-27 20:35:20        4.0       0          1       0   \n",
       "2013-02-27 20:35:28        5.0       0          1       0   \n",
       "2013-02-27 20:35:36        4.0       0          0       0   \n",
       "2013-02-27 20:35:44        4.0       0          0       0   \n",
       "...                        ...     ...        ...     ...   \n",
       "2013-04-08 05:14:32      176.0       0          0       0   \n",
       "2013-04-08 05:14:40      174.0       0          0       0   \n",
       "2013-04-08 05:14:48        0.0       0          0       0   \n",
       "2013-04-08 05:14:56        0.0       1          0       0   \n",
       "2013-04-08 05:15:04        0.0       0          0       0   \n",
       "\n",
       "                     electric space heater  \n",
       "0                                           \n",
       "2013-02-27 20:35:12                      0  \n",
       "2013-02-27 20:35:20                      0  \n",
       "2013-02-27 20:35:28                      0  \n",
       "2013-02-27 20:35:36                      0  \n",
       "2013-02-27 20:35:44                      0  \n",
       "...                                    ...  \n",
       "2013-04-08 05:14:32                      0  \n",
       "2013-04-08 05:14:40                      0  \n",
       "2013-04-08 05:14:48                      0  \n",
       "2013-04-08 05:14:56                      0  \n",
       "2013-04-08 05:15:04                      0  \n",
       "\n",
       "[425100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_devices(data):\n",
    "\n",
    "    devices = set()\n",
    "    real_str = \" \"\n",
    "    for key in data.keys():\n",
    "        if key == \"aggregate\":\n",
    "            continue\n",
    "        real_str+= key + \", \"\n",
    "        devices.add(preprocess_string(key))\n",
    "        \n",
    "\n",
    "    return list(devices), real_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../Energy_graph/data/training_data/real_house.pkl\", \"wb\") as f:\n",
    "    pickle.dump(processed, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAWE_1\n",
      "2\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "7/7 [==============================] - 1s 26ms/step\n",
      "0.5008752495756639 0.6483688948976613 0.46766169154228854\n",
      "HES_1\n",
      "2\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "0.9336485220415739 1.0 0.8800236406619385\n",
      "REDD_1\n",
      "2\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "0.19309986010022545 0.4746600741656366 0.16563658838071693\n",
      "REDD_2\n",
      "2\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "0.48504702660240756 0.582089552238806 0.43656716417910446\n",
      "REDD_3\n",
      "2\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 27ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "0.19243143563985396 0.43884892086330934 0.1630695443645084\n",
      "REDD_4\n",
      "2\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "3/3 [==============================] - 0s 25ms/step\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "3/3 [==============================] - 0s 26ms/step\n",
      "0.09473283976124885 0.37109375 0.05859375\n",
      "REDD_5\n",
      "2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0.4062575794324521 0.4838709677419355 0.3709677419354839\n",
      "REDD_6\n",
      "2\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "0.19418426691153964 0.3501683501683502 0.18518518518518517\n",
      "DRED_1\n",
      "2\n",
      "20/20 [==============================] - 1s 26ms/step\n",
      "20/20 [==============================] - 0s 25ms/step\n",
      "20/20 [==============================] - 1s 25ms/step\n",
      "20/20 [==============================] - 0s 25ms/step\n",
      "20/20 [==============================] - 1s 26ms/step\n",
      "0.3613230390192975 0.6204464870610003 0.32207407407407407\n",
      "HEART_33\n",
      "2\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "0.3430030937477516 0.33018970189701896 0.36829268292682926\n",
      "HEART_7\n",
      "2\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n",
      "0.35986203191925475 0.35865287188359446 0.3611342785654712\n",
      "DEDDIAG_8\n",
      "2\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "53/53 [==============================] - 1s 26ms/step\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "53/53 [==============================] - 1s 25ms/step\n",
      "0.5356829900571298 0.8939638012865683 0.43100065402223675\n",
      "ECO_1\n",
      "2\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "0.27352528289181305 0.8616935142537381 0.22764227642276422\n",
      "ECO_6\n",
      "2\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "24/24 [==============================] - 1s 25ms/step\n",
      "0.45390835202627133 0.6848777000471161 0.3463541666666667\n",
      "ECO_2\n",
      "2\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "0.2957932328210172 0.6231134045085699 0.278561209891682\n",
      "ECO_5\n",
      "2\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "28/28 [==============================] - 1s 25ms/step\n",
      "0.47195018658088017 0.9077831547647703 0.43652371485373476\n",
      "ECO_4\n",
      "2\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "0.7032456460306385 0.7452192998889746 0.6678032148075986\n",
      "ECO_3\n",
      "2\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 25ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 25ms/step\n",
      "0.6029031141997384 0.7598817009248773 0.6286644951140065\n",
      "ENERTALK_1\n",
      "2\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "0.5368382177333908 0.9717026819888461 0.4714537963507946\n",
      "ENERTALK_18\n",
      "2\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "0.6620204606809102 0.75 0.5997340425531915\n",
      "ENERTALK_12\n",
      "2\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "0.4739489736147738 0.5833426086316169 0.44888108819657746\n",
      "ENERTALK_20\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "0.31736878506109273 0.33426573426573425 0.3020979020979021\n",
      "ENERTALK_15\n",
      "2\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "0.938832021005673 0.9875956027108486 0.8947368421052632\n",
      "ENERTALK_6\n",
      "2\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 25ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "6/6 [==============================] - 0s 26ms/step\n",
      "0.47444418120802656 0.7590144230769231 0.43345543345543347\n",
      "ENERTALK_8\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.3234914276434333 0.5 0.24074074074074073\n",
      "ENERTALK_2\n",
      "2\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 24ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "0.6806172047162045 0.9927098186492077 0.6247464503042597\n",
      "ENERTALK_11\n",
      "2\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "0.7878276003276004 1.0 0.6722689075630253\n",
      "ENERTALK_16\n",
      "2\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 26ms/step\n",
      "0.9436090225563909 1.0 0.8932384341637011\n",
      "ENERTALK_5\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.4966494157471601 0.5063011015911874 0.511578947368421\n",
      "ENERTALK_7\n",
      "2\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.5 0.5 0.5\n",
      "ENERTALK_14\n",
      "2\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "13/13 [==============================] - 0s 26ms/step\n",
      "0.15347721822541965 1.0 0.08311688311688312\n",
      "ENERTALK_13\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "0.3365146100483498 0.3501006036217304 0.323943661971831\n",
      "ENERTALK_19\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "0.6481948714749804 0.9740255257239991 0.48577680525164113\n",
      "ENERTALK_21\n",
      "2\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "0.33072191433779696 0.5159235668789809 0.2791932059447983\n",
      "ENERTALK_0\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "0.5085194757098893 0.6464675913862539 0.4950805008944544\n",
      "ENERTALK_4\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "0.7128795064755322 0.9353220825833164 0.6495934959349593\n",
      "ENERTALK_17\n",
      "2\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 25ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "11/11 [==============================] - 0s 26ms/step\n",
      "0.5552453930852128 0.7818407279385825 0.5315822388993121\n",
      "ENERTALK_10\n",
      "2\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "0.4207670310715854 1.0 0.3534675615212528\n",
      "ENERTALK_3\n",
      "2\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 25ms/step\n",
      "14/14 [==============================] - 0s 26ms/step\n",
      "0.04211619543650794 0.4965277777777778 0.02199074074074074\n",
      "ENERTALK_9\n",
      "2\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "15/15 [==============================] - 0s 26ms/step\n",
      "0.4397902574645901 0.9538688210745934 0.40801354401805867\n",
      "REFIT_13\n",
      "2\n",
      "43/43 [==============================] - 1s 26ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 26ms/step\n",
      "43/43 [==============================] - 1s 26ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "0.4794494784860013 0.7986656579340571 0.40770725388601037\n",
      "REFIT_6\n",
      "2\n",
      "60/60 [==============================] - 2s 25ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "0.6749540826097389 0.9081763593824688 0.593030242339275\n",
      "REFIT_1\n",
      "2\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "69/69 [==============================] - 2s 26ms/step\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "69/69 [==============================] - 2s 25ms/step\n",
      "0.3980972921759798 0.766385631188681 0.4018416677324466\n",
      "REFIT_21\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "0.45505766917916557 0.6217043220527281 0.4354744808849214\n",
      "REFIT_8\n",
      "2\n",
      "62/62 [==============================] - 2s 25ms/step\n",
      "62/62 [==============================] - 2s 25ms/step\n",
      "62/62 [==============================] - 2s 25ms/step\n",
      "62/62 [==============================] - 2s 26ms/step\n",
      "62/62 [==============================] - 2s 26ms/step\n",
      "0.6186423292351364 0.897914435512884 0.5729982887507881\n",
      "REFIT_9\n",
      "2\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 25ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "60/60 [==============================] - 2s 26ms/step\n",
      "0.498968870074634 0.903633554551245 0.4430954272098037\n",
      "REFIT_20\n",
      "2\n",
      "54/54 [==============================] - 1s 26ms/step\n",
      "54/54 [==============================] - 1s 26ms/step\n",
      "54/54 [==============================] - 1s 26ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "0.6125393778774827 0.8743209111549864 0.5311355311355311\n",
      "REFIT_7\n",
      "2\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "66/66 [==============================] - 2s 26ms/step\n",
      "0.6184004394121984 0.8309242965030035 0.5740226986128626\n",
      "REFIT_15\n",
      "2\n",
      "60/60 [==============================] - 2s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "60/60 [==============================] - 1s 25ms/step\n",
      "0.4571135315464248 0.8047035226950325 0.4426062980699463\n",
      "REFIT_12\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 26ms/step\n",
      "0.6213138032576755 0.9043113210512488 0.5349623482403565\n",
      "REFIT_4\n",
      "2\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "67/67 [==============================] - 2s 26ms/step\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "67/67 [==============================] - 2s 25ms/step\n",
      "0.5927398444822197 0.8935679968195096 0.511986301369863\n",
      "REFIT_3\n",
      "2\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "68/68 [==============================] - 2s 26ms/step\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "68/68 [==============================] - 2s 25ms/step\n",
      "0.5835016399019964 0.742024950596074 0.5790172642762285\n",
      "REFIT_18\n",
      "2\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "52/52 [==============================] - 1s 25ms/step\n",
      "0.6613141886259535 0.9273352337435147 0.5542554673352834\n",
      "REFIT_11\n",
      "2\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "43/43 [==============================] - 1s 25ms/step\n",
      "0.3239436741091002 0.7806991503168546 0.28835462058602557\n",
      "REFIT_16\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "0.608615766294815 0.6987664035047385 0.5841446453407511\n",
      "REFIT_17\n",
      "2\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "55/55 [==============================] - 1s 25ms/step\n",
      "0.6129264919242996 0.9148831072099401 0.5414779631061527\n",
      "REFIT_10\n",
      "2\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "63/63 [==============================] - 2s 25ms/step\n",
      "0.49508534801038906 0.7048704588249881 0.47951558876578204\n",
      "REFIT_19\n",
      "2\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "54/54 [==============================] - 1s 25ms/step\n",
      "0.47783858611386104 0.6254716616733725 0.41051616015436565\n",
      "REFIT_2\n",
      "2\n",
      "57/57 [==============================] - 1s 26ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "57/57 [==============================] - 1s 25ms/step\n",
      "0.4822822485346139 0.7896359941103305 0.4353133839511687\n",
      "REFIT_5\n",
      "2\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "72/72 [==============================] - 2s 26ms/step\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "72/72 [==============================] - 2s 25ms/step\n",
      "0.5289473319012725 0.7802312454689498 0.4613081873970962\n",
      "UKDALE_5\n",
      "2\n",
      "17/17 [==============================] - 0s 25ms/step\n",
      "17/17 [==============================] - 0s 25ms/step\n",
      "17/17 [==============================] - 0s 26ms/step\n",
      "17/17 [==============================] - 0s 26ms/step\n",
      "17/17 [==============================] - 0s 26ms/step\n",
      "0.29220143640157703 0.5050456606805548 0.24193294506532348\n",
      "UKDALE_2\n",
      "2\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "25/25 [==============================] - 1s 26ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "0.2287783570121012 0.2753419158712448 0.2076704895794796\n",
      "UKDALE_1\n",
      "2\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "198/198 [==============================] - 5s 25ms/step\n",
      "0.1896290030041717 0.4496575532812503 0.1608097535532664\n",
      "UKDALE_3\n",
      "2\n",
      "5/5 [==============================] - 0s 26ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "5/5 [==============================] - 0s 26ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "0.033344139304404205 0.2980132450331126 0.017660044150110375\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "labels = pd.read_pickle(labels_path)\n",
    "\n",
    "for f in os.listdir(\"./models/test_IDEAL/\"):\n",
    "        if \"last\" in f:\n",
    "            model = tf.keras.models.load_model(\"./models/test_IDEAL/\"+f, custom_objects={'F1Score': NUK.F1Score, \"WeightedF1\": NUK.WeightedF1Score(class_weighs_pre)})\n",
    "            models.append(model)\n",
    "\n",
    "for h in processed:\n",
    "    print(h)\n",
    "    df = processed[h]\n",
    "    curr_devices, real_str = get_devices(data[h])\n",
    "    X = np.array([i[0] for i in df])\n",
    "    y = np.array([i[1] for i in df])\n",
    "    X= normalize(X)\n",
    "    print(len(X.shape))\n",
    "\n",
    "    if len(X.shape) !=2 or X.shape[1] != 2688:\n",
    "        print(\"wrong shape\", X.shape)\n",
    "        continue\n",
    "    \n",
    "    model_predictions = []\n",
    "    for m in models:\n",
    "            y_pred = m.predict(X)\n",
    "            model_predictions.append(y_pred)\n",
    "\n",
    "\n",
    "    y_pred_avg = np.mean(model_predictions, axis=0)\n",
    "    y_pred_tf = np.where(y_pred_avg > 0.3, 1, 0)\n",
    "\n",
    "    y_pred_num_devices = np.sum(y_pred_tf, axis=1)\n",
    "    y_real_num_devices = np.sum(y, axis=1)\n",
    "\n",
    "    # print(\"average number of devices misspredcited: \", np.mean(y_pred_num_devices - y_real_num_devices)\t)\n",
    "\n",
    "    res = metrics.classification_report(y, y_pred_tf, target_names=labels, zero_division=0, output_dict=True)\n",
    "    print(res[\"weighted avg\"][\"f1-score\"], res[\"weighted avg\"][\"precision\"], res[\"weighted avg\"][\"recall\"])\n",
    "\n",
    "    # barplot y_pred_tf\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(labels, y_pred_tf.sum(axis=0))\n",
    "    # plt.title(h + real_str + \"f1: \"+ str(res[\"weighted avg\"][\"f1-score\"]) + \"precision: \"+ str(res[\"weighted avg\"][\"precision\"]) + \"recall: \"+ str(res[\"weighted avg\"][\"recall\"]))\n",
    "    plt.title(h + \" f1: \"+ str(res[\"weighted avg\"][\"f1-score\"])[:5] + \" average number of devices misspredcited: \" + str(np.mean(y_pred_num_devices - y_real_num_devices))[:5]\t)\n",
    "\n",
    "    # Get current axis\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Loop through label list and set the color to red for highlighted labels\n",
    "    for label in ax.get_xticklabels():\n",
    "        if label.get_text() in curr_devices:\n",
    "            label.set_color('red')\n",
    "\n",
    "    # Rotate all labels\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f\"./plots/real_ideal/{h}_barplot.svg\", format=\"svg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n",
      "television\n",
      "audio system\n",
      "fridge\n",
      "washer dryer\n",
      "kettle\n",
      "washing machine\n"
     ]
    }
   ],
   "source": [
    "tb = pd.read_pickle(\"../../Energy_graph/data/tracebase/devices_data.pkl\")\n",
    "x = tb[\"fridge\"][0][:2668]\n",
    "x = x.values.reshape(1, -1)\n",
    "# print(x.values.reshape(1, -1)[0])\n",
    "model_predictions = []\n",
    "for m in models:\n",
    "    y_pred = m.predict(x)\n",
    "    model_predictions.append(y_pred)\n",
    "    \n",
    "y_pred_avg = np.mean(model_predictions, axis=0)\n",
    "y_pred_tf = np.where(y_pred_avg > 0.3, 1, 0)\n",
    "print(y_pred_tf)\n",
    "for i in range(len(labels)):\n",
    "    if y_pred_tf[0][i] == 1:\n",
    "        print(labels[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def preprocess_string(string : str) -> str:\n",
    "    string = string.lower().strip()\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    string = string.replace(\"-\", \" \")\n",
    "    string = string.replace(\"&\", \" \")\n",
    "    string = string.split(\"(\")[0]\n",
    "    string = string.split(\"#\")[0]\n",
    "\n",
    "    string = string.strip()\n",
    "\n",
    "    # handle known synoynms\n",
    "    synonyms = {\n",
    "        \"refrigerator\": \"fridge\",\n",
    "        \"vaccumcleaner\": \"vacuum cleaner\",\n",
    "        \"breadmaker\": \"bread maker\",\n",
    "      \n",
    "        \n",
    "    }\n",
    "    if \"freezer\" in string:\n",
    "        string = \"fridge\"\n",
    "\n",
    "    if string in synonyms:\n",
    "        string = synonyms[string]\n",
    "\n",
    "    if 'hi fi' in string:\n",
    "        string = \"audio system\"\n",
    "\n",
    "    if \"router\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    if \"treadmill\" in string:\n",
    "        string = \"running machine\"\n",
    "        \n",
    "\n",
    "    if \"laptop\" in string:\n",
    "        string = \"laptop\"\n",
    "    \n",
    "    if \"server\" in string:\n",
    "        string = \"server\"\n",
    "\n",
    "    if \"monitor\" in string and not \"baby\" in string:\n",
    "        string = \"monitor\"\n",
    "    # special cases\n",
    "    if \"computer\" in string and \"charger\" not in string:\n",
    "        string = \"pc\"\n",
    "\n",
    "    if \"tv\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"television\" in string:\n",
    "        string = \"television\"\n",
    "\n",
    "    if \"macbook\" in string:\n",
    "        string = \"laptop\"\n",
    "        \n",
    "    if \"car charger\" == string:\n",
    "        string = \"ev\"\n",
    "    \n",
    "    if \"toast\" in string:\n",
    "        string = \"toaster\"\n",
    "    \n",
    "    if \"modem\" in string:\n",
    "        string = \"router\"\n",
    "\n",
    "    # we treat all audio devices as speakers so subwoofer is also a speaker\n",
    "    if \"subwoofer\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"speaker\" in string:\n",
    "        string = \"speaker\"\n",
    "\n",
    "    if \"iron\" in string and \"soldering\" not in string:\n",
    "        string = \"iron\"\n",
    "\n",
    "    \n",
    "    if \"coffeemachine\" in string:\n",
    "        string = \"coffee machine\"\n",
    "    if \"coffee maker\" in string:\n",
    "        string = \"coffee machine\"\n",
    "\n",
    "    if \"dishwasher\" in string:\n",
    "        string = \"dish washer\"\n",
    "    if \"air conditioner\" in string:\n",
    "        string = \"ac\"\n",
    "\n",
    "    if \"air conditioning\" in string:\n",
    "        string = \"ac\"\n",
    "    \n",
    "    string = re.sub(' +', ' ', string)\n",
    "    string = re.sub(r'\\d+', '', string)\n",
    "    return string.strip()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
