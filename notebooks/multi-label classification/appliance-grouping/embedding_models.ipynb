{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "device_list = pd.read_pickle('./Energy_graph/embeddings/device_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coffeemachine',\n",
       " 'food mixer',\n",
       " 'electric oven',\n",
       " 'games console',\n",
       " 'electric stove',\n",
       " 'refrigerator',\n",
       " 'stereo',\n",
       " 'treadmill',\n",
       " 'air conditioner',\n",
       " 'bouncy castle pump',\n",
       " 'electric heating element',\n",
       " 'food processor',\n",
       " 'hair straighteners',\n",
       " 'oven',\n",
       " 'water purifier',\n",
       " 'audio system',\n",
       " 'tumble dryer',\n",
       " 'heat kitchen',\n",
       " 'heat bedroom #3',\n",
       " 'electric heater ',\n",
       " 'fridge freezer',\n",
       " 'kimchi fridge',\n",
       " 'heat basement',\n",
       " 'router',\n",
       " 'washing machine ',\n",
       " 'heat garage',\n",
       " 'whirlpool bath',\n",
       " 'freezer',\n",
       " 'office desk',\n",
       " 'microwave',\n",
       " 'combination microwave',\n",
       " 'electric heater',\n",
       " 'hi fi',\n",
       " 'solar thermal pumping station',\n",
       " 'htpc',\n",
       " 'air conditioning',\n",
       " 'charger',\n",
       " 'usb hub',\n",
       " 'ce appliance',\n",
       " 'audio amplifier',\n",
       " 'fridge',\n",
       " 'air exhaust',\n",
       " 'cooker',\n",
       " 'laptops',\n",
       " 'handmixer',\n",
       " 'coffee maker',\n",
       " 'dishwasher',\n",
       " 'mobile phone charger',\n",
       " 'heat bedroom #2',\n",
       " 'blender',\n",
       " 'tumble dryer ',\n",
       " 'breadmaker',\n",
       " 'set top box',\n",
       " 'fan',\n",
       " 'stove oven',\n",
       " 'vivarium',\n",
       " 'coffee machine',\n",
       " 'television',\n",
       " 'dish washer',\n",
       " 'air exchanger',\n",
       " 'printer',\n",
       " 'baby monitor',\n",
       " 'running machine',\n",
       " 'tablet',\n",
       " 'hairdryer straightener',\n",
       " 'dryer',\n",
       " 'gaming pc',\n",
       " 'broadband router',\n",
       " 'chest freezer ',\n",
       " 'heat dining room',\n",
       " 'oven extractor fan',\n",
       " 'hair dryer',\n",
       " 'motor',\n",
       " 'washing machine',\n",
       " 'pond pump',\n",
       " 'chest freezer',\n",
       " 'rice cooker',\n",
       " 'fountain',\n",
       " 'fridge freezer ',\n",
       " 'radio',\n",
       " 'heat living room',\n",
       " 'vacuum cleaner',\n",
       " 'electric space heater',\n",
       " 'pc',\n",
       " 'minioven',\n",
       " 'vaccumcleaner',\n",
       " 'car charger',\n",
       " 'waste disposal unit',\n",
       " 'upright freezer',\n",
       " 'air handling unit',\n",
       " 'iron',\n",
       " 'electric furnace',\n",
       " 'toaster',\n",
       " 'subpanel',\n",
       " 'water heater',\n",
       " 'stove',\n",
       " 'heater',\n",
       " 'watercooker',\n",
       " 'kettle',\n",
       " 'dehumidifier/heater',\n",
       " 'towel dryer',\n",
       " 'external hard disk',\n",
       " 'smoke alarm',\n",
       " 'monitor',\n",
       " 'network attached storage',\n",
       " 'heat bedroom #1',\n",
       " 'tumble dryer 3',\n",
       " 'laptop',\n",
       " 'speaker',\n",
       " 'bread maker',\n",
       " 'wet appliance',\n",
       " 'boiler',\n",
       " 'projector',\n",
       " 'washer dryer',\n",
       " 'freezer ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embedding\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(hidden_states, dim=1)\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "# Sample list of devices\n",
    "device_list = pd.read_pickle('./Energy_graph/device_list.pkl')\n",
    "\n",
    "# Generate BERT embeddings for each device\n",
    "device_embeddings = np.array([get_bert_embedding(device).flatten() for device in device_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfasttext\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfasttext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load FastText model\n",
    "ft = fasttext.load_model('./Energy_graph/embeddings/fastText/cc.en.300.bin')\n",
    "\n",
    "# Device names to be clustered\n",
    "# device_list = [\"laptop\", \"mobile\", \"tablet\", \"desktop\", \"router\", \"fridge\", \"washing machine\", \"oven\"]\n",
    "\n",
    "# Generate embeddings for each device name\n",
    "device_vectors = np.array([ft.get_sentence_vector(device) for device in device_list])\n",
    "device_vectors = np.array([ft.get_sentence_vector(device) for device in device_list], dtype='double')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2-medium'  # You can choose other versions as well: 'gpt2', 'gpt2-large', 'gpt2-xl'\n",
    "model = GPT2Model.from_pretrained(model_name, output_hidden_states=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=50)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the embeddings from one of the hidden layers, for example, the second to last layer\n",
    "    embeddings = outputs.hidden_states[-2][0]\n",
    "    return embeddings.mean(dim=0)\n",
    "\n",
    "word_list = device_list\n",
    "embeddings = [get_embedding(word, model, tokenizer).numpy() for word in word_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 07:49:38.602106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "N_CLUSTERS = 25\n",
    "\n",
    "# Load the largge English model in spaCy (contains word vectors)\n",
    "# nlp = spacy.load('en_core_web_trf')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# List of devices\n",
    "devices = device_list\n",
    "\n",
    "# Create vectors for each device name\n",
    "device_vectors = np.array([nlp(device).vector for device in devices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.30.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.9.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.8.8)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (65.5.1)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.2.1)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=f279d55d2e5505204a458d4cedb5af903b405bec3acdb80cd62f71df1879e920\n",
      "  Stored in directory: /home/jovyan/shared/anaconda/pip/wheels/0a/f5/dd/9d00836c4e9e279c2a59d5b0ab72dafa66cbc626a327c550dd\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, torchvision, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99 torchvision-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(device_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 5: ['coffeemachine', 'coffee maker', 'coffee machine']\n",
      "Group 75: ['food mixer']\n",
      "Group 0: ['electric oven', 'oven', 'stove oven']\n",
      "Group 65: ['games console']\n",
      "Group 11: ['electric stove', 'stove']\n",
      "Group 72: ['refrigerator', 'fridge']\n",
      "Group 52: ['stereo']\n",
      "Group 29: ['treadmill']\n",
      "Group 8: ['air conditioner', 'air conditioning']\n",
      "Group 63: ['bouncy castle pump']\n",
      "Group 84: ['electric heating element']\n",
      "Group 31: ['food processor']\n",
      "Group 14: ['hair straighteners', 'hairdryer straightener']\n",
      "Group 9: ['water purifier']\n",
      "Group 12: ['audio system']\n",
      "Group 7: ['tumble dryer', 'tumble dryer ', 'tumble dryer 3']\n",
      "Group 80: ['heat kitchen']\n",
      "Group 6: ['heat bedroom #3', 'heat bedroom #2', 'heat bedroom #1']\n",
      "Group 30: ['electric heater ', 'electric heater', 'heater']\n",
      "Group 1: ['fridge freezer', 'fridge freezer ']\n",
      "Group 73: ['kimchi fridge']\n",
      "Group 61: ['heat basement']\n",
      "Group 22: ['router', 'broadband router']\n",
      "Group 13: ['washing machine ', 'washing machine']\n",
      "Group 51: ['heat garage']\n",
      "Group 55: ['whirlpool bath']\n",
      "Group 81: ['freezer', 'upright freezer', 'freezer ']\n",
      "Group 54: ['office desk']\n",
      "Group 27: ['microwave', 'combination microwave']\n",
      "Group 26: ['hi fi']\n",
      "Group 25: ['solar thermal pumping station']\n",
      "Group 42: ['htpc']\n",
      "Group 20: ['charger', 'car charger']\n",
      "Group 49: ['usb hub']\n",
      "Group 44: ['ce appliance']\n",
      "Group 77: ['audio amplifier']\n",
      "Group 38: ['air exhaust']\n",
      "Group 35: ['cooker', 'rice cooker']\n",
      "Group 16: ['laptops', 'laptop']\n",
      "Group 33: ['handmixer']\n",
      "Group 46: ['dishwasher', 'dish washer']\n",
      "Group 78: ['mobile phone charger']\n",
      "Group 4: ['blender']\n",
      "Group 23: ['breadmaker', 'bread maker']\n",
      "Group 28: ['set top box']\n",
      "Group 57: ['fan']\n",
      "Group 19: ['vivarium']\n",
      "Group 32: ['television']\n",
      "Group 58: ['air exchanger']\n",
      "Group 41: ['printer']\n",
      "Group 17: ['baby monitor', 'monitor']\n",
      "Group 56: ['running machine']\n",
      "Group 66: ['tablet']\n",
      "Group 15: ['dryer', 'hair dryer', 'towel dryer', 'washer dryer']\n",
      "Group 82: ['gaming pc']\n",
      "Group 39: ['chest freezer ', 'chest freezer']\n",
      "Group 83: ['heat dining room']\n",
      "Group 62: ['oven extractor fan']\n",
      "Group 3: ['motor']\n",
      "Group 36: ['pond pump']\n",
      "Group 68: ['fountain']\n",
      "Group 67: ['radio']\n",
      "Group 70: ['heat living room']\n",
      "Group 59: ['vacuum cleaner']\n",
      "Group 2: ['electric space heater']\n",
      "Group 10: ['pc']\n",
      "Group 34: ['minioven']\n",
      "Group 24: ['vaccumcleaner']\n",
      "Group 40: ['waste disposal unit']\n",
      "Group 74: ['air handling unit']\n",
      "Group 45: ['iron']\n",
      "Group 76: ['electric furnace']\n",
      "Group 71: ['toaster']\n",
      "Group 37: ['subpanel']\n",
      "Group 79: ['water heater']\n",
      "Group 50: ['watercooker']\n",
      "Group 60: ['kettle']\n",
      "Group 53: ['dehumidifier/heater']\n",
      "Group 18: ['external hard disk']\n",
      "Group 43: ['smoke alarm']\n",
      "Group 21: ['network attached storage']\n",
      "Group 64: ['speaker']\n",
      "Group 47: ['wet appliance']\n",
      "Group 69: ['boiler']\n",
      "Group 48: ['projector']\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 170\n",
    "# Perform KMeans clustering\n",
    "n_clusters = 85  # You can choose a different number based on your requirements\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED)\n",
    "kmeans.fit(embeddings)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Group devices by their cluster labels\n",
    "grouped_devices = {}\n",
    "for label, device in zip(labels, device_list):\n",
    "    if label not in grouped_devices:\n",
    "        grouped_devices[label] = []\n",
    "    grouped_devices[label].append(device)\n",
    "\n",
    "# Print grouped devices\n",
    "for label, devices in grouped_devices.items():\n",
    "    print(f\"Group {label}: {devices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m silhouette_score\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[39m# Run initial KMeans\u001b[39;00m\n\u001b[1;32m      5\u001b[0m best_score \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Run initial KMeans\n",
    "best_score = -1\n",
    "best_n_clusters = 0\n",
    "best_labels = None\n",
    "\n",
    "for n_clusters in tqdm(range(10, 51, 5)):  # Ranging from 10 to 50 clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    labels = kmeans.fit_predict(device_vectors)\n",
    "    score = silhouette_score(device_vectors, labels)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_n_clusters = n_clusters\n",
    "        best_labels = labels\n",
    "\n",
    "# Run KMeans again with the best number of clusters\n",
    "kmeans = KMeans(n_clusters=best_n_clusters)\n",
    "labels = kmeans.fit_predict(device_vectors)\n",
    "\n",
    "# Detect and reassign outliers\n",
    "grouped_appliances = {}\n",
    "for label, appliance in tqdm(zip(labels, device_list)):\n",
    "    if label not in grouped_appliances:\n",
    "        grouped_appliances[label] = []\n",
    "    grouped_appliances[label].append(appliance)\n",
    "\n",
    "for label, group in tqdm(grouped_appliances.items()):\n",
    "    if len(group) < 3:  # Assuming a cluster with less than 3 points is an \"outlier\"\n",
    "        for appliance in group:\n",
    "            vector = ft.get_sentence_vector(appliance)\n",
    "            new_label = kmeans.predict([vector])[0]\n",
    "            grouped_appliances[new_label].append(appliance)\n",
    "        del grouped_appliances[label]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
