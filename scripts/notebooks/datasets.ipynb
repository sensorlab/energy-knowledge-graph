{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEKN\n",
    "\n",
    "https://data.open-power-system-data.org/household_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Energy_graph/data/temp/household_data_15min_singleindex_filtered.csv\")\n",
    "df =df.drop(columns=[\"utc_timestamp\", \"interpolated\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cet_cest_timestamp\"] = df[\"cet_cest_timestamp\"].apply(lambda x: x.split(\"+\")[0])\n",
    "df[\"cet_cest_timestamp\"] = pd.to_datetime(df[\"cet_cest_timestamp\"], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "df = df.set_index(\"cet_cest_timestamp\")\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract household identifiers\n",
    "households = set(column.split('_')[2] for column in df.columns)\n",
    "\n",
    "# Create a dictionary of dataframes, one for each household\n",
    "dfs = {}\n",
    "\n",
    "for household in households:\n",
    "    # Filter columns relevant to this household\n",
    "    relevant_columns = [col for col in df.columns if household in col]\n",
    "    temp_df = df[relevant_columns].copy()\n",
    "\n",
    "    # Rename columns to remove the prefix and retain the device name\n",
    "    rename_dict = {col: col.replace(f\"DE_KN_{household}_\", \"\") for col in relevant_columns}\n",
    "    temp_df.rename(columns=rename_dict, inplace=True)\n",
    "    temp_df.rename(columns={'cet_cest_timestamp': 'timestamp', \"grid_import\": \"aggregate\"}, inplace=True)\n",
    "    if \"grid_export\" in temp_df.columns:\n",
    "        temp_df.drop(columns=['grid_export'], inplace=True)\n",
    "    if \"pv\" in temp_df.columns:\n",
    "        temp_df.drop(columns=['pv'], inplace=True)\n",
    "    # temp_df.drop(columns=['grid_export', 'pv'], inplace=True)\n",
    "    data = {}\n",
    "    name =\"DEKN_\" +str(household[-1])\n",
    "    for c in temp_df.columns:\n",
    "        data[c] = pd.DataFrame(temp_df[c].dropna())\n",
    "        \n",
    "    dfs[name] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"./Energy_graph/data/temp/household_data.xlsx\")\n",
    "df2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREEND\n",
    "https://sourceforge.net/projects/greend/\n",
    "\n",
    "\n",
    "\n",
    "GREEND download form\n",
    "Great to get to know you! \n",
    "\n",
    "Here are our dataset snapshots and the associated password:\n",
    "\n",
    "v0.1: \n",
    "http://sourceforge.net/projects/greend/files/GREEND_0-1_311014.zip/download\n",
    "\n",
    "PWD:\"Vienna\"\n",
    "\n",
    "\n",
    "https://www.academia.edu/7794767/GREEND_An_Energy_Consumption_Dataset_of_Households_in_Italy_and_Austria\n",
    "\n",
    "http://www.andreatonello.com/wp-content/uploads/PAPERS/CONFERENCES/SGC2014_2.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Energy_graph/data/temp/GREEND/building0/dataset_2013-12-07.csv\", on_bad_lines=\"skip\")\n",
    "df\n",
    "\n",
    "\n",
    "# TODO either fix NILMTK if possible or try to get id to device mapping from somewhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENERTALK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Energy_graph/data/temp/ENERTALK/enertalk\"\n",
    "def convert2KRtime(df):\n",
    "    \"\"\"\n",
    "    convert dateframe's unix timestamp into Asia/Seoul Timezone\n",
    "    \n",
    "    input\n",
    "    ----\n",
    "        df: dataframe (columns: timestamp, active_power, reactive_power, appliance)\n",
    "    \n",
    "    output\n",
    "    ----\n",
    "        df_kr: dataframe (columns: timestamp, active_power, reactive_power, appliance, KR timezone)\n",
    "    \"\"\" \n",
    "\n",
    "    df_kr = df\n",
    "    df_kr['timestamp'] = df_kr['timestamp'].dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "    df_kr = df_kr.set_index(pd.DatetimeIndex(df_kr['timestamp']))\n",
    "    return df_kr\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the name of the file to get the device name\"\n",
    "    \"\"\"\n",
    "    df.drop(columns=[\"reactive_power\"], inplace=True)\n",
    "    # convert unix timestamp to datetime and set as index\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\").dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "    # convert to kWh\n",
    "    df  = df/1000 * (1/15)/3600\n",
    "    # resample to 1 second\n",
    "    df = df.resample(\"1S\").sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the name of the file to get the device name\"\n",
    "    \"\"\"\n",
    "    # remove the extension\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    # get the device name\n",
    "    file_name = file_name.split(\"_\")[1]\n",
    " \n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "\n",
    "def process_house(house):\n",
    "    house_path = os.path.join(data_path, house)\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    \n",
    "    for day in os.listdir(house_path):\n",
    "        day_path = os.path.join(house_path, day)\n",
    "        \n",
    "        for device in os.listdir(day_path):\n",
    "            device_path = os.path.join(day_path, device)\n",
    "            name = parse_name(device)\n",
    "            \n",
    "            df = preprocess_dataframe(pd.read_parquet(device_path))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "    \n",
    "    return house_name, house_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serial program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "data_path = \"./Energy_graph/data/temp/ENERTALK/enertalk\"\n",
    "data_dict = {}\n",
    "for house in os.listdir(data_path):\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    for day in tqdm(os.listdir(data_path + \"/\" + house)):\n",
    "        for device in os.listdir(data_path + \"/\" + house + \"/\" + day):\n",
    "            name = parse_name(device)\n",
    "            df = preprocess_dataframe(pd.read_parquet(data_path + \"/\" + house + \"/\" + day + \"/\" + device))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "    \n",
    "    data_dict[house_name] = house_dict\n",
    "    break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"./Energy_graph/data/temp/ENERTALK/enertalk\"\n",
    "data_dict = {}\n",
    "\n",
    "def process_house(house, progress_bar=None):\n",
    "    house_path = os.path.join(data_path, house)\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    \n",
    "    for day in os.listdir(house_path):\n",
    "        day_path = os.path.join(house_path, day)\n",
    "        for device in os.listdir(day_path):\n",
    "            device_path = os.path.join(day_path, device)\n",
    "            name = parse_name(device)\n",
    "            df = preprocess_dataframe(pd.read_parquet(device_path))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "\n",
    "    if progress_bar:\n",
    "        progress_bar.update(1)  # Increment the progress bar when a house is processed\n",
    "\n",
    "    return house_name, house_dict\n",
    "\n",
    "houses = os.listdir(data_path)\n",
    "# Create a progress bar with a total equal to the number of houses\n",
    "with tqdm(total=len(houses), desc=\"Processing houses\", unit=\"house\") as progress_bar:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Pass the progress_bar to the worker function\n",
    "        futures = [executor.submit(process_house, house, progress_bar) for house in houses]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            house_name, house_dict = future.result()\n",
    "            data_dict[house_name] = house_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "def process_house(house_path, queue):\n",
    "    house = os.path.basename(house_path)  # Extract house name from the path\n",
    "    house_dict = defaultdict(list)\n",
    "    house_name = \"ENERTALK_\" + str(int(house))\n",
    "    \n",
    "    for day in os.listdir(house_path):\n",
    "        day_path = os.path.join(house_path, day)\n",
    "        for device in os.listdir(day_path):\n",
    "            device_path = os.path.join(day_path, device)\n",
    "            name = parse_name(device)\n",
    "            df = preprocess_dataframe(pd.read_parquet(device_path))\n",
    "            house_dict[name].append(df)\n",
    "\n",
    "    for key in house_dict:\n",
    "        house_dict[key] = pd.concat(house_dict[key], axis=0)\n",
    "\n",
    "    queue.put(1)  # Indicate that one house has been processed\n",
    "    return house_name, house_dict\n",
    "\n",
    "# Construct full paths for each house directory\n",
    "data_path = \"./Energy_graph/data/temp/ENERTALK/\"\n",
    "house_paths = [os.path.join(data_path, house) for house in os.listdir(data_path)]\n",
    "queue = multiprocessing.Manager().Queue()\n",
    "\n",
    "with tqdm(total=len(house_paths), desc=\"Processing houses\", unit=\"house\") as progress_bar:\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=os.cpu_count()/2) as executor:\n",
    "        futures = [executor.submit(process_house, house_path, queue) for house_path in house_paths]\n",
    "        \n",
    "        # Update progress bar based on queue\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            progress_bar.update(queue.get())\n",
    "\n",
    "        for future in futures:\n",
    "            house_name, house_dict = future.result()\n",
    "            data_dict[house_name] = house_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"ENERTALK_0\"]\n",
    "# save with pickle\n",
    "import pickle\n",
    "with open(\"./Energy_graph/data/processed/ENERTALK.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(\"./Energy_graph/data/temp/ENERTALK/enertalk/00/20161101/02_washing-machine.parquet.gzip\").drop(columns=[\"reactive_power\"])\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\").dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "df  = df/1000 * (1/15)/3600\n",
    "df.resample(\"1S\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"02_washing-machine.parquet.gzip\"\n",
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the name of the file to get the device name\"\n",
    "    \"\"\"\n",
    "    # remove the extension\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    # get the device name\n",
    "    file_name = file_name.split(\"_\")[1]\n",
    " \n",
    "\n",
    "    return file_name\n",
    "\n",
    "print(parse_name(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEDDIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get map of item_id to label for appliance\n",
    "labels = pd.read_csv(\"./Energy_graph/data/temp/DEDDIAG/house_08/items.tsv\", sep=\"\\t\")\n",
    "labels.set_index(\"item_id\", inplace=True)\n",
    "id_label_map = labels[\"category\"].to_dict()\n",
    "id_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_id(file_name : str) -> int:\n",
    "    return int(file_name.split('_')[1])\n",
    "\n",
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Energy_graph/data/temp/DEDDIAG/house_08/\"\n",
    "from tqdm import tqdm\n",
    "data = {}\n",
    "\n",
    "for device in tqdm([d for d in os.listdir(data_path) if \"data\" in d]):\n",
    "    label = id_label_map[parse_id(device)]\n",
    "    if \"Phase\" not in label:\n",
    "        if \"Total\" in label:\n",
    "            label = \"aggregate\"\n",
    "        df = pd.read_csv(data_path + device, sep=\"\\t\")\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df.drop(columns=[\"item_id\"], inplace=True)\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        df = df.resample(\"1s\").ffill()\n",
    "        df.dropna(inplace=True)\n",
    "        df = watts2kwh(df, 1/3600)\n",
    "        print(label)\n",
    "        data[label] = df\n",
    "\n",
    "    \n",
    "data_dict = {\n",
    "    \"DEDDIAG_8\": data,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"aggregate\"].resample(\"D\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSTData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Energy_graph/data/temp/SUSTData/\"\n",
    "# aggregate consumption data\n",
    "df_aggregate = pd.DataFrame()\n",
    "for file in os.listdir(path + \"aggregate\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_aggregate = pd.concat([df_aggregate,(pd.read_csv(path+\"aggregate/\" + file))])\n",
    "\n",
    "df_aggregate[\"timestamp\"] = pd.to_datetime(df_aggregate[\"timestamp\"])\n",
    "df_aggregate.set_index(\"timestamp\", inplace=True)\n",
    "df_aggregate.drop(columns=['Unnamed: 0', \"Q\",\"V\",\"I\"], inplace=True)\n",
    "df_aggregate.rename(columns={\"P\":\"power\"}, inplace=True)\n",
    "data_dict = {\"aggregate\":df_aggregate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the file name to get the appliance name\n",
    "    \"\"\"\n",
    "    # appliance name\n",
    "    appliance_name = file_name.split(\".\")[0].split(\"_\")[1]\n",
    "    # date\n",
    "    return appliance_name\n",
    "\n",
    "\n",
    "# appliance consumption data\n",
    "for file in os.listdir(path+\"appliances/\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(parse_name(file))\n",
    "        data_dict[parse_name(file)] = pd.read_csv(path + \"appliances/\" + file).set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"aggregate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFRED\n",
    "unused for now because of aggregated apartments might be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Energy_graph/data/temp/MFRED/MFRED_Aggregates_15min_2019Q1-Q4.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBED\n",
    "TODO\n",
    "http://embed-dataset.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n",
    "def parse_name(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse the file name to get the house name\n",
    "    \"\"\"\n",
    "    # appliance name\n",
    "    appliance_name = file_name.split(\".\")[0]\n",
    "\n",
    "    # date\n",
    "    return appliance_name[:5] + \"_\" + appliance_name[5:]\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"./Energy_graph/data/temp/HEART/HERON33.csv\")\n",
    "# df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n",
    "\n",
    "# df = df.set_index(\"Timestamp\").drop(columns=[\"dw\", \"wm\"])\n",
    "# df = watts2kwh(df, 1/3600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Energy_graph/data/temp/HEART/\"\n",
    "data_dict = {}\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # \n",
    "        df = pd.read_csv(data_path + file)\n",
    "        # convert unix timestamp to datetime\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"ms\")\n",
    "        # set datetime as index and drop unnecessary columns\n",
    "        df = df.set_index(\"Timestamp\").drop(columns=[\"dw\", \"wm\"])\n",
    "        \n",
    "        df.rename(columns={\"Value\": \"aggregate\"}, inplace=True)\n",
    "        # convert watts to kilowatt hours\n",
    "        df = watts2kwh(df, 1/3600)\n",
    "        df.dropna(inplace=True)\n",
    "        # create a dictionary of dataframes for each device\n",
    "        devices_dict = {}\n",
    "        for device in df.columns:\n",
    "                devices_dict[device] = pd.DataFrame(df[device])\n",
    "        # add the device dictionary to the data dictionary\n",
    "        data_dict[parse_name(file)] = devices_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watts to kWh given data frequency as a fraction of an hour (e.g. 0.5 for half-hourly data)\n",
    "def watts2kwh(df, data_frequency):\n",
    "    df = df/1000 * data_frequency\n",
    "    return df\n",
    "def read_and_preprocess_df(path):\n",
    "    df = pd.read_csv(path, header=None, names=[\"timestamp\", \"value\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    # set timestamp as index\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    df.sort_index(inplace=True)\n",
    "    # resample to 7s and forward fill up to 35s\n",
    "    df = df.resample(\"7s\").ffill(limit=7).dropna()\n",
    "\n",
    "    # convert to kWh\n",
    "    df = watts2kwh(df, 7/3600)\n",
    "    return df\n",
    "# get house name and appliance name from file name\n",
    "def parse_name(file_name : str):\n",
    "    file_name = file_name.split(\"_\")\n",
    "    house_name = file_name[0].replace(\"home\", \"IDEAL_\")\n",
    "    appliance_name = file_name[3]\n",
    "    if appliance_name == \"electric-mains\":\n",
    "        appliance_name = \"aggregate\"\n",
    "\n",
    "    if appliance_name == \"electric-appliance\":\n",
    "        appliance_name = file_name[4].split(\".\")[0]\n",
    "    return house_name, appliance_name\n",
    "\n",
    "def process_house(house, file_list, data_path):\n",
    "    house_data = {}\n",
    "    for file in file_list:\n",
    "        _, label, df = process_file(file, data_path)\n",
    "        house_data[label] = df\n",
    "    return house, house_data\n",
    "\n",
    "\n",
    "def process_file(file,data_path):\n",
    "    house, label = parse_name(file)\n",
    "    return house, label, read_and_preprocess_df(data_path + \"data_merged/\" + file)\n",
    "\n",
    "def process_files_for_home(house, file_list, data_path):\n",
    "    house_data = {}\n",
    "    for file in file_list:\n",
    "        _, label, df = process_file(file, data_path)\n",
    "        house_data[label] = df\n",
    "    return house, house_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"home168_kitchen1534_sensor12520_electric-appliance_washingmachinetumbledrier.csv.gz\"\n",
    "\n",
    "\n",
    "parse_name(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serial program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data_path = \"./Energy_graph/data/temp/IDEAL/\"\n",
    "files = [file for file in os.listdir(data_path + \"data_merged/\") if (\"electric-appliance\" in file or \"electric-mains\" in file) and \"home223\" not in file]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    house, label = parse_name(file)\n",
    "    data.setdefault(house, {})[label] = read_and_preprocess_df(data_path+\"data_merged/\" + file)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessed(takes around 1m:30s with 64 cores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "def unpack_and_process(p):\n",
    "    return process_house(*p)\n",
    "# Main script body\n",
    "data_path = \"./Energy_graph/data/temp/IDEAL/\"\n",
    "data_dict = {}\n",
    "files_grouped_by_home = defaultdict(list)\n",
    "files = [file for file in os.listdir(data_path + \"data_merged/\") if (\"electric-appliance\" in file or \"electric-mains\" in file) and \"home223\" not in file]\n",
    "for file in files:\n",
    "    house, _ = parse_name(file)\n",
    "    files_grouped_by_home[house].append(file)\n",
    "\n",
    "total_houses = len(files_grouped_by_home)\n",
    "\n",
    "print(\"Processing houses...\")\n",
    "with ProcessPoolExecutor(max_workers=int(os.cpu_count()/2)) as executor, tqdm(total=total_houses, desc=\"Processing houses\", unit=\"house\") as t:\n",
    "    args = ((house, files_grouped_by_home[house], data_path) for house in files_grouped_by_home)\n",
    "    \n",
    "    for house_name, house_data in executor.map(unpack_and_process, args):\n",
    "        data_dict[house_name] = house_data\n",
    "        t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save with pickle to: energy-knowledge-graph\\data\\processed\\IDEAL.pkl\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./Energy_graph/data/processed/IDEAL.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAE\n",
    "TODO\n",
    "needs to be cited\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZJW4LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
